{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/opt/homebrew/lib/python3.9/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead\n",
      "  warnings.warn('jax.experimental.optimizers is deprecated, '\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "from jax import random, grad, jit, vmap\n",
    "from functools import partial\n",
    "key = random.PRNGKey(0)\n",
    "from jax.experimental import optimizers\n",
    "from NODE_ICNN_CANN_MF_fns import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from jax.config import config\n",
    "import pandas as pd\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NODE, CANN, ICNN, MFNN, directly against rubber stress data\n",
    "\n",
    "Train directly against the stress data from the three rubber datasets.\n",
    "Note. I wanted to do these four different schemes. the last one I wanted to do was this MF scheme where I would combine a CANN with a NODE but that doesnt seem necessary in that the CANN and the NODE alone can do just fine for rubber data. So at least for rubber doesnt seem necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CANN_model():\n",
    "    def __init__(self, params_I1, params_I2,normalization):\n",
    "        self.params_I1 = params_I1\n",
    "        self.params_I2 = params_I2\n",
    "        self.normalization = normalization\n",
    "    # Psi1\n",
    "    def Psi1norm(self, I1norm):\n",
    "        # Note: I1norm = (I1-3)/normalization\n",
    "        return CANN_dpsidInorm(I1norm,self.params_I1)[:,0]/normalization[0]\n",
    "    # Psi2\n",
    "    def Psi2norm(self, I2norm):\n",
    "        # Note: I2norm = (I2-3)/normalization\n",
    "        return CANN_dpsidInorm(I2norm,self.params_I2)[:,0]/normalization[1]\n",
    "\n",
    "# play with ICNN a bit, how do we get that one to work\n",
    "class ICNN_model():\n",
    "    def __init__(self, params_I1, params_I2,normalization):\n",
    "        self.params_I1 = params_I1\n",
    "        self.params_I2 = params_I2\n",
    "        self.normalization = normalization\n",
    "    # Psi1\n",
    "    # note: for ICNN the prediction is the function not the gradient\n",
    "    # but the P11_UT, P11_ET, etc expects the model to output the derivative\n",
    "    def Psi1norm(self, I1norm):\n",
    "        # Note: I1norm = (I1-3)/normalization\n",
    "        f1 = lambda x: icnn_forwardpass(x, self.params_I1)[0]\n",
    "        #normalization = [I1_factor,I2_factor,Psi1_factor,Psi2_factor ]\n",
    "        df1 = grad(f1)\n",
    "        return vmap(df1)(I1norm[:,None])[:,0]/self.normalization[0]\n",
    "    # Psi2\n",
    "    # note: for ICNN the prediction is the function not the gradient\n",
    "    def Psi2norm(self, I2norm):\n",
    "        # Note: I2norm = (I2-3)/normalization\n",
    "        f2 = lambda x: icnn_forwardpass(x, self.params_I2)[0]\n",
    "        #normalization = [I1_factor,I2_factor,Psi1_factor,Psi2_factor ]\n",
    "        df2 = grad(f2)\n",
    "        return vmap(df2)(I2norm[:,None])[:,0]/self.normalization[1]\n",
    "        \n",
    "## NODE model outputs normalized strain energy given normalized invariants\n",
    "class NODE_model():\n",
    "    def __init__(self, params_I1, params_I2):\n",
    "        self.params_I1 = params_I1\n",
    "        self.params_I2 = params_I2\n",
    "    def Psi1norm(self, I1norm):\n",
    "        # Note: I1norm = (I1-3)/normalization\n",
    "        return NODE_posb_vmap(I1norm, self.params_I1)\n",
    "    def Psi2norm(self, I2norm):\n",
    "        # Note: I2norm = (I2-3)/normalization\n",
    "        return NODE_posb_vmap(I2norm, self.params_I2)\n",
    "\n",
    "def plotstresses(x_gt,y_gt,x_pr,y_pr):\n",
    "    fig, ax = plt.subplots(1,3,figsize=(12,4))\n",
    "    labels = ['UT', 'ET', 'PS']\n",
    "    for axi, x_gti, y_gti, x_pri, y_pri, label in zip(ax, x_gt, y_gt, x_pr, y_pr, labels):\n",
    "        axi.plot(x_gti, y_gti, 'k.')\n",
    "        axi.plot(x_pri, y_pri)\n",
    "        axi.set_title(label)\n",
    "        axi.set_xlabel(r'Stretch $\\lambda [-]$')\n",
    "        axi.set_ylabel(r'Nominal stress $P_{11} [MPa]$')\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P11_UT(lamb, model, normalization):\n",
    "    I1_factor = normalization[0]\n",
    "    I2_factor = normalization[1]\n",
    "    Psi1_factor = normalization[2]\n",
    "    Psi2_factor = normalization[3]\n",
    "    I1 = lamb**2 + 2/lamb\n",
    "    I2 = 2*lamb + 1/lamb**2\n",
    "    I1norm = (I1-3)/I1_factor\n",
    "    I2norm = (I2-3)/I2_factor\n",
    "    Psi1 = model.Psi1norm(I1norm)*Psi1_factor\n",
    "    Psi2 = model.Psi2norm(I2norm)*Psi2_factor\n",
    "    return 2*(Psi1 + Psi2/lamb)*(lamb-1/lamb**2)\n",
    "def P11_ET(lamb, model, normalization):\n",
    "    I1_factor = normalization[0]\n",
    "    I2_factor = normalization[1]\n",
    "    Psi1_factor = normalization[2]\n",
    "    Psi2_factor = normalization[3]\n",
    "    I1 = 2*lamb**2 + 1/lamb**4\n",
    "    I2 = lamb**4 + 2/lamb**2\n",
    "    I1norm = (I1-3)/I1_factor\n",
    "    I2norm = (I2-3)/I2_factor\n",
    "    Psi1 = model.Psi1norm(I1norm)*Psi1_factor\n",
    "    Psi2 = model.Psi2norm(I2norm)*Psi2_factor\n",
    "    return 2*(Psi1 + Psi2*lamb**2)*(lamb-1/lamb**5)\n",
    "def P11_PS(lamb, model, normalization):\n",
    "    I1_factor = normalization[0]\n",
    "    I2_factor = normalization[1]\n",
    "    Psi1_factor = normalization[2]\n",
    "    Psi2_factor = normalization[3]\n",
    "    I1 =  lamb**2 + 1/lamb**2 + 1\n",
    "    I2 =  lamb**2 + 1/lamb**2 + 1\n",
    "    I1norm = (I1-3)/I1_factor\n",
    "    I2norm = (I2-3)/I2_factor\n",
    "    Psi1 = model.Psi1norm(I1norm)*Psi1_factor\n",
    "    Psi2 = model.Psi2norm(I2norm)*Psi2_factor\n",
    "    return 2*(Psi1+Psi2)*(lamb-1/lamb**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eval invariants for the three types of deformation\n",
    "def evalI1_UT(lam):\n",
    "    return lam**2 + (2/lam)\n",
    "def evalI2_UT(lam):\n",
    "    return 2*lam + (1/lam**2)\n",
    "\n",
    "def evalI1_ET(lam):\n",
    "    return 2*lam**2 +1/lam**4\n",
    "def evalI2_ET(lam):\n",
    "    return lam**4 + 2/lam**2\n",
    "\n",
    "def evalI1_PS(lam):\n",
    "    return lam**2 + 1/lam**2 + 1\n",
    "def evalI2_PS(lam):\n",
    "    return lam**2 + 1/lam**2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "UTdata = pd.read_csv('Data/UT20.csv')\n",
    "ETdata = pd.read_csv('Data/ET20.csv')\n",
    "PSdata = pd.read_csv('Data/PS20.csv')\n",
    "# stack into single array \n",
    "P11_data = np.hstack([UTdata['P11'].to_numpy(),ETdata['P11'].to_numpy(),PSdata['P11'].to_numpy()])\n",
    "F11_data = np.hstack([UTdata['F11'].to_numpy(),ETdata['F11'].to_numpy(),PSdata['F11'].to_numpy()])\n",
    "# indices for the three data sets\n",
    "indET = len(UTdata['P11'])\n",
    "indPS = indET + len(ETdata['P11'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "I1_factor = 30\n",
    "Psi1_factor = 0.3\n",
    "I2_factor = 250\n",
    "Psi2_factor = 0.001\n",
    "normalization = [I1_factor,I2_factor,Psi1_factor,Psi2_factor ]\n",
    "\n",
    "@partial(jit, static_argnums=(2,))\n",
    "def loss_P11_all(params, F11_data, mdlnumber):\n",
    "    lamUT = F11_data[0:indET]\n",
    "    lamET = F11_data[indET:indPS]\n",
    "    lamPS = F11_data[indPS:]\n",
    "    params_I1 = params[0]\n",
    "    params_I2 = params[1]\n",
    "    if mdlnumber == 1:\n",
    "        model = CANN_model(params_I1,params_I2,normalization)\n",
    "    elif mdlnumber == 2:\n",
    "        model = ICNN_model(params_I1,params_I2,normalization)\n",
    "    else:\n",
    "        model = NODE_model(params_I1,params_I2)\n",
    "    P11UT_pr = P11_UT(lamUT, model, normalization)\n",
    "    P11ET_pr = P11_ET(lamET, model, normalization)\n",
    "    P11PS_pr = P11_PS(lamPS, model, normalization)    \n",
    "    return np.mean((P11UT_pr-P11_data[0:indET])**2)+np.mean((P11ET_pr-P11_data[indET:indPS])**2)+np.mean((P11PS_pr-P11_data[indPS:])**2)\n",
    "\n",
    "@partial(jit, static_argnums=(2,))\n",
    "def loss_P11_UT(params, F11_data, mdlnumber):\n",
    "    lamUT = F11_data[0:indET]\n",
    "    params_I1 = params[0]\n",
    "    params_I2 = params[1]\n",
    "    if mdlnumber == 1:\n",
    "        model = CANN_model(params_I1,params_I2,normalization)\n",
    "    elif mdlnumber == 2:\n",
    "        model = ICNN_model(params_I1,params_I2,normalization)\n",
    "    else:\n",
    "        model = NODE_model(params_I1,params_I2)\n",
    "    P11UT_pr = P11_UT(lamUT, model, normalization)\n",
    "    return np.mean((P11UT_pr-P11_data[0:indET])**2)\n",
    "\n",
    "@partial(jit, static_argnums=(2,))\n",
    "def loss_P11_ET(params, F11_data, mdlnumber):\n",
    "    lamET = F11_data[indET:indPS]\n",
    "    params_I1 = params[0]\n",
    "    params_I2 = params[1]\n",
    "    if mdlnumber == 1:\n",
    "        model = CANN_model(params_I1,params_I2,normalization)\n",
    "    elif mdlnumber == 2:\n",
    "        model = ICNN_model(params_I1,params_I2,normalization)\n",
    "    else:\n",
    "        model = NODE_model(params_I1,params_I2)\n",
    "    P11ET_pr = P11_ET(lamET, model, normalization)\n",
    "    return np.mean((P11ET_pr-P11_data[indET:indPS])**2)\n",
    "\n",
    "@partial(jit, static_argnums=(2,))\n",
    "def loss_P11_PS(params, F11_data, mdlnumber):\n",
    "    lamPS = F11_data[indPS:]\n",
    "    params_I1 = params[0]\n",
    "    params_I2 = params[1]\n",
    "    if mdlnumber == 1:\n",
    "        model = CANN_model(params_I1,params_I2,normalization)\n",
    "    elif mdlnumber == 2:\n",
    "        model = ICNN_model(params_I1,params_I2,normalization)\n",
    "    else:\n",
    "        model = NODE_model(params_I1,params_I2)\n",
    "    P11PS_pr = P11_PS(lamPS, model, normalization)    \n",
    "    return np.mean((P11PS_pr-P11_data[indPS:])**2)\n",
    "\n",
    "\n",
    "@partial(jit, static_argnums=(0,1,))\n",
    "def step_jp(loss, mdlnumber, i, opt_state, X_batch):\n",
    "    params = get_params(opt_state)\n",
    "    g = grad(loss)(params, X_batch, mdlnumber)\n",
    "    return opt_update(i, g, opt_state)\n",
    "\n",
    "def train_jp(loss, mdlnumber, X, opt_state, key, nIter = 10000, print_freq = 1000):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for it in range(nIter):\n",
    "        opt_state = step_jp(loss, mdlnumber, it, opt_state, X)         \n",
    "        if (it+1)% print_freq == 0 or it==0 or it==nIter:\n",
    "            params = get_params(opt_state)\n",
    "            train_loss_value = loss(params, X, mdlnumber)\n",
    "            train_loss.append(train_loss_value)\n",
    "            to_print = \"it %i, train loss = %e\" % (it+1, train_loss_value)\n",
    "            print(to_print)\n",
    "    return get_params(opt_state), train_loss, val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_params_cann = 12\n",
    "n_params_icnn = 4 + 12*2 + 3*2 = 34 #Use layers= [1,4,3,1] for icnn #actually 32?\n",
    "n_params_node = 5 + 25 + 5 = 35 #Use layers = [1,5,5,1] \n",
    "\n",
    "\n",
    "What if we use [1,2,3,1] for node? Then use [1,4,1] for icnn\n",
    "n_params_cann = 12\n",
    "n_params_node = 2+6+3 = 11\n",
    "n_params_icnn = 4+2*4 = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_node(key, layers = [1,5,5,1]):\n",
    "    params_I1 = init_params_posb(layers, key) \n",
    "    params_I2 = init_params_posb(layers, key)\n",
    "    return [params_I1,params_I2]\n",
    "\n",
    "def init_icnn(key, layers = [1,3,4,1]):\n",
    "    params_I1 = init_params_icnn(layers, key)\n",
    "    params_I2 = init_params_icnn(layers, key)\n",
    "    return [params_I1,params_I2]\n",
    "\n",
    "def init_cann(key, layers = None):\n",
    "    params_I1 = init_params_cann(key)\n",
    "    params_I2 = init_params_cann(key)\n",
    "    return [params_I1,params_I2]\n",
    "\n",
    "def n_params_node(layers=[1,5,5,1]):\n",
    "    n = 0\n",
    "    for i in range(len(layers)-1):\n",
    "        n+= layers[i]*layers[i+1]\n",
    "    return n*2\n",
    "\n",
    "def n_params_icnn(layers=[1,4,3,1]):\n",
    "    n = 0\n",
    "    n += layers[0]*layers[1] + layers[1]\n",
    "    for i in range(1,len(layers)-1):\n",
    "        n+= layers[i]*layers[i+1] #Wz\n",
    "        n+= layers[0]*layers[i+1] #Wy\n",
    "        n+= layers[i+1] #b\n",
    "    return n*2\n",
    "\n",
    "def opt_arch_node(n): # return the \"sorta\" best architecture given # of parameters\n",
    "    archs = [[1,2,2,1],[1,3,3,1],[1,4,4,1],[1,5,5,1],[1,4,4,4,1],[1,5,5,5,1],[1,6,6,6,1],[1,6,6,6,6,1]]\n",
    "    m = onp.array([16, 30, 48, 70, 80, 120, 168, 240])\n",
    "\n",
    "    idx = (onp.abs(m-n)).argmin()\n",
    "    nominal_arch = archs[idx]\n",
    "    closest_arch = archs[idx]\n",
    "    closest_m = m[idx]\n",
    "\n",
    "    diff = onp.abs(closest_m-n)\n",
    "    for i in range(len(nominal_arch)-2):\n",
    "        for j in [-1,0,1]:\n",
    "            arch = onp.copy(nominal_arch)\n",
    "            arch[i+1]+= j\n",
    "            m = n_params_node(arch)\n",
    "            if np.abs(m-n)<diff:\n",
    "                diff = onp.abs(m-n)\n",
    "                closest_arch = onp.copy(arch)\n",
    "                closest_m = m\n",
    "    return closest_m, closest_arch\n",
    "\n",
    "\n",
    "def opt_arch_icnn(n): # return the \"sorta\" best architecture given # of parameters\n",
    "    archs = [[1,2,1],[1,3,1],[1,4,1],[1,2,2,1],[1,3,3,1],[1,4,4,1],[1,5,5,1],[1,4,4,4,1],[1,5,5,5,1],[1,6,6,6,1],[1,6,6,6,6,1]]\n",
    "    m = onp.array([16, 22, 28, 32, 52, 76, 104, 124, 174, 232, 328])\n",
    "\n",
    "    idx = (onp.abs(m-n)).argmin()\n",
    "    nominal_arch = archs[idx]\n",
    "    closest_arch = archs[idx]\n",
    "    closest_m = m[idx]\n",
    "\n",
    "    diff = onp.abs(closest_m-n)\n",
    "    for i in range(len(nominal_arch)-2):\n",
    "        for j in [-1,0,1]:\n",
    "            arch = onp.copy(nominal_arch)\n",
    "            arch[i+1]+= j\n",
    "            m = n_params_icnn(arch)\n",
    "            if np.abs(m-n)<diff:\n",
    "                diff = onp.abs(m-n)\n",
    "                closest_arch = onp.copy(arch)\n",
    "                closest_m = m\n",
    "    return closest_m, closest_arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50 runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 1, train loss = 8.404907e+00\n",
      "0.05193170387154177 0.052095787796723664 0.025198639797350313\n",
      "it 1, train loss = 1.326932e+06\n",
      "0.045904429565109275 0.05320992831410337 0.025581879337100575\n",
      "it 1, train loss = 6.712172e+00\n",
      "0.05193175685103336 0.05209580498840594 0.02519861352083013\n",
      "it 1, train loss = 7.790573e+00\n",
      "0.0519347446486862 0.052095797240054496 0.025197279820158076\n",
      "it 1, train loss = 3.801197e+00\n",
      "0.045654749100684386 0.05320483279728786 0.0255860475354747\n",
      "it 1, train loss = 7.855335e+00\n",
      "0.046947120848907994 0.053178546173881465 0.02564241808621302\n",
      "it 1, train loss = 1.653415e+02\n",
      "0.05193167458927169 0.052095843857941734 0.025198638166204785\n",
      "it 1, train loss = 5.326038e+00\n",
      "0.04769043587890498 0.054257354609790025 0.024956621718624235\n",
      "it 1, train loss = 1.198337e+13\n",
      "0.05441523382011651 0.05926188021826067 0.03356219927057632\n",
      "it 1, train loss = 5.524966e+06\n",
      "0.04549461034216314 0.05320417358443523 0.02558577165333445\n",
      "it 1, train loss = 1.439198e+05\n",
      "0.047751964504588906 0.05322673930648628 0.02553641239536044\n",
      "it 1, train loss = 7.008740e+00\n",
      "0.051931199707053295 0.05209580133405689 0.025198852850086634\n",
      "it 1, train loss = 1.264380e+01\n",
      "0.048335771747233636 0.05597714418704733 0.026700092063865248\n",
      "it 1, train loss = 1.837173e+00\n",
      "0.045917806007825884 0.05325030247166193 0.025532938273013388\n",
      "it 1, train loss = 9.768751e+00\n",
      "0.05193167359048049 0.052095845780763664 0.025198638107700906\n",
      "it 1, train loss = 8.376907e+00\n",
      "0.04515050453760711 0.05322739231873136 0.025539785235307276\n",
      "it 1, train loss = 1.068678e+01\n",
      "0.0519316976482226 0.052095799648750815 0.025198639451468722\n",
      "it 1, train loss = 1.084365e+01\n",
      "0.05193296806884783 0.05209576771228068 0.02519810587241322\n",
      "it 1, train loss = 1.086664e+01\n",
      "0.05193171571216732 0.052095792091363245 0.02519863371778969\n",
      "it 1, train loss = 1.003771e+01\n",
      "0.051931672871569176 0.05209584705769812 0.0251986380975131\n",
      "it 1, train loss = 9.442838e+00\n",
      "0.05193284019796411 0.052095764976221204 0.02519816698944995\n",
      "it 1, train loss = 6.780192e+00\n",
      "0.05193216350024618 0.052095735403028226 0.025198446680547437\n",
      "it 1, train loss = 4.075341e+02\n",
      "0.04633025237672072 0.053239482921564656 0.025559082541188827\n",
      "it 1, train loss = 6.721448e+10\n",
      "0.04567303525317201 0.05321224225703401 0.025611904118726547\n",
      "it 1, train loss = 6.323711e+00\n",
      "0.05193169521578724 0.05209580435322671 0.025198639331701557\n",
      "it 1, train loss = 4.030004e+00\n",
      "0.051931723145228224 0.05209578421158056 0.025198632575132354\n",
      "it 1, train loss = 9.917313e+00\n",
      "0.051931699010732225 0.052095797062681325 0.025198639527590487\n",
      "it 1, train loss = 8.499724e+00\n",
      "0.05193169288079009 0.052095783719767516 0.02519864562907525\n",
      "it 1, train loss = 7.839172e+00\n",
      "0.046087730934897486 0.05323466191723401 0.02555744049673227\n",
      "it 1, train loss = 1.588573e+02\n",
      "0.051962658963039615 0.052095550388532266 0.02518533171717835\n",
      "it 1, train loss = 3.980017e+00\n",
      "0.0519317080283926 0.05209577974829319 0.025198640035332003\n",
      "it 1, train loss = 3.928053e+00\n",
      "0.051942323695183 0.05209563317843563 0.025194165878220716\n",
      "it 1, train loss = 1.677313e+01\n",
      "0.051931642052406454 0.05209590637278922 0.02519863638347359\n",
      "it 1, train loss = 1.786098e+03\n",
      "0.0523246882090344 0.05170592519262526 0.025290350694699322\n",
      "it 1, train loss = 8.873269e+00\n",
      "0.05193172628330644 0.052095745902238144 0.025198640745972372\n",
      "it 1, train loss = 9.792637e+00\n",
      "0.047721726825606554 0.05411095839534449 0.024946039941010658\n",
      "it 1, train loss = 8.319005e+00\n",
      "0.0481950298512101 0.055917480938937086 0.026875441581047137\n",
      "it 1, train loss = 1.980579e+08\n",
      "0.04820501252670034 0.05222764330972502 0.027559786944802326\n",
      "it 1, train loss = 4.777524e+00\n",
      "0.05193169895310778 0.05209580337486786 0.025198637994101148\n",
      "it 1, train loss = 3.985013e+00\n",
      "0.05193175456896972 0.05209575753309659 0.025198625957747238\n",
      "it 1, train loss = 7.958517e+00\n",
      "0.05193172497480475 0.05209574724342641 0.025198640980005942\n",
      "it 1, train loss = 1.293624e+02\n",
      "0.04596771855537529 0.05322548885603119 0.02557740152308865\n",
      "it 1, train loss = 1.255889e+05\n",
      "0.04587208231286212 0.05321142449235828 0.02559229713036032\n",
      "it 1, train loss = 1.023887e+01\n",
      "0.051931629866125206 0.05209592954329471 0.025198635751542853\n",
      "it 1, train loss = 9.567712e+00\n",
      "0.05193171269615382 0.05209577080329746 0.025198640300824627\n",
      "it 1, train loss = 1.077213e+01\n",
      "0.051931700229036794 0.05209579549032323 0.025198639407582362\n",
      "it 1, train loss = 4.637776e+00\n",
      "0.05193166935508118 0.05209577047312098 0.02519865846927228\n",
      "it 1, train loss = 9.353661e+00\n",
      "0.05193211616804186 0.05209570779334724 0.02519849482728915\n",
      "it 1, train loss = 1.055425e+01\n",
      "0.04575931070421466 0.05321140246250881 0.025596507732547995\n",
      "it 1, train loss = 1.594917e+09\n",
      "0.04668609448445142 0.05222978181376564 0.027553525022339154\n"
     ]
    }
   ],
   "source": [
    "lamUT_vec = np.array(UTdata['F11'])\n",
    "lamET_vec = np.array(ETdata['F11'])\n",
    "lamPS_vec = np.array(PSdata['F11'])\n",
    "r2_list = []\n",
    "mae_list = []\n",
    "for i in range(50): #50 runs for every architecture\n",
    "    key, subkey = random.split(key)\n",
    "    params = init_cann(key)\n",
    "\n",
    "    opt_init, opt_update, get_params = optimizers.adam(2.e-4) #Original: 1.e-4\n",
    "    opt_state = opt_init(params)\n",
    "    params, train_loss, val_loss = train_jp(loss_P11_all, 1, F11_data, opt_state, key, nIter = 100000, print_freq=1000000)\n",
    "\n",
    "    model = CANN_model(params[0], params[1], normalization)\n",
    "\n",
    "    lambdas     = [lamUT_vec,       lamET_vec,      lamPS_vec       ]\n",
    "    P11_gt_list = [UTdata['P11'],   ETdata['P11'],  PSdata['P11']   ]\n",
    "    P11funs     = [P11_UT,          P11_ET,         P11_PS          ]\n",
    "\n",
    "    r2 = []\n",
    "    mae = []\n",
    "    for lam, P11_gt, P11fun in zip(lambdas, P11_gt_list, P11funs):\n",
    "        P11 = P11fun(lam, model, normalization)\n",
    "        r2i = r2_score(P11_gt, P11)\n",
    "        r2i = np.clip(r2i, a_min=0.0, a_max=1.0)\n",
    "        r2.append(r2i)\n",
    "\n",
    "        maei = onp.mean(onp.abs(P11_gt-P11))\n",
    "        mae.append(maei)\n",
    "    # r2 = np.mean(np.array(r2))\n",
    "    print(*mae)\n",
    "    r2_list.append(r2)\n",
    "    mae_list.append(mae)\n",
    "cann_r2 = np.array(r2_list)\n",
    "cann_mae = np.array(mae_list)\n",
    "\n",
    "# cann_r2 = np.expand_dims(cann_r2,axis=1)\n",
    "# cann_mae = np.expand_dims(cann_mae,axis=1)\n",
    "with open('savednet/CANN_r2_efficiency_ALL.npy', 'wb') as f:\n",
    "    pickle.dump([[24], cann_r2], f)\n",
    "\n",
    "with open('savednet/CANN_mae_efficiency_ALL.npy', 'wb') as f:\n",
    "    pickle.dump([[24], cann_mae], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 [1 1 2 1]\n",
      "it 1, train loss = 2.659942e+00\n",
      "0.07873514637541149 0.04594043029643912 0.027139178557338946\n",
      "it 1, train loss = 7.676506e-01\n",
      "0.07886734892642665 0.04588603286749532 0.027166190152970178\n",
      "it 1, train loss = 2.133252e+00\n",
      "0.07884234551596593 0.04586640439439101 0.027157435718781042\n",
      "it 1, train loss = 2.656305e-01\n",
      "0.07885668130266309 0.04586447314336738 0.027161783178000953\n",
      "it 1, train loss = 1.084453e+00\n",
      "0.07978586421591409 0.045270535972084386 0.02739330749851256\n",
      "it 1, train loss = 1.495428e-01\n",
      "0.07976081517948447 0.045285372001262404 0.027383290978014636\n",
      "it 1, train loss = 5.954160e-01\n",
      "0.07885773374328182 0.04587912980040447 0.02716337951533711\n",
      "it 1, train loss = 1.112935e+00\n",
      "0.058872287030178716 0.04985842578821872 0.029872886990288734\n",
      "it 1, train loss = 1.004365e+00\n",
      "0.07882447623978192 0.04588872655775398 0.027154534933698602\n",
      "it 1, train loss = 2.845121e+00\n",
      "0.078730640313736 0.04594115315875976 0.02713903329493319\n",
      "it 1, train loss = 2.837800e+00\n",
      "0.07880040061276893 0.04590307360367347 0.02714619781696991\n",
      "it 1, train loss = 1.578439e-01\n",
      "0.07887276029479715 0.04585197666294576 0.027166127843518174\n",
      "it 1, train loss = 1.871905e+00\n",
      "0.07878364223038373 0.045924487914028436 0.027144633106057938\n",
      "it 1, train loss = 7.729144e-01\n",
      "0.0788246101126146 0.045887343006211875 0.02715296854120276\n",
      "it 1, train loss = 1.364673e+00\n",
      "0.0788405003273952 0.045873228183055954 0.027157211378220216\n",
      "it 1, train loss = 1.213139e+00\n",
      "0.058857479269937556 0.04987171422119294 0.029881046835794765\n",
      "it 1, train loss = 1.908830e+00\n",
      "0.07881372478023203 0.04589879057215013 0.027150139174660397\n",
      "it 1, train loss = 2.453508e+00\n",
      "0.07979400160413914 0.04527301220952491 0.027397974404902083\n",
      "it 1, train loss = 4.619594e-01\n",
      "0.0798213491822667 0.04526794056432751 0.027405259411390888\n",
      "it 1, train loss = 7.979341e-02\n",
      "0.05885686751931088 0.04985333104176385 0.029868709179719198\n",
      "it 1, train loss = 2.763314e+00\n",
      "0.07881273691527349 0.04589556653145658 0.027149675336170593\n",
      "it 1, train loss = 2.337141e+00\n",
      "0.058874424501214495 0.04987227040073146 0.02987516784462823\n",
      "it 1, train loss = 2.639106e+00\n",
      "0.07880240217762177 0.0459072061636362 0.027147015386530127\n",
      "it 1, train loss = 3.182854e-01\n",
      "0.07885596973759332 0.045862580781838067 0.027161473195395722\n",
      "it 1, train loss = 6.828285e-01\n",
      "0.07882694365237336 0.045887077520815334 0.02715367876299189\n",
      "it 1, train loss = 2.090569e+00\n",
      "0.07882238513945423 0.045884915278805616 0.02715213840635285\n",
      "it 1, train loss = 1.410783e-01\n",
      "0.058850949654935904 0.04985861806598875 0.029872968669082473\n",
      "it 1, train loss = 2.257963e+00\n",
      "0.07878475974191693 0.04591009622334345 0.02714167022359464\n",
      "it 1, train loss = 1.478138e+00\n",
      "0.07879751144163141 0.045915093706989596 0.027145863356512097\n",
      "it 1, train loss = 1.305442e-01\n",
      "0.0788826144495631 0.04584141532086948 0.027168604973502968\n",
      "it 1, train loss = 6.047311e-01\n",
      "0.07883223618402024 0.04588266564132403 0.02715510625919022\n",
      "it 1, train loss = 5.443234e-01\n",
      "0.07883124095576885 0.045870227573413136 0.02715540763011367\n",
      "it 1, train loss = 5.985558e-01\n",
      "0.07883949878924126 0.04586346014374991 0.027156425297700394\n",
      "it 1, train loss = 7.265717e-01\n",
      "0.07975610196831336 0.045305476940469123 0.027400704998123488\n",
      "it 1, train loss = 9.359298e-01\n",
      "0.07888539145747496 0.04587283585270555 0.027171143498700537\n",
      "it 1, train loss = 8.492026e-01\n",
      "0.079768649777749 0.045292137809433794 0.027387244065181986\n",
      "it 1, train loss = 7.487762e-01\n",
      "0.0798479345396859 0.045221926492677124 0.027412863198046446\n",
      "it 1, train loss = 5.573079e-01\n",
      "0.07982611625686101 0.04526279953154577 0.027407041599761782\n",
      "it 1, train loss = 2.316142e+00\n",
      "0.07881333295531982 0.04589713940972567 0.027149934769639672\n",
      "it 1, train loss = 4.469502e-01\n",
      "0.07884333134429844 0.04587532936944588 0.027158191153481375\n",
      "it 1, train loss = 1.984761e+00\n",
      "0.07881652613981313 0.045902581732860444 0.027148189825067465\n",
      "it 1, train loss = 1.409109e+00\n",
      "0.07880231681035553 0.045906576909254544 0.027146985534696317\n",
      "it 1, train loss = 2.078602e+00\n",
      "0.07978287263375351 0.04528896947567709 0.02739207588439518\n",
      "it 1, train loss = 1.884870e+00\n",
      "0.07881255760750318 0.045903988432891576 0.027150025376339613\n",
      "it 1, train loss = 3.028673e-01\n",
      "0.07890084816151716 0.04585059260625528 0.02717473532832515\n",
      "it 1, train loss = 9.083875e-02\n",
      "0.0797339054346034 0.04530179156407432 0.027372998957938117\n",
      "it 1, train loss = 6.612618e-01\n",
      "0.07882485466313531 0.045879084917733494 0.027152645749115127\n",
      "it 1, train loss = 1.517782e+00\n",
      "0.07881080116583658 0.045901012295633634 0.027149334139393775\n",
      "it 1, train loss = 1.480004e+00\n",
      "0.07883383057607961 0.04587097298716131 0.027155035360361398\n",
      "it 1, train loss = 8.686017e-01\n",
      "0.078829177969108 0.04588179402006484 0.027154135034479252\n",
      "16 [1, 2, 2, 1]\n",
      "it 1, train loss = 2.596330e+00\n",
      "0.04826674774755252 0.05147364567510501 0.022811726771933005\n",
      "it 1, train loss = 8.281521e-01\n",
      "0.05334431097997788 0.04986088689877547 0.0251264129284376\n",
      "it 1, train loss = 6.875392e-01\n",
      "0.04775653008334753 0.051391830694997656 0.022602558400143916\n",
      "it 1, train loss = 1.154236e+00\n",
      "0.0532699752352562 0.04977441041513624 0.02504696531133476\n",
      "it 1, train loss = 2.719570e+00\n",
      "0.07961228068303461 0.04526724330558305 0.0273096458040139\n",
      "it 1, train loss = 1.826130e-01\n",
      "0.05792555077647883 0.048442629695585146 0.028198092839887362\n",
      "it 1, train loss = 4.788306e-01\n",
      "0.05860316442992728 0.049298984131948984 0.029284717371841674\n",
      "it 1, train loss = 1.420630e+00\n",
      "0.05325199913973808 0.04989796616770311 0.02512530918280345\n",
      "it 1, train loss = 2.078209e+00\n",
      "0.048102566634402884 0.05151118340508309 0.022882785545609845\n",
      "it 1, train loss = 5.938252e-01\n",
      "0.07961498407507413 0.0453766615559096 0.02731490754361831\n",
      "it 1, train loss = 2.935693e+00\n",
      "0.07963151721813244 0.04536683920280737 0.027321494334015876\n",
      "it 1, train loss = 2.091844e+00\n",
      "0.048035356926005364 0.05136652398134097 0.022555761519915254\n",
      "it 1, train loss = 9.048428e-01\n",
      "0.07961860138091428 0.045378854336150354 0.02731644624598836\n",
      "it 1, train loss = 1.621942e+00\n",
      "0.05336862723379347 0.049847735863369755 0.025082040334880564\n",
      "it 1, train loss = 2.114689e-01\n",
      "0.04758625079820294 0.051519048975909 0.022616959743472383\n",
      "it 1, train loss = 1.459635e+00\n",
      "0.04993099617069387 0.051589082530673076 0.02400750691280935\n",
      "it 1, train loss = 7.667699e-02\n",
      "0.07960536319252118 0.04538482626817793 0.027310249876705956\n",
      "it 1, train loss = 2.717137e+00\n",
      "0.0796287271229971 0.045376115512614786 0.027320492645947138\n",
      "it 1, train loss = 1.306267e+00\n",
      "0.047678718013258264 0.05154774497470328 0.02264850633079563\n",
      "it 1, train loss = 3.528270e-01\n",
      "0.053216713301158874 0.049817713734864516 0.025077419530077454\n",
      "it 1, train loss = 4.248677e-01\n",
      "0.048076981005081655 0.05142008778753534 0.022625164275258014\n",
      "it 1, train loss = 1.748827e+00\n",
      "0.05018839886884988 0.05165045568539056 0.024464842688015746\n",
      "it 1, train loss = 4.041207e-01\n",
      "0.05334548228221633 0.04984784838682804 0.025056442951502715\n",
      "it 1, train loss = 2.162832e-01\n",
      "0.07961231501616017 0.04538393734912257 0.02731356723387875\n",
      "it 1, train loss = 2.527974e-01\n",
      "0.0795954614952184 0.04527739173500106 0.02730284188452396\n",
      "it 1, train loss = 4.896505e-01\n",
      "0.07962993565586113 0.045358348352201226 0.027320329744247134\n",
      "it 1, train loss = 1.674686e+00\n",
      "0.05367620268078441 0.04991059119355908 0.025190439522328918\n",
      "it 1, train loss = 2.075292e+00\n",
      "0.053246803698496464 0.049800952083323094 0.025000465733338667\n",
      "it 1, train loss = 2.198327e+00\n",
      "0.0534179191907495 0.0498481138532688 0.025151080132166406\n",
      "it 1, train loss = 7.487280e-01\n",
      "0.07959155801913792 0.045280080058963876 0.027301685190543348\n",
      "it 1, train loss = 1.757786e+00\n",
      "0.07959697784286686 0.04527674225575016 0.027303846378808474\n",
      "it 1, train loss = 2.196766e+00\n",
      "0.048054967016875505 0.051487126879042736 0.02277210306879276\n",
      "it 1, train loss = 2.095607e+00\n",
      "0.07824517094253233 0.04628958474243278 0.02698005636943739\n",
      "it 1, train loss = 2.019434e+00\n",
      "0.07956780574247937 0.045301243684652916 0.02729748464411742\n",
      "it 1, train loss = 8.224175e-01\n",
      "0.057891606447574614 0.04848908708819798 0.02820938447440095\n",
      "it 1, train loss = 1.181842e+00\n",
      "0.07962335713065684 0.045376727759647385 0.02731810771318683\n",
      "it 1, train loss = 2.774698e+00\n",
      "0.07959495994692162 0.04527798745148898 0.027303097026795776\n",
      "it 1, train loss = 2.900626e+00\n",
      "0.07963569396565086 0.04536849188942339 0.027323124932083522\n",
      "it 1, train loss = 3.796228e-01\n",
      "0.04786510881422387 0.051471671843615625 0.022597838301790057\n",
      "it 1, train loss = 2.753241e+00\n",
      "0.07959980742282462 0.0452750993449613 0.02730504954831794\n",
      "it 1, train loss = 1.561435e+00\n",
      "0.04796851436556834 0.05148440953074301 0.02258574560378713\n",
      "it 1, train loss = 1.723531e+00\n",
      "0.05779875655665899 0.04839239403018942 0.028120463076886194\n",
      "it 1, train loss = 1.828594e-01\n",
      "0.07962397286245185 0.0453624963526138 0.02731777730669935\n",
      "it 1, train loss = 2.795289e+00\n",
      "0.07960575500657732 0.045270806461648015 0.027306789830523166\n",
      "it 1, train loss = 1.442072e+00\n",
      "0.053280852201996326 0.0498345782365875 0.024993230248493543\n",
      "it 1, train loss = 1.137160e-01\n",
      "0.05301408288054612 0.04992080845826026 0.02495821673193085\n",
      "it 1, train loss = 2.768512e+00\n",
      "0.0795700662669656 0.04530032555978929 0.027298793284829533\n",
      "it 1, train loss = 5.462782e-01\n",
      "0.0796083724693874 0.04537851243856144 0.02731215133364009\n",
      "it 1, train loss = 1.425613e+00\n",
      "0.05808954548348776 0.04865429173035857 0.02849385791007025\n",
      "it 1, train loss = 2.161152e+00\n",
      "0.07825154068597039 0.04628519849110916 0.02698193718626309\n",
      "22 [1 3 2 1]\n",
      "it 1, train loss = 2.701753e+00\n",
      "0.047511568516281806 0.0514913306510335 0.02266502902113673\n",
      "it 1, train loss = 1.880722e+00\n",
      "0.0795640727191817 0.04529376635724156 0.02728622736502417\n",
      "it 1, train loss = 1.740264e-01\n",
      "0.04738115979964125 0.05147282044733198 0.0226917550640946\n",
      "it 1, train loss = 2.095797e-01\n",
      "0.057673904093645374 0.048448129235461 0.02816037375342196\n",
      "it 1, train loss = 3.928630e-01\n",
      "0.053628406933308226 0.04966065158720721 0.025288055498122804\n",
      "it 1, train loss = 1.251180e+00\n",
      "0.07959942899216553 0.04538257548915599 0.02730379863631151\n",
      "it 1, train loss = 1.293032e+00\n",
      "0.047582502893029115 0.05119294811098663 0.02280999848303376\n",
      "it 1, train loss = 1.714623e-01\n",
      "0.0795868300489262 0.04539015728154669 0.027298535297926363\n",
      "it 1, train loss = 8.098977e-01\n",
      "0.04750547841585144 0.05153297855051325 0.02273163761226835\n",
      "it 1, train loss = 9.572576e-01\n",
      "0.04723233331038154 0.05157656151623499 0.022853258655547968\n",
      "it 1, train loss = 6.603640e-01\n",
      "0.07958892562845189 0.04538524518881944 0.02729961381476082\n",
      "it 1, train loss = 2.014613e+00\n",
      "0.047628468155954166 0.05152296187538208 0.022721440092484557\n",
      "it 1, train loss = 1.941578e+00\n",
      "0.07957206400618759 0.04528843211700868 0.02728912944222684\n",
      "it 1, train loss = 1.697728e+00\n",
      "0.07957150909664856 0.045287672036185625 0.02728828424508502\n",
      "it 1, train loss = 1.134968e+00\n",
      "0.04734490617591192 0.05155183071133809 0.02276802029408407\n",
      "it 1, train loss = 6.591977e-01\n",
      "0.048111192880794934 0.05132756947443109 0.022540577235951684\n",
      "it 1, train loss = 4.964285e-01\n",
      "0.07956404182570394 0.04529309365991238 0.027285685164473588\n",
      "it 1, train loss = 1.915245e+00\n",
      "0.07958794771834327 0.04540084075234957 0.02729969399750729\n",
      "it 1, train loss = 2.024999e-01\n",
      "0.0795621851750483 0.0452952310374816 0.027285402025657577\n",
      "it 1, train loss = 2.503560e+00\n",
      "0.04760456098253325 0.05155587766351029 0.022665483794140427\n",
      "it 1, train loss = 5.073900e-01\n",
      "0.07957146533024449 0.045288653170667346 0.027288689728078994\n",
      "it 1, train loss = 2.542712e-01\n",
      "0.04754845770782077 0.05159546159150458 0.02283083574501405\n",
      "it 1, train loss = 2.629822e-01\n",
      "0.07643651408508297 0.04721140757415626 0.0257440271693438\n",
      "it 1, train loss = 2.584065e+00\n",
      "0.07956347011428035 0.045293337591154045 0.027285543429517802\n",
      "it 1, train loss = 2.318503e-01\n",
      "0.07955643517610421 0.045297473384234496 0.027282329423275634\n",
      "it 1, train loss = 2.683948e+00\n",
      "0.04734257980137495 0.051511391978552354 0.022697627671194962\n",
      "it 1, train loss = 1.104641e+00\n",
      "0.047393652793959164 0.05149199507981057 0.022684652825850582\n",
      "it 1, train loss = 5.150102e-01\n",
      "0.07956322990678538 0.04529356120843509 0.027285356778429904\n",
      "it 1, train loss = 4.420222e-01\n",
      "0.047289850782821136 0.05151674646856296 0.02272216508707708\n",
      "it 1, train loss = 1.857490e-01\n",
      "0.04738338569418124 0.05146383326926444 0.02270068925333343\n",
      "it 1, train loss = 1.731939e-01\n",
      "0.04793840736741057 0.05123274200512624 0.022667523791928084\n",
      "it 1, train loss = 1.520881e+00\n",
      "0.07959050221115206 0.04539772972878612 0.027300589634373183\n",
      "it 1, train loss = 2.088589e-01\n",
      "0.057759196443957814 0.04851438850245077 0.028227614977786912\n",
      "it 1, train loss = 1.418696e+00\n",
      "0.07958357574746514 0.04540725328781928 0.027297855685578513\n",
      "it 1, train loss = 1.595046e+00\n",
      "0.0795885467338139 0.0453940741845652 0.027299743006162717\n",
      "it 1, train loss = 1.241297e+00\n",
      "0.07957308782728069 0.04528776864791428 0.02728946144913414\n",
      "it 1, train loss = 6.163847e-01\n",
      "0.0795776034244248 0.04527961595666894 0.02728809443018036\n",
      "it 1, train loss = 2.855550e+00\n",
      "0.07956388297749852 0.04529334446571447 0.02728587098127535\n",
      "it 1, train loss = 2.947613e-01\n",
      "0.048082782905364406 0.05134746238663424 0.022568156590513724\n",
      "it 1, train loss = 6.874234e-02\n",
      "0.048811087313550196 0.05139445337346972 0.022886624436405927\n",
      "it 1, train loss = 9.665455e-02\n",
      "0.05743558067572582 0.04831501124026022 0.02795582122170139\n",
      "it 1, train loss = 1.272003e+00\n",
      "0.052499309190114335 0.05043407896829738 0.02524938316186789\n",
      "it 1, train loss = 2.988713e+00\n",
      "0.07959588103841365 0.045401532009965134 0.027302619594262806\n",
      "it 1, train loss = 1.699854e+00\n",
      "0.07958660664535915 0.04539714817834174 0.02729897121718373\n",
      "it 1, train loss = 2.265843e+00\n",
      "0.07947843925480758 0.04538142805472551 0.027272408652239066\n",
      "it 1, train loss = 4.923293e-01\n",
      "0.05769429324982491 0.04848115560286394 0.028209603969135653\n",
      "it 1, train loss = 2.136095e+00\n",
      "0.07957074224036764 0.045289236841389434 0.027288627022268665\n",
      "it 1, train loss = 1.326688e+00\n",
      "0.04741239442126906 0.051641905685578256 0.022787778269153437\n",
      "it 1, train loss = 1.699572e+00\n",
      "0.05046226545031993 0.05132068111552248 0.024226366447014907\n",
      "it 1, train loss = 5.642706e-01\n",
      "0.07957032634880133 0.04528948911379896 0.027288343944593056\n",
      "30 [1, 3, 3, 1]\n",
      "it 1, train loss = 7.793884e-01\n",
      "0.052927590753258966 0.050044911535953286 0.025011038944474036\n",
      "it 1, train loss = 5.017152e-01\n",
      "0.04753884343110328 0.05109831478535455 0.02306982494444113\n",
      "it 1, train loss = 1.593229e+00\n",
      "0.05296800638746138 0.049985726244845395 0.02500729047385413\n",
      "it 1, train loss = 2.875996e+00\n",
      "0.04936291097825011 0.05122469268426239 0.02299949325029168\n",
      "it 1, train loss = 2.576194e+00\n",
      "0.05289796838493584 0.05008994142787381 0.025197753752809526\n",
      "it 1, train loss = 9.160176e-01\n",
      "0.052964452540156554 0.04992476826036292 0.024915784244020803\n",
      "it 1, train loss = 2.758364e+00\n",
      "0.07957572566821802 0.045336965152001106 0.027294816218271956\n",
      "it 1, train loss = 9.080267e-01\n",
      "0.05789372516896235 0.04866276292392624 0.028440238254925167\n",
      "it 1, train loss = 2.242510e+00\n",
      "0.0784156673457111 0.04605172843865518 0.02702095581692629\n",
      "it 1, train loss = 9.624427e-01\n",
      "0.04996955681439683 0.051358715609412735 0.02366429292400699\n",
      "it 1, train loss = 1.375080e+00\n",
      "0.050497126008966775 0.051392186177921226 0.024174144038533574\n",
      "it 1, train loss = 1.810866e+00\n",
      "0.04748332072475284 0.05128272408168149 0.02321998219804015\n",
      "it 1, train loss = 2.342512e+00\n",
      "0.047528327135664614 0.05111714985301079 0.02302328946501813\n",
      "it 1, train loss = 2.295570e+00\n",
      "0.053498966591196295 0.04932942155092931 0.02463593933823391\n",
      "it 1, train loss = 2.606311e+00\n",
      "0.07956601835320173 0.04529308153881603 0.02728813500376829\n",
      "it 1, train loss = 4.327176e-01\n",
      "0.048450930615768835 0.05124514910492244 0.022491329484349478\n",
      "it 1, train loss = 2.544402e+00\n",
      "0.053070070038119435 0.04988886533294604 0.024952071434020666\n",
      "it 1, train loss = 1.247493e+00\n",
      "0.047715550371163255 0.05070482596969578 0.023944672916727187\n",
      "it 1, train loss = 2.867883e-01\n",
      "0.05326789586408214 0.05012809174298244 0.025480723484127023\n",
      "it 1, train loss = 2.901881e+00\n",
      "0.07956027250434138 0.04529762964196213 0.027286687859081758\n",
      "it 1, train loss = 8.180427e-01\n",
      "0.04813081450994041 0.051449684510533576 0.022710827915439807\n",
      "it 1, train loss = 5.653132e-01\n",
      "0.04750572220661018 0.05118967178726419 0.022849030631967606\n",
      "it 1, train loss = 2.556291e+00\n",
      "0.053165532310907516 0.04997760856743267 0.025132866188366212\n",
      "it 1, train loss = 1.366135e+00\n",
      "0.04745867840279807 0.051212519263668065 0.022883805287742757\n",
      "it 1, train loss = 1.154651e+00\n",
      "0.07956696014399793 0.045292504847999064 0.027288486172091342\n",
      "it 1, train loss = 4.170224e-01\n",
      "0.047480466049287386 0.05108678470677789 0.02303220889840783\n",
      "it 1, train loss = 1.219509e+00\n",
      "0.051040670662695166 0.050955752362751086 0.023939809602108557\n",
      "it 1, train loss = 2.625677e+00\n",
      "0.04977178959258843 0.05110276862532738 0.02288428930507218\n",
      "it 1, train loss = 2.982503e+00\n",
      "0.047535556049867166 0.051194093326780384 0.022815974758476115\n",
      "it 1, train loss = 9.292211e-01\n",
      "0.04787897160061942 0.05134620986431878 0.02259678162210379\n",
      "it 1, train loss = 2.580032e+00\n",
      "0.04747966344912815 0.05117769804609527 0.022805319753205794\n",
      "it 1, train loss = 2.426418e+00\n",
      "0.04773215951587226 0.05049846701652281 0.024024745872673642\n",
      "it 1, train loss = 1.381824e+00\n",
      "0.04737305482773882 0.05131556535202114 0.02275146010481312\n",
      "it 1, train loss = 8.614922e-01\n",
      "0.05314722333141954 0.05000301292349356 0.02518484047705593\n",
      "it 1, train loss = 2.470360e+00\n",
      "0.04756283645735781 0.05140313057917444 0.0227442385545244\n",
      "it 1, train loss = 1.425413e-01\n",
      "0.07956205722418123 0.04529468186918709 0.027285704245550737\n",
      "it 1, train loss = 2.345743e-01\n",
      "0.05251496866885866 0.05010627616329931 0.02489987418542256\n",
      "it 1, train loss = 2.238560e+00\n",
      "0.04745161627748522 0.05123011960410955 0.02278208369435384\n",
      "it 1, train loss = 1.309093e-01\n",
      "0.052483366726065836 0.050090888618853 0.02481708535207951\n",
      "it 1, train loss = 1.706540e+00\n",
      "0.05273613950481311 0.05007402672985231 0.0249826267217735\n",
      "it 1, train loss = 5.058287e-01\n",
      "0.05267541276827539 0.050061794434572156 0.024905544521776204\n",
      "it 1, train loss = 2.393548e+00\n",
      "0.04741362548452019 0.05125601839306607 0.02280209535915419\n",
      "it 1, train loss = 2.723462e+00\n",
      "0.07956712575112979 0.0452924708367956 0.027288607995591095\n",
      "it 1, train loss = 6.021823e-01\n",
      "0.05250472583345386 0.05012297863495004 0.024900275400927787\n",
      "it 1, train loss = 9.570299e-02\n",
      "0.04871894034120568 0.0512988852265002 0.022929809287031037\n",
      "it 1, train loss = 2.513591e+00\n",
      "0.052473276297381376 0.050189583033760365 0.02480077708044277\n",
      "it 1, train loss = 1.209389e+00\n",
      "0.07956191744963395 0.04529556151654468 0.027286423291263705\n",
      "it 1, train loss = 2.403754e+00\n",
      "0.05254853985884507 0.05014157872748662 0.024977325846656646\n",
      "it 1, train loss = 2.129362e+00\n",
      "0.05249907418688059 0.050128376012093336 0.024882384175373456\n",
      "it 1, train loss = 1.361016e+00\n",
      "0.04772771743481982 0.051397097412312825 0.022611010544834772\n",
      "38 [1 4 3 1]\n",
      "it 1, train loss = 1.005384e-01\n",
      "0.05261072641744122 0.05017553481940949 0.025061373050920606\n",
      "it 1, train loss = 1.084760e+00\n",
      "0.04745516398549482 0.05119706484993643 0.022905100586482235\n",
      "it 1, train loss = 2.691660e+00\n",
      "0.04797777824279396 0.05163538085599727 0.0229943201588943\n",
      "it 1, train loss = 2.010935e+00\n",
      "0.04928236295934638 0.05136188186058111 0.023222701870973896\n",
      "it 1, train loss = 1.470904e+00\n",
      "0.04766908916338082 0.05049813205385019 0.024004169436358604\n",
      "it 1, train loss = 8.284302e-01\n",
      "0.052342555461560546 0.05030890412763132 0.024883283033418916\n",
      "it 1, train loss = 2.134544e+00\n",
      "0.052722513316983105 0.05010223387484286 0.02503437641791934\n",
      "it 1, train loss = 3.782527e-01\n",
      "0.047414367025087625 0.05131748766708569 0.022743653472602036\n",
      "it 1, train loss = 6.229500e-01\n",
      "0.0474092034649916 0.051306632852142924 0.022721631278078126\n",
      "it 1, train loss = 1.832026e+00\n",
      "0.04749207449975087 0.05122574587121274 0.02280473931159244\n",
      "it 1, train loss = 1.040102e-01\n",
      "0.052101079583836626 0.05003637988294689 0.024432573596221707\n",
      "it 1, train loss = 2.720282e+00\n",
      "0.047398202411501285 0.05120758295340638 0.022873586892766508\n",
      "it 1, train loss = 1.379562e+00\n",
      "0.047454328354792706 0.05116661491895431 0.02290596649828653\n",
      "it 1, train loss = 1.389344e+00\n",
      "0.047493355386041304 0.051108777481184316 0.023150483194866657\n",
      "it 1, train loss = 4.378125e-01\n",
      "0.047739176230330374 0.05116020591779311 0.023365518466264296\n",
      "it 1, train loss = 1.481023e-01\n",
      "0.048038728785929435 0.051440309630390604 0.022802732897120728\n",
      "it 1, train loss = 1.555078e-01\n",
      "0.047449629185576485 0.051262639274592806 0.022852499029106108\n",
      "it 1, train loss = 2.310099e+00\n",
      "0.04748066309837665 0.05127274238173279 0.02283308910557042\n",
      "it 1, train loss = 1.907391e-01\n",
      "0.047432830141272496 0.05129867037054605 0.02270215070588199\n",
      "it 1, train loss = 7.653795e-02\n",
      "0.05254806908881388 0.05007587965144293 0.024842444605670994\n",
      "it 1, train loss = 1.468580e+00\n",
      "0.047509280818405546 0.05133256110066776 0.02265915517859048\n",
      "it 1, train loss = 1.552470e+00\n",
      "0.04761185722890476 0.05103138076859367 0.023591626022854622\n",
      "it 1, train loss = 1.160681e+00\n",
      "0.04741471687414225 0.05126272107677055 0.022806592179759496\n",
      "it 1, train loss = 1.382457e-01\n",
      "0.04767321544662234 0.05109047951207363 0.023179025585071573\n",
      "it 1, train loss = 1.540349e+00\n",
      "0.05312385889122483 0.049974962424996355 0.02511101441847491\n",
      "it 1, train loss = 2.145289e+00\n",
      "0.05284003406117086 0.05020671125278162 0.025238613430435024\n",
      "it 1, train loss = 3.179309e-01\n",
      "0.04748662167416689 0.051097710909713864 0.022998242548623395\n",
      "it 1, train loss = 1.647265e+00\n",
      "0.07296371393083372 0.04834889847089817 0.020836142770418987\n",
      "it 1, train loss = 2.110295e+00\n",
      "0.05720909339961512 0.04807064281464339 0.027465559298722592\n",
      "it 1, train loss = 8.142264e-01\n",
      "0.0475571231472313 0.051401237392553215 0.022655563160272958\n",
      "it 1, train loss = 2.620216e+00\n",
      "0.05276414383272891 0.050131729342839826 0.02510318738179134\n",
      "it 1, train loss = 2.889576e+00\n",
      "0.04763841547991458 0.05097587525647899 0.0234853899436968\n",
      "it 1, train loss = 1.878049e+00\n",
      "0.047513675562555265 0.05132511790051351 0.022655025795295696\n",
      "it 1, train loss = 2.853373e+00\n",
      "0.04766761677095564 0.05080076364317989 0.023832102966172858\n",
      "it 1, train loss = 1.213539e+00\n",
      "0.04751826559092957 0.05097376007569876 0.023337695068552346\n",
      "it 1, train loss = 2.573560e+00\n",
      "0.04798982028941067 0.051527263307474244 0.023100328249782594\n",
      "it 1, train loss = 8.536257e-01\n",
      "0.0473759230676417 0.051379801875471955 0.02274143315123781\n",
      "it 1, train loss = 9.112805e-01\n",
      "0.04756574177751212 0.050932806410435835 0.02343610636615301\n",
      "it 1, train loss = 1.887265e+00\n",
      "0.048991312772386345 0.051135356125414706 0.022756700435695963\n",
      "it 1, train loss = 2.701339e+00\n",
      "0.07955375926894864 0.04529840854893931 0.027280235964096938\n",
      "it 1, train loss = 1.463724e+00\n",
      "0.04766514769355277 0.05096515024840398 0.023566637266538876\n",
      "it 1, train loss = 2.865305e+00\n",
      "0.04749496973302417 0.05135303404072458 0.02266721013219693\n",
      "it 1, train loss = 1.502604e+00\n",
      "0.047641863793250534 0.05094537852264936 0.023522208954513433\n",
      "it 1, train loss = 1.200547e-01\n",
      "0.047416233742423854 0.05130055374611979 0.02269036663317722\n",
      "it 1, train loss = 1.004330e+00\n",
      "0.04777904656430956 0.05124348400191538 0.022818040387968957\n",
      "it 1, train loss = 2.960613e+00\n",
      "0.047686036342685326 0.05154477932865052 0.022763044385488682\n",
      "it 1, train loss = 7.593903e-01\n",
      "0.047368100627784494 0.051212980916404444 0.022796293854364363\n",
      "it 1, train loss = 1.103962e-01\n",
      "0.052541022195789884 0.050080314396320956 0.02482176775315401\n",
      "it 1, train loss = 2.448120e+00\n",
      "0.04742065488988148 0.051328502423116186 0.02273444087894851\n",
      "it 1, train loss = 1.872412e+00\n",
      "0.04751864244175544 0.051113584674794814 0.022990645142057713\n",
      "48 [1, 4, 4, 1]\n",
      "it 1, train loss = 1.762457e+00\n",
      "0.05235645213111547 0.05021102160099907 0.024838132637942108\n",
      "it 1, train loss = 1.742716e+00\n",
      "0.05253035161984786 0.050234265362184116 0.025048533767419924\n",
      "it 1, train loss = 6.315905e-02\n",
      "0.04761994195015106 0.05125849722038932 0.022649393137358864\n",
      "it 1, train loss = 1.340302e+00\n",
      "0.05261795369548378 0.05017116572833612 0.024923843418682718\n",
      "it 1, train loss = 2.578488e-01\n",
      "0.04988275471262897 0.05133458731221115 0.023484787263469028\n",
      "it 1, train loss = 7.736924e-01\n",
      "0.0475817268700155 0.050667963804593354 0.023738396958281973\n",
      "it 1, train loss = 1.906105e+00\n",
      "0.048463970057918145 0.05150726054884062 0.022900363221412194\n",
      "it 1, train loss = 3.731307e-01\n",
      "0.04746600297188405 0.05128705345667593 0.02288298431493275\n",
      "it 1, train loss = 6.719651e-02\n",
      "0.04765016201689473 0.051218641686051114 0.02350614643491411\n",
      "it 1, train loss = 2.407335e+00\n",
      "0.04758793325509068 0.05100892154528699 0.023229152206991958\n",
      "it 1, train loss = 7.052451e-01\n",
      "0.05224705995741985 0.05034658988487384 0.02484612534039723\n",
      "it 1, train loss = 2.749930e+00\n",
      "0.07955695849665294 0.045295530261902445 0.02728099432737539\n",
      "it 1, train loss = 2.586325e+00\n",
      "0.04984361074670003 0.051387336599165404 0.023619128014907897\n",
      "it 1, train loss = 4.860226e-01\n",
      "0.047336751379789585 0.05111110916255335 0.022906806602014284\n",
      "it 1, train loss = 2.342440e+00\n",
      "0.0525785474963136 0.050229341919961326 0.025038790309659546\n",
      "it 1, train loss = 2.076096e+00\n",
      "0.04759511557830781 0.05068232871122175 0.023752357001671765\n",
      "it 1, train loss = 2.364065e+00\n",
      "0.04778457592464351 0.05100021187664723 0.023766731638744647\n",
      "it 1, train loss = 1.982658e+00\n",
      "0.04756043470159625 0.05114243239515514 0.02330333478268822\n",
      "it 1, train loss = 2.363764e+00\n",
      "0.04756447651445684 0.050903343312182735 0.023383759217056738\n",
      "it 1, train loss = 4.549637e-01\n",
      "0.047400922121873135 0.05123414139488484 0.022961083446389603\n",
      "it 1, train loss = 3.480920e-01\n",
      "0.047453294724503985 0.051302362504945444 0.022892340881823094\n",
      "it 1, train loss = 1.832044e+00\n",
      "0.04744752947562984 0.05118198445116755 0.022874425928057877\n",
      "it 1, train loss = 6.303040e-01\n",
      "0.04768106197812147 0.05036438660755535 0.024189680725837624\n",
      "it 1, train loss = 9.807693e-01\n",
      "0.04752308739622626 0.05117311505007241 0.022812565992779626\n",
      "it 1, train loss = 1.081783e+00\n",
      "0.047605226238290846 0.05129908006511041 0.022719411290706825\n",
      "it 1, train loss = 1.971015e+00\n",
      "0.04939794805131136 0.05139727384698209 0.023394964753785864\n",
      "it 1, train loss = 2.444835e+00\n",
      "0.049666355272794824 0.05117909987814179 0.022770833405894975\n",
      "it 1, train loss = 1.268822e+00\n",
      "0.04731763093906826 0.05121576041221749 0.02289610589258769\n",
      "it 1, train loss = 1.249695e-01\n",
      "0.047836469714866096 0.05102029715566866 0.023834210261379985\n",
      "it 1, train loss = 2.000284e-01\n",
      "0.04794076554234504 0.050761308545129905 0.024255852090736534\n",
      "it 1, train loss = 1.947464e+00\n",
      "0.052433430008613745 0.0502087104790724 0.02492188030690321\n",
      "it 1, train loss = 9.467060e-02\n",
      "0.04746627850644053 0.0512212974957461 0.022789903136999023\n",
      "it 1, train loss = 2.964476e-01\n",
      "0.047472886282439355 0.05117176575730595 0.023029225221431915\n",
      "it 1, train loss = 6.774902e-01\n",
      "0.04755494183186449 0.05133725577231358 0.022696505412534058\n",
      "it 1, train loss = 9.119804e-02\n",
      "0.047440698550122444 0.05135048378270646 0.02268456534424831\n",
      "it 1, train loss = 9.863085e-01\n",
      "0.04743552080583349 0.05118970598414261 0.02277292465192509\n",
      "it 1, train loss = 7.791749e-01\n",
      "0.04745070666015124 0.05107141279539033 0.02307411447236756\n",
      "it 1, train loss = 5.996836e-01\n",
      "0.04827117387260268 0.05127122925585162 0.02265689944957677\n",
      "it 1, train loss = 2.871390e+00\n",
      "0.047442937207436184 0.05103420717629375 0.02306023425981213\n",
      "it 1, train loss = 1.709232e-01\n",
      "0.04743887137491169 0.051287416911620476 0.022697718221360557\n",
      "it 1, train loss = 1.971181e+00\n",
      "0.05293545597908768 0.05017068784715946 0.025242081023045506\n",
      "it 1, train loss = 1.973608e+00\n",
      "0.05249118971960172 0.05015344694774056 0.024879668810419697\n",
      "it 1, train loss = 6.561410e-01\n",
      "0.04829851052326319 0.051294176720284634 0.022517574753300747\n",
      "it 1, train loss = 2.127443e+00\n",
      "0.050588981897846816 0.050809404787147315 0.023559444737625026\n",
      "it 1, train loss = 2.611628e+00\n",
      "0.04741907219709086 0.05133279199275592 0.022704898017378954\n",
      "it 1, train loss = 1.663912e+00\n",
      "0.05232703243551576 0.05021697942152486 0.024809156777205743\n",
      "it 1, train loss = 1.083692e+00\n",
      "0.04746593861027291 0.05116415249688489 0.022812656930771863\n",
      "it 1, train loss = 2.784281e-01\n",
      "0.04772203709575461 0.05056655774452516 0.0240439092136672\n",
      "it 1, train loss = 4.107409e-01\n",
      "0.04741116904512956 0.05125587941644028 0.0228209925141071\n",
      "it 1, train loss = 2.422972e+00\n",
      "0.04737841455966735 0.05139920578146809 0.02277934435026346\n"
     ]
    }
   ],
   "source": [
    "lamUT_vec = np.array(UTdata['F11'])\n",
    "lamET_vec = np.array(ETdata['F11'])\n",
    "lamPS_vec = np.array(PSdata['F11'])\n",
    "np_list = []\n",
    "r2_list_global = []\n",
    "mae_list_global = []\n",
    "for n in range(10,50):\n",
    "    n_params, layers = opt_arch_node(n)\n",
    "    if n_params in np_list:\n",
    "        continue\n",
    "    print(n_params, layers)\n",
    "    np_list.append(n_params)\n",
    "\n",
    "    r2_list_arch = []\n",
    "    mae_list_arch = []\n",
    "    for i in range(50): #50 runs for every architecture\n",
    "        key, subkey = random.split(key)\n",
    "        params = init_node(key, layers)\n",
    "\n",
    "        # Make sure you are starting at a good place\n",
    "        loss = loss_P11_all(params, F11_data, 3)\n",
    "        while loss>3.0:\n",
    "            key, subkey = random.split(key)\n",
    "            params = init_node(key, layers)\n",
    "            loss = loss_P11_all(params, F11_data, 3)\n",
    "\n",
    "        opt_init, opt_update, get_params = optimizers.adam(2.e-4) #Original: 2.e-4\n",
    "        opt_state = opt_init(params)\n",
    "        params, train_loss, val_loss = train_jp(loss_P11_all, 3, F11_data, opt_state, key, nIter = 100000, print_freq=1000000)\n",
    "\n",
    "        model = NODE_model(params[0], params[1])\n",
    "\n",
    "        lambdas     = [lamUT_vec,       lamET_vec,      lamPS_vec       ]\n",
    "        P11_gt_list = [UTdata['P11'],   ETdata['P11'],  PSdata['P11']   ]\n",
    "        P11funs     = [P11_UT,          P11_ET,         P11_PS          ]\n",
    "\n",
    "        r2 = []\n",
    "        mae = []\n",
    "        for lam, P11_gt, P11fun in zip(lambdas, P11_gt_list, P11funs):\n",
    "            P11 = P11fun(lam, model, normalization)\n",
    "            r2i = r2_score(P11_gt, P11)\n",
    "            r2i = np.clip(r2i, a_min=0.0, a_max=1.0)\n",
    "            r2.append(r2i)\n",
    "\n",
    "            maei = onp.mean(onp.abs(P11_gt-P11))\n",
    "            mae.append(maei)\n",
    "        # r2 = np.mean(np.array(r2))\n",
    "        # print(*r2)\n",
    "        print(*mae)\n",
    "        r2_list_arch.append(r2)\n",
    "        mae_list_arch.append(mae)\n",
    "    r2_list_global.append(r2_list_arch)\n",
    "    mae_list_global.append(mae_list_arch)\n",
    "node_r2 = np.array(r2_list_global)\n",
    "node_mae = np.array(mae_list_global)\n",
    "\n",
    "# node_r2 = np.expand_dims(node_r2,axis=2)\n",
    "# node_mae = np.expand_dims(node_mae,axis=2)\n",
    "with open('savednet/NODE_r2_efficiency_ALL.npy', 'wb') as f:\n",
    "    pickle.dump([np_list, node_r2], f)\n",
    "\n",
    "with open('savednet/NODE_mae_efficiency_ALL.npy', 'wb') as f:\n",
    "    pickle.dump([np_list, node_mae], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 [1 1 1]\n",
      "it 1, train loss = 1.214360e+01\n",
      "0.24155035362440103 0.1277499326876768 0.11737896049063608\n",
      "it 1, train loss = 1.077325e+01\n",
      "0.6968447652047837 0.049930674466077234 0.148995185322304\n",
      "it 1, train loss = 1.136538e+01\n",
      "0.2415539819738375 0.12776160470077338 0.11737832963747512\n",
      "it 1, train loss = 1.181444e+01\n",
      "0.24154425136788876 0.12770357657834433 0.11738145834182254\n",
      "it 1, train loss = 1.036483e+01\n",
      "0.6819975818716211 0.05666127553558107 0.17072285138668844\n",
      "it 1, train loss = 1.183014e+01\n",
      "0.24155073670396404 0.1277495798906639 0.11737898545230929\n",
      "it 1, train loss = 1.087192e+01\n",
      "0.7003717235979514 0.04835397420401322 0.14389435553587346\n",
      "it 1, train loss = 1.105991e+01\n",
      "0.057196277761301956 0.04815445083897761 0.027015896494059135\n",
      "it 1, train loss = 1.141338e+01\n",
      "0.241551467686939 0.12774051966840957 0.11737951131113393\n",
      "it 1, train loss = 1.140694e+01\n",
      "0.24155242484871978 0.12774789147133683 0.1173790214902465\n",
      "it 1, train loss = 1.048988e+01\n",
      "0.05717098165502249 0.048120195758123874 0.026988400081202978\n",
      "it 1, train loss = 1.148165e+01\n",
      "0.24155128671085813 0.1277490780146345 0.11737901570814395\n",
      "it 1, train loss = 1.201043e+01\n",
      "0.24155146408007394 0.12774889500467074 0.11737895424115849\n",
      "it 1, train loss = 1.053246e+01\n",
      "0.688048651283189 0.05387273570852678 0.1617183461924411\n",
      "it 1, train loss = 1.267294e+01\n",
      "0.24155022570814272 0.1277502269902253 0.11737894151661724\n",
      "it 1, train loss = 1.075930e+01\n",
      "0.6963844686307535 0.050052223769542104 0.1496413607671154\n",
      "it 1, train loss = 9.592883e+00\n",
      "0.057215230118806526 0.04814374335819506 0.02700986857519397\n",
      "it 1, train loss = 1.012048e+01\n",
      "0.24155132127284124 0.12774882801562362 0.1173789592764093\n",
      "it 1, train loss = 8.596938e+00\n",
      "0.05722441418679781 0.04813860622988128 0.027006959138613024\n",
      "it 1, train loss = 1.064728e+01\n",
      "0.24154997997273064 0.1277503888097591 0.117378868390545\n",
      "it 1, train loss = 1.231924e+01\n",
      "0.24155131037478 0.12774906637806166 0.11737901801578736\n",
      "it 1, train loss = 1.105457e+01\n",
      "0.7081985289984398 0.04586645500602775 0.13437869413146228\n",
      "it 1, train loss = 1.109781e+01\n",
      "0.05718685136671116 0.04816008678848285 0.02701891160800311\n",
      "it 1, train loss = 1.188707e+01\n",
      "0.24155183425518495 0.12774845389531436 0.11737909152907422\n",
      "it 1, train loss = 1.044044e+01\n",
      "0.6847301715486708 0.05540016452070618 0.1666506889852936\n",
      "it 1, train loss = 8.682797e+00\n",
      "0.057233068320359806 0.04813313029913577 0.027004153286624557\n",
      "it 1, train loss = 8.608436e+00\n",
      "0.057264673049118336 0.048112673002489725 0.026993866867073696\n",
      "it 1, train loss = 1.077167e+01\n",
      "0.2415516141935164 0.1277486005856692 0.11737900354202666\n",
      "it 1, train loss = 1.151570e+01\n",
      "0.24155203921971885 0.1277482792173718 0.11737907843056891\n",
      "it 1, train loss = 1.224954e+01\n",
      "0.24155117445322108 0.12774901842750075 0.11737894506819604\n",
      "it 1, train loss = 1.138924e+01\n",
      "0.7236982736606563 0.043099349792311496 0.1172951651578876\n",
      "it 1, train loss = 1.251624e+01\n",
      "0.2415510193002034 0.12774938748370956 0.11737895598922139\n",
      "it 1, train loss = 1.137341e+01\n",
      "0.7229860786081236 0.04315270901423816 0.11806965244191592\n",
      "it 1, train loss = 1.072488e+01\n",
      "0.057205924451620635 0.04814898738308123 0.02701281932647667\n",
      "it 1, train loss = 1.068475e+01\n",
      "0.057207667833516565 0.048147677116928383 0.02701223239509095\n",
      "it 1, train loss = 1.080151e+01\n",
      "0.05720556731809934 0.048148804042049465 0.027012887210901965\n",
      "it 1, train loss = 1.077718e+01\n",
      "0.05720129880916623 0.0481515312235419 0.027014280424627526\n",
      "it 1, train loss = 1.121555e+01\n",
      "0.24155226910178107 0.12774792789630865 0.11737907357367261\n",
      "it 1, train loss = 1.145035e+01\n",
      "0.2415525361299777 0.1277478024094321 0.11737908997478662\n",
      "it 1, train loss = 1.010713e+01\n",
      "0.05721835707638354 0.04814194356487662 0.027008871492194973\n",
      "it 1, train loss = 1.118341e+01\n",
      "0.24155098616592452 0.12774925760637088 0.11737898329536776\n",
      "it 1, train loss = 1.084701e+01\n",
      "0.057207163096046475 0.04818236105507142 0.027018555801487015\n",
      "it 1, train loss = 1.159702e+01\n",
      "0.24155336445400405 0.12775861626550933 0.11737850221251377\n",
      "it 1, train loss = 1.100147e+01\n",
      "0.05721657396719254 0.048142632345653746 0.027009404859101804\n",
      "it 1, train loss = 1.165010e+01\n",
      "0.24155145717771603 0.12774537756384305 0.11737924305694915\n",
      "it 1, train loss = 1.173179e+01\n",
      "0.24155156751518092 0.12774776752598982 0.11737899084216505\n",
      "it 1, train loss = 1.128530e+01\n",
      "0.7188725795714381 0.04368622322189355 0.12261622876798241\n",
      "it 1, train loss = 1.042972e+01\n",
      "0.057201005166963276 0.04815209795460778 0.027014416269714823\n",
      "it 1, train loss = 1.039989e+01\n",
      "0.05722162627339133 0.048139224087191596 0.02700774369443415\n",
      "it 1, train loss = 1.233287e+01\n",
      "0.24155055377438106 0.1277498277081779 0.1173789754896908\n",
      "16 [1, 2, 1]\n",
      "it 1, train loss = 1.081726e+01\n",
      "0.058538238227561566 0.04743647498361597 0.026696562912359667\n",
      "it 1, train loss = 1.107046e+01\n",
      "0.05860765681072456 0.04740765668898731 0.026688741065398338\n",
      "it 1, train loss = 1.097689e+01\n",
      "0.058370273971713814 0.047527763685644146 0.026741210033408687\n",
      "it 1, train loss = 1.099519e+01\n",
      "0.058300937200286285 0.047581125965342176 0.026772449277371022\n",
      "it 1, train loss = 1.072697e+01\n",
      "0.05859560053409741 0.04742879058218429 0.026712471758093877\n",
      "it 1, train loss = 9.875552e+00\n",
      "0.05825005222961311 0.04758033038525631 0.026755148964230275\n",
      "it 1, train loss = 1.228380e+01\n",
      "0.058298070276193964 0.0475620300999615 0.026752664038441204\n",
      "it 1, train loss = 1.006422e+01\n",
      "0.05897366643306076 0.047233229067266166 0.026590159447745836\n",
      "it 1, train loss = 1.087897e+01\n",
      "0.05862027602350373 0.047395775918709415 0.026679388191354163\n",
      "it 1, train loss = 1.161697e+01\n",
      "0.05853997904497045 0.04744858487566416 0.026713706027799038\n",
      "it 1, train loss = 1.110432e+01\n",
      "0.05840507493357861 0.04751192917668653 0.02673588941281097\n",
      "it 1, train loss = 1.024708e+01\n",
      "0.058578990999748975 0.04741653055230378 0.026687378987948755\n",
      "it 1, train loss = 1.166635e+01\n",
      "0.05838389206729659 0.04752972683858623 0.026748782327385676\n",
      "it 1, train loss = 9.879785e+00\n",
      "0.05882238391566057 0.04729070019881584 0.026614938598299963\n",
      "it 1, train loss = 1.084156e+01\n",
      "0.05839045183361095 0.047532180461008634 0.026755066964120444\n",
      "it 1, train loss = 1.146295e+01\n",
      "0.24155165412509466 0.12774860444628355 0.11737892993919856\n",
      "it 1, train loss = 1.168502e+01\n",
      "0.05909072960389568 0.047235400191589234 0.026578116323507112\n",
      "it 1, train loss = 1.189284e+01\n",
      "0.058402835484465225 0.04753237334961308 0.02676053035733041\n",
      "it 1, train loss = 1.175952e+01\n",
      "0.05866634193330014 0.047386578371033425 0.02668743552538322\n",
      "it 1, train loss = 1.057649e+01\n",
      "0.05840934657628459 0.0475057581318995 0.02672932646814254\n",
      "it 1, train loss = 9.861238e+00\n",
      "0.058678002308964826 0.04736958500817663 0.026672334900626916\n",
      "it 1, train loss = 1.105056e+01\n",
      "0.058371520383047173 0.04753390593460956 0.0267486932088477\n",
      "it 1, train loss = 1.146180e+01\n",
      "0.05865705267441571 0.04737620712322232 0.026670274746621585\n",
      "it 1, train loss = 1.143798e+01\n",
      "0.0584173661328745 0.04750456426938952 0.02673252514073045\n",
      "it 1, train loss = 1.054456e+01\n",
      "0.0587000927709397 0.04736136279524425 0.026671144434680234\n",
      "it 1, train loss = 1.129910e+01\n",
      "0.05835815591260371 0.04755323106479255 0.026768254074253105\n",
      "it 1, train loss = 1.095357e+01\n",
      "0.24155035163050959 0.1277372943095748 0.11737951028384905\n",
      "it 1, train loss = 1.068772e+01\n",
      "0.24155213568028938 0.12774804331097903 0.1173789774340339\n",
      "it 1, train loss = 1.105673e+01\n",
      "0.059077880184274074 0.04719459927158498 0.026574183988186447\n",
      "it 1, train loss = 1.182174e+01\n",
      "0.05837449180355732 0.04753973595477212 0.02675754203096345\n",
      "it 1, train loss = 1.177435e+01\n",
      "0.058517030431578736 0.04745743261060646 0.02671349135180572\n",
      "it 1, train loss = 1.162226e+01\n",
      "0.058696090808795114 0.04739514690286953 0.026730984133117048\n",
      "it 1, train loss = 1.052328e+01\n",
      "0.05853244069908732 0.047442885540964735 0.026704559756367367\n",
      "it 1, train loss = 1.100253e+01\n",
      "0.05840866534064118 0.04751319817492057 0.026738492667325944\n",
      "it 1, train loss = 1.236724e+01\n",
      "0.0584813191425384 0.04750280948097756 0.026758580295469243\n",
      "it 1, train loss = 1.148690e+01\n",
      "0.24155156787135884 0.12774298604578097 0.11737937032314458\n",
      "it 1, train loss = 1.035325e+01\n",
      "0.05838535916759148 0.047520177504228996 0.026737210585496045\n",
      "it 1, train loss = 1.124123e+01\n",
      "0.058535997943868934 0.0474348873549197 0.026693994990924876\n",
      "it 1, train loss = 1.195782e+01\n",
      "0.05852036040679388 0.04745990594127734 0.026718639763275893\n",
      "it 1, train loss = 1.131395e+01\n",
      "0.058553639546499135 0.047436052052720466 0.026703148616489807\n",
      "it 1, train loss = 1.089349e+01\n",
      "0.058340576942568154 0.04754629518910132 0.026751323552710943\n",
      "it 1, train loss = 1.185374e+01\n",
      "0.05857348574862499 0.047460488159096356 0.02674962534394979\n",
      "it 1, train loss = 1.127679e+01\n",
      "0.05849550546883931 0.04746384034219211 0.02671310280474115\n",
      "it 1, train loss = 1.130396e+01\n",
      "0.23856660482274036 0.12737141499389132 0.11641308439386015\n",
      "it 1, train loss = 1.127449e+01\n",
      "0.24138178265638788 0.1279478378238397 0.11743935208498371\n",
      "it 1, train loss = 1.081978e+01\n",
      "0.05836569116716941 0.047547365021195664 0.026696519169717967\n",
      "it 1, train loss = 1.257514e+01\n",
      "0.05846097987493595 0.04749026586607086 0.02673274132739356\n",
      "it 1, train loss = 9.728554e+00\n",
      "0.05862646242022523 0.04738450821330021 0.026667746710679223\n",
      "it 1, train loss = 1.049977e+01\n",
      "0.05828198321470863 0.04759328293142266 0.02678057495965425\n",
      "it 1, train loss = 1.145795e+01\n",
      "0.7264860045507535 0.042738004865482154 0.11420953976761393\n",
      "22 [1, 3, 1]\n",
      "it 1, train loss = 1.115343e+01\n",
      "0.058357657762199304 0.04754487349355346 0.026757075244671493\n",
      "it 1, train loss = 1.044090e+01\n",
      "0.058496986616779746 0.04746974396786394 0.026719760000360228\n",
      "it 1, train loss = 1.055979e+01\n",
      "0.058380850577770135 0.04751801255597352 0.026733205848454317\n",
      "it 1, train loss = 1.039540e+01\n",
      "0.048501260403071594 0.05042017691652528 0.023618383918793198\n",
      "it 1, train loss = 1.196560e+01\n",
      "0.2415532652481492 0.1277502548966412 0.11737904295061773\n",
      "it 1, train loss = 1.082433e+01\n",
      "0.048646578082913644 0.05007993019818895 0.023826394284325057\n",
      "it 1, train loss = 1.113175e+01\n",
      "0.058363272832662016 0.047551657926040165 0.026768650353572083\n",
      "it 1, train loss = 1.055383e+01\n",
      "0.05812646304332441 0.047741536640284826 0.02686094471226468\n",
      "it 1, train loss = 1.086654e+01\n",
      "0.059282196962529426 0.04736076007411866 0.0269491569556517\n",
      "it 1, train loss = 1.070373e+01\n",
      "0.058340938113262075 0.047545915449108495 0.02675120182655289\n",
      "it 1, train loss = 1.121610e+01\n",
      "0.05849206258096618 0.04747319283615346 0.02672458068255556\n",
      "it 1, train loss = 1.051051e+01\n",
      "0.05228506802162441 0.050142565302548917 0.025741968065307104\n",
      "it 1, train loss = 1.096987e+01\n",
      "0.04832751152593555 0.05066430146257087 0.023741703559816085\n",
      "it 1, train loss = 1.043598e+01\n",
      "0.04864168362619507 0.05016090247753491 0.023774418457452318\n",
      "it 1, train loss = 1.028089e+01\n",
      "0.04865129953678741 0.05005302312687234 0.02383247764189323\n",
      "it 1, train loss = 1.226420e+01\n",
      "0.05838943357486118 0.04753664121747382 0.026760140595823715\n",
      "it 1, train loss = 1.114869e+01\n",
      "0.05838432634052367 0.04753068666635564 0.0267499912560439\n",
      "it 1, train loss = 1.076161e+01\n",
      "0.04829799061986427 0.050667749767904 0.02365916333803774\n",
      "it 1, train loss = 9.817001e+00\n",
      "0.058393817711158216 0.047517394089605536 0.026737352457560228\n",
      "it 1, train loss = 1.145301e+01\n",
      "0.05874692370517334 0.04745505591243745 0.0268458040406043\n",
      "it 1, train loss = 1.075627e+01\n",
      "0.05833883463407335 0.04754391474797948 0.026747484946251527\n",
      "it 1, train loss = 1.044251e+01\n",
      "0.04847544976089044 0.05049837077217696 0.023783781702561697\n",
      "it 1, train loss = 1.010470e+01\n",
      "0.048428768604182035 0.050525895906751266 0.023739581438845938\n",
      "it 1, train loss = 1.059999e+01\n",
      "0.058412107744274785 0.04750951901259049 0.026735229349042825\n",
      "it 1, train loss = 1.245765e+01\n",
      "0.05848923600300806 0.047496471216598515 0.026752726872158746\n",
      "it 1, train loss = 1.134622e+01\n",
      "0.04860059266761684 0.04996569514325305 0.023893151445556347\n",
      "it 1, train loss = 1.190168e+01\n",
      "0.05840147993171933 0.047538194533086435 0.026767503539049806\n",
      "it 1, train loss = 1.159496e+01\n",
      "0.048595351527338265 0.050295652545579786 0.0237320556461538\n",
      "it 1, train loss = 1.127293e+01\n",
      "0.0586095821909027 0.04740693355896612 0.026689338811979973\n",
      "it 1, train loss = 1.049491e+01\n",
      "0.05827832244110664 0.04757544870942133 0.0267613103680861\n",
      "it 1, train loss = 1.139132e+01\n",
      "0.0486467298937581 0.049893709957120454 0.023979669659476066\n",
      "it 1, train loss = 1.128137e+01\n",
      "0.04864524139297455 0.05004404755072616 0.023866195618804046\n",
      "it 1, train loss = 1.039732e+01\n",
      "0.05030261138628963 0.050653687063925194 0.02403312209520544\n",
      "it 1, train loss = 1.156720e+01\n",
      "0.058443875994941975 0.047493774889209334 0.02672974071045334\n",
      "it 1, train loss = 1.174079e+01\n",
      "0.05836868587776402 0.04754932225093758 0.026767668923831693\n",
      "it 1, train loss = 1.058127e+01\n",
      "0.05840982788774754 0.047502022465868717 0.02672525006886913\n",
      "it 1, train loss = 1.094272e+01\n",
      "0.04862152106895156 0.04977066164648602 0.024034214580422344\n",
      "it 1, train loss = 1.075878e+01\n",
      "0.05847463033438729 0.04747238857369568 0.026715170428658886\n",
      "it 1, train loss = 1.057369e+01\n",
      "0.048474775167990085 0.050727041247619606 0.02360499655584697\n",
      "it 1, train loss = 1.175843e+01\n",
      "0.05840370829817905 0.047532525811667045 0.02676118795934061\n",
      "it 1, train loss = 1.141203e+01\n",
      "0.04870490954656017 0.050723083257036464 0.02324640817107811\n",
      "it 1, train loss = 1.116290e+01\n",
      "0.05840978847823727 0.04751191966569851 0.0267378409841613\n",
      "it 1, train loss = 1.093675e+01\n",
      "0.04864432305871077 0.049831123399065244 0.023980820135979488\n",
      "it 1, train loss = 1.063243e+01\n",
      "0.05846973857030985 0.04748672063240433 0.02673011474623684\n",
      "it 1, train loss = 1.023787e+01\n",
      "0.05831221449771297 0.04756357470039763 0.026761142743062037\n",
      "it 1, train loss = 1.123267e+01\n",
      "0.058343884058423966 0.047544621351348036 0.02675099681796079\n",
      "it 1, train loss = 1.149956e+01\n",
      "0.04859889039022072 0.05028281662160727 0.02372744301004079\n",
      "it 1, train loss = 1.110145e+01\n",
      "0.048569811873336644 0.050335393219725806 0.023724012574683984\n",
      "it 1, train loss = 1.079063e+01\n",
      "0.04863678642657128 0.050021633603025174 0.02385030210917561\n",
      "it 1, train loss = 1.124892e+01\n",
      "0.048618953182516925 0.05003401498331104 0.023850883549268797\n",
      "28 [1, 4, 1]\n",
      "it 1, train loss = 1.070787e+01\n",
      "0.04863578073908939 0.05016476841105151 0.023786023691573455\n",
      "it 1, train loss = 1.076685e+01\n",
      "0.058371430030123884 0.047539163918114746 0.026756137989635782\n",
      "it 1, train loss = 1.015449e+01\n",
      "0.048415020193471055 0.05072030521776686 0.02366844279586135\n",
      "it 1, train loss = 1.035113e+01\n",
      "0.04889284914557906 0.05088600305400818 0.023543533503363256\n",
      "it 1, train loss = 1.219608e+01\n",
      "0.04861510940602431 0.05081250095811202 0.023639523688108673\n",
      "it 1, train loss = 1.117931e+01\n",
      "0.05848159383732181 0.04747612209470091 0.02672331205968406\n",
      "it 1, train loss = 1.126273e+01\n",
      "0.05835137842190271 0.0475565094595775 0.026769106548981587\n",
      "it 1, train loss = 1.120554e+01\n",
      "0.0584057824083005 0.0475157620079405 0.026741075275814683\n",
      "it 1, train loss = 1.119239e+01\n",
      "0.058426950717152246 0.047516983366650854 0.026751887139890098\n",
      "it 1, train loss = 1.108032e+01\n",
      "0.04863332522313867 0.050015504875056106 0.023862567371743266\n",
      "it 1, train loss = 1.147792e+01\n",
      "0.048619353683384325 0.050037566884446966 0.023864593531916407\n",
      "it 1, train loss = 1.166881e+01\n",
      "0.048602354168267185 0.05001196486448427 0.023886510876812896\n",
      "it 1, train loss = 1.092756e+01\n",
      "0.048620231164677195 0.05023799952107837 0.023765386393307407\n",
      "it 1, train loss = 1.142975e+01\n",
      "0.048628768083058486 0.04997950415794284 0.023904337831305873\n",
      "it 1, train loss = 1.017304e+01\n",
      "0.048649394888220135 0.050078490126294666 0.023822258642832415\n",
      "it 1, train loss = 1.079103e+01\n",
      "0.04863391694291163 0.04999308828019883 0.023883735875635467\n",
      "it 1, train loss = 1.123506e+01\n",
      "0.0583834316723953 0.047547905585307734 0.026772444673996275\n",
      "it 1, train loss = 1.155686e+01\n",
      "0.04864806854459138 0.05012402590111411 0.02382417124186453\n",
      "it 1, train loss = 1.149028e+01\n",
      "0.048535632970002004 0.05040518028128444 0.02372962486933372\n",
      "it 1, train loss = 1.005210e+01\n",
      "0.04864266770763301 0.05006228884839352 0.02380427334545734\n",
      "it 1, train loss = 1.144907e+01\n",
      "0.04858974772874358 0.05000085117579746 0.023879537132740127\n",
      "it 1, train loss = 1.106029e+01\n",
      "0.04855319886367782 0.04995102446042851 0.02390210734702855\n",
      "it 1, train loss = 1.037241e+01\n",
      "0.0485512810166483 0.05035066237992291 0.023752031185346016\n",
      "it 1, train loss = 1.139068e+01\n",
      "0.05842791522504784 0.04751487187241824 0.026749613711303927\n",
      "it 1, train loss = 1.209709e+01\n",
      "0.05840837099565253 0.04753199166547622 0.026762748463254635\n",
      "it 1, train loss = 1.157189e+01\n",
      "0.048461180732237416 0.05071468962084252 0.023543703760073555\n",
      "it 1, train loss = 9.733586e+00\n",
      "0.0486394161571655 0.05012820578289044 0.023791517825015658\n",
      "it 1, train loss = 1.053873e+01\n",
      "0.048651560808362436 0.0500471406777717 0.023842690418683896\n",
      "it 1, train loss = 1.114482e+01\n",
      "0.04861362777441149 0.05001527590015411 0.023869437451193236\n",
      "it 1, train loss = 1.153082e+01\n",
      "0.04862891478815305 0.05005075006354726 0.02385032747569673\n",
      "it 1, train loss = 1.019213e+01\n",
      "0.04864934480376043 0.05006723101824185 0.02383538342907297\n",
      "it 1, train loss = 1.065147e+01\n",
      "0.058464592336556695 0.04748195929163492 0.026722760644665586\n",
      "it 1, train loss = 9.938912e+00\n",
      "0.0486494895332685 0.05004528380181876 0.023840188617872148\n",
      "it 1, train loss = 1.076034e+01\n",
      "0.05080189272864434 0.050544972449840145 0.025096104178537425\n",
      "it 1, train loss = 1.111521e+01\n",
      "0.04864532838420268 0.05012389931915748 0.02381522895411248\n",
      "it 1, train loss = 1.141272e+01\n",
      "0.04863848492164644 0.050167891198809524 0.023795142006310784\n",
      "it 1, train loss = 1.090032e+01\n",
      "0.048435187822012636 0.050521730573255985 0.023713585624859694\n",
      "it 1, train loss = 1.013194e+01\n",
      "0.05831905053309074 0.04755682178793065 0.02675555662170932\n",
      "it 1, train loss = 1.092853e+01\n",
      "0.04861005554236897 0.0502370952299745 0.023751615206897402\n",
      "it 1, train loss = 1.146401e+01\n",
      "0.049016657138638974 0.050936485434091315 0.023516894639007923\n",
      "it 1, train loss = 1.075203e+01\n",
      "0.048632719713921475 0.05018456181529646 0.02377613927303763\n",
      "it 1, train loss = 1.095393e+01\n",
      "0.04842306610438058 0.05072468301713142 0.02366387719643408\n",
      "it 1, train loss = 1.191019e+01\n",
      "0.04864722481035037 0.05007040326053634 0.023861642954574578\n",
      "it 1, train loss = 1.096495e+01\n",
      "0.048645876825742826 0.05009305873218254 0.023845762029910535\n",
      "it 1, train loss = 1.078671e+01\n",
      "0.04834006526757061 0.050687831448304736 0.023672123983537297\n",
      "it 1, train loss = 1.149720e+01\n",
      "0.048613510516268645 0.05003034089021605 0.023866553787024562\n",
      "it 1, train loss = 1.110978e+01\n",
      "0.0584397626635428 0.04751148152976656 0.026750034838721563\n",
      "it 1, train loss = 1.112514e+01\n",
      "0.04864046077428329 0.050071491112385104 0.02383579378686777\n",
      "it 1, train loss = 1.175319e+01\n",
      "0.05836642597183536 0.047554326671636137 0.02677323522464212\n",
      "it 1, train loss = 1.172311e+01\n",
      "0.048565828298863085 0.05001554216395714 0.023875590574255923\n",
      "32 [1, 2, 2, 1]\n",
      "it 1, train loss = 1.015613e+01\n",
      "0.04739972994299499 0.04878032655084185 0.024549669533958412\n",
      "it 1, train loss = 1.169179e+01\n",
      "0.04724204304800468 0.048756550289342206 0.024676752111873774\n",
      "it 1, train loss = 1.147889e+01\n",
      "0.05738401674199347 0.04786405329720045 0.026566613993760813\n",
      "it 1, train loss = 1.108423e+01\n",
      "0.057887530179032455 0.04770040383378545 0.026667901216686076\n",
      "it 1, train loss = 1.114125e+01\n",
      "0.04660773838975883 0.04881811370397046 0.024546055786574324\n",
      "it 1, train loss = 1.041284e+01\n",
      "0.04660906602797333 0.04868715394350612 0.02464141900328722\n",
      "it 1, train loss = 1.131015e+01\n",
      "0.05855933712418634 0.04729580372124394 0.026355241288392443\n",
      "it 1, train loss = 9.457488e+00\n",
      "0.047684011687948384 0.04888149758473195 0.024581352992154366\n",
      "it 1, train loss = 1.107705e+01\n",
      "0.05887712485142844 0.04791495531126942 0.02755685795667509\n",
      "it 1, train loss = 1.117124e+01\n",
      "0.058913157812518685 0.0473503112140167 0.02664197620420783\n",
      "it 1, train loss = 1.207466e+01\n",
      "0.05873727377537645 0.04727580869875533 0.02638871620715267\n",
      "it 1, train loss = 1.035590e+01\n",
      "0.058143680596141945 0.04752281926800146 0.02644927805813754\n",
      "it 1, train loss = 1.149336e+01\n",
      "0.05835585700431089 0.047430156843693186 0.02652456584372981\n",
      "it 1, train loss = 1.180883e+01\n",
      "0.05844709661643654 0.048147769913713764 0.027589091355626615\n",
      "it 1, train loss = 1.171368e+01\n",
      "0.05845085269234671 0.04733332568093531 0.026402465664305836\n",
      "it 1, train loss = 1.175644e+01\n",
      "0.05865414934761931 0.04729880189749108 0.026467364375477198\n",
      "it 1, train loss = 1.118125e+01\n",
      "0.058639036126690254 0.0472794217671425 0.026408181388437587\n",
      "it 1, train loss = 1.119205e+01\n",
      "0.7142798945420796 0.04433674466508181 0.12765820690991017\n",
      "it 1, train loss = 1.096636e+01\n",
      "0.058309542863319984 0.04765196837998504 0.02686617462849462\n",
      "it 1, train loss = 9.835608e+00\n",
      "0.05853178424165991 0.04827744062027916 0.02768534695241338\n",
      "it 1, train loss = 1.195927e+01\n",
      "0.058533894009775655 0.047297782487750545 0.026410797348795017\n",
      "it 1, train loss = 1.120390e+01\n",
      "0.05871454736109854 0.04723651839693099 0.026358384209547076\n",
      "it 1, train loss = 1.104987e+01\n",
      "0.05855129860600423 0.04817979456039066 0.02763939023351436\n",
      "it 1, train loss = 1.155120e+01\n",
      "0.05820999663774707 0.04775372558419051 0.02679519722089583\n",
      "it 1, train loss = 1.116012e+01\n",
      "0.05878486461091134 0.04735749801323245 0.026658846577441924\n",
      "it 1, train loss = 1.169552e+01\n",
      "0.05799444406366011 0.048427892066383676 0.02767911139174049\n",
      "it 1, train loss = 1.053199e+01\n",
      "0.05882006357098736 0.047964199518034066 0.027535837028943484\n",
      "it 1, train loss = 1.081206e+01\n",
      "0.04761468547062503 0.04907170380633145 0.024372874190630698\n",
      "it 1, train loss = 1.076005e+01\n",
      "0.05852488676142909 0.04733094427654538 0.026416748219236372\n",
      "it 1, train loss = 1.076755e+01\n",
      "0.05812149022937378 0.04751217716269102 0.02653886854744606\n",
      "it 1, train loss = 1.168680e+01\n",
      "0.05832041263222802 0.04750084690790109 0.026548148603342876\n",
      "it 1, train loss = 1.213802e+01\n",
      "0.058322483047019216 0.04832825746593619 0.027693703684043194\n",
      "it 1, train loss = 1.072175e+01\n",
      "0.046897034086435446 0.04859810081166708 0.02462594999900578\n",
      "it 1, train loss = 1.134055e+01\n",
      "0.058485893131138125 0.047398154139368695 0.02656215380703812\n",
      "it 1, train loss = 1.075194e+01\n",
      "0.04636184111881681 0.048636801312882484 0.024632343789469692\n",
      "it 1, train loss = 1.204916e+01\n",
      "0.058292879556884954 0.04746049206950236 0.026545143315377256\n",
      "it 1, train loss = 1.092616e+01\n",
      "0.046809671235540896 0.048677474709193876 0.024583959327862896\n",
      "it 1, train loss = 1.066670e+01\n",
      "0.05818183653185076 0.047532710659819746 0.026633098814184526\n",
      "it 1, train loss = 1.089965e+01\n",
      "0.05864344096435212 0.04729484807491979 0.026428672030790672\n",
      "it 1, train loss = 1.187555e+01\n",
      "0.05766773233683432 0.04780757743578154 0.026725681749271427\n",
      "it 1, train loss = 1.035533e+01\n",
      "0.05854802234775684 0.047290165516552726 0.026389170097858017\n",
      "it 1, train loss = 1.301752e+01\n",
      "0.05878341898079208 0.04725407714996197 0.02634345905621346\n",
      "it 1, train loss = 1.132447e+01\n",
      "0.05836159785783877 0.04742188405065327 0.0264476195271025\n",
      "it 1, train loss = 1.119268e+01\n",
      "0.05771333045885191 0.049569604009373235 0.025890575938760024\n",
      "it 1, train loss = 1.108607e+01\n",
      "0.05851569507854142 0.04733112918942659 0.026498391740609195\n",
      "it 1, train loss = 1.187511e+01\n",
      "0.058519214240552495 0.04733537167748622 0.026425698896185672\n",
      "it 1, train loss = 1.122238e+01\n",
      "0.05818143093105333 0.04753462760205156 0.0265433466712258\n",
      "it 1, train loss = 1.161865e+01\n",
      "0.05858947406604074 0.04729150930365772 0.026379543950437196\n",
      "it 1, train loss = 1.083802e+01\n",
      "0.04654560228341868 0.04861404491318031 0.024624271271790257\n",
      "it 1, train loss = 1.169117e+01\n",
      "0.047445715777986334 0.04876799537799298 0.024563080495147153\n",
      "40 [1 3 2 1]\n",
      "it 1, train loss = 9.874278e+00\n",
      "0.058060991277353334 0.04766377223417274 0.026581920386940027\n",
      "it 1, train loss = 1.056621e+01\n",
      "0.058595681053472494 0.047358374128320294 0.0264980610432128\n",
      "it 1, train loss = 1.166288e+01\n",
      "0.05840437491542543 0.04758469581383722 0.026777824813854205\n",
      "it 1, train loss = 1.036695e+01\n",
      "0.047567254631098475 0.04896691180977847 0.02447809783524329\n",
      "it 1, train loss = 1.083630e+01\n",
      "0.05853083134975297 0.04737304929119046 0.026504582806156734\n",
      "it 1, train loss = 1.103785e+01\n",
      "0.058146269755346446 0.047527686108113046 0.02656997747467536\n",
      "it 1, train loss = 1.030563e+01\n",
      "0.058327551222869846 0.04742294875012874 0.026477020410135242\n",
      "it 1, train loss = 1.168495e+01\n",
      "0.05875515073624313 0.04728761931603468 0.026527118328743705\n",
      "it 1, train loss = 1.112303e+01\n",
      "0.04567709533761933 0.04866488136463083 0.02467414578362193\n",
      "it 1, train loss = 1.147816e+01\n",
      "0.058821745944320894 0.047203975157591775 0.026300414725967194\n",
      "it 1, train loss = 1.146343e+01\n",
      "0.05821607135128058 0.047545870823510465 0.02665299730209233\n",
      "it 1, train loss = 1.035975e+01\n",
      "0.04774639386489263 0.049804536101290146 0.02418350719610445\n",
      "it 1, train loss = 1.186527e+01\n",
      "0.047538791465460625 0.04886211800979486 0.02461675297442701\n",
      "it 1, train loss = 1.052365e+01\n",
      "0.058341255198204704 0.04738649442477599 0.026479015450858265\n",
      "it 1, train loss = 1.172862e+01\n",
      "0.05854570674908435 0.0473520468502119 0.02643887008626012\n",
      "it 1, train loss = 1.087831e+01\n",
      "0.04704969843670872 0.048683335480176754 0.024617324818979717\n",
      "it 1, train loss = 1.215303e+01\n",
      "0.058694197326756854 0.04729072948915866 0.026382785540042843\n",
      "it 1, train loss = 1.101315e+01\n",
      "0.05405663027297308 0.049475571570169914 0.026143443849352587\n",
      "it 1, train loss = 1.099531e+01\n",
      "0.058520594492143946 0.04737148288555235 0.026468706675181662\n",
      "it 1, train loss = 1.164633e+01\n",
      "0.05830478750323149 0.0476042154576557 0.026456824536216034\n",
      "it 1, train loss = 1.111459e+01\n",
      "0.047829481536016634 0.04900166235187585 0.02452821830651211\n",
      "it 1, train loss = 1.041955e+01\n",
      "0.058682802080950086 0.04735049916041102 0.026480767506441186\n",
      "it 1, train loss = 1.092191e+01\n",
      "0.0480507434085062 0.050506020644828176 0.024038409448674924\n",
      "it 1, train loss = 1.128289e+01\n",
      "0.04777289595851661 0.04915047067006661 0.024385916846791596\n",
      "it 1, train loss = 9.849997e+00\n",
      "0.047263419669611666 0.048757925657616906 0.02468100045287214\n",
      "it 1, train loss = 1.166290e+01\n",
      "0.04830535811553192 0.049615717315431775 0.02423732054764464\n",
      "it 1, train loss = 9.930499e+00\n",
      "0.05807751646651696 0.047861872971314265 0.02642577369065367\n",
      "it 1, train loss = 1.054693e+01\n",
      "0.05948710283725006 0.04704275391225021 0.026606734419828505\n",
      "it 1, train loss = 1.056213e+01\n",
      "0.05853231710523963 0.04735287532623705 0.026452519074771103\n",
      "it 1, train loss = 1.039627e+01\n",
      "0.05810031853461675 0.04755704064871522 0.02659825107800759\n",
      "it 1, train loss = 1.226670e+01\n",
      "0.056651332169441425 0.049111913334503945 0.027537569793491888\n",
      "it 1, train loss = 1.078553e+01\n",
      "0.057948442626932974 0.04764385053673945 0.02670876492599899\n",
      "it 1, train loss = 1.091908e+01\n",
      "0.058470589467802536 0.04746223429983547 0.026540388103105002\n",
      "it 1, train loss = 1.088414e+01\n",
      "0.04662557539983864 0.048703567961606846 0.02461356541015604\n",
      "it 1, train loss = 1.243247e+01\n",
      "0.04691613461501991 0.04872516403041098 0.024639512446781246\n",
      "it 1, train loss = 1.071635e+01\n",
      "0.05815363121670762 0.04762486331571455 0.02651620889641843\n",
      "it 1, train loss = 1.052740e+01\n",
      "0.0548449569373241 0.04885820856207524 0.02632654195470021\n",
      "it 1, train loss = 9.723475e+00\n",
      "0.058349814767292084 0.04736705207834664 0.026458222627887044\n",
      "it 1, train loss = 1.072873e+01\n",
      "0.058304714941641765 0.047476751278371115 0.0265741786210226\n",
      "it 1, train loss = 1.116971e+01\n",
      "0.057973604019870936 0.047656794415029645 0.02663656829514242\n",
      "it 1, train loss = 1.155176e+01\n",
      "0.04711088625666857 0.0487108308336839 0.024639311199916154\n",
      "it 1, train loss = 1.075210e+01\n",
      "0.05853285690622021 0.047378575509985345 0.026490926897483804\n",
      "it 1, train loss = 1.173547e+01\n",
      "0.058806260088251905 0.04720839114793752 0.026299516632301734\n",
      "it 1, train loss = 1.161972e+01\n",
      "0.058592010949211334 0.04733576537766446 0.02649551835193324\n",
      "it 1, train loss = 1.130399e+01\n",
      "0.047593616597807066 0.04955538262183383 0.024315302355260856\n",
      "it 1, train loss = 1.057306e+01\n",
      "0.04760055762812103 0.04886980848152254 0.024622847752722547\n",
      "it 1, train loss = 1.147158e+01\n",
      "0.0572992447370955 0.04891440451835628 0.0277349797412119\n",
      "it 1, train loss = 1.114427e+01\n",
      "0.0477726405930408 0.048895360678420575 0.024469801005616305\n",
      "it 1, train loss = 1.087431e+01\n",
      "0.04772785809088312 0.04889026525535603 0.024560620344112355\n",
      "it 1, train loss = 1.103495e+01\n",
      "0.047587536672777125 0.04885153769606971 0.024574352235260024\n",
      "42 [1 2 3 1]\n",
      "it 1, train loss = 1.124303e+01\n",
      "0.0583022238755931 0.047478852818351944 0.02658896109363219\n",
      "it 1, train loss = 1.155932e+01\n",
      "0.042840237620527334 0.049589669448433814 0.0225843444456227\n",
      "it 1, train loss = 1.110239e+01\n",
      "0.05850674490032359 0.04750949632916233 0.026202149478893256\n",
      "it 1, train loss = 1.101296e+01\n",
      "0.055177662084524466 0.04886910372913756 0.026622315904981677\n",
      "it 1, train loss = 1.242847e+01\n",
      "0.0454033822568935 0.04908670921290093 0.024078566395372918\n",
      "it 1, train loss = 1.105176e+01\n",
      "0.0475775578319713 0.04897715589529385 0.024486948289868127\n",
      "it 1, train loss = 1.125960e+01\n",
      "0.047335112319951224 0.048645423939465866 0.024644303346555367\n",
      "it 1, train loss = 1.048103e+01\n",
      "0.047674940488891926 0.04892740637641298 0.024453736861406766\n",
      "it 1, train loss = 1.068964e+01\n",
      "0.04773720366256978 0.05023143604920531 0.024050784452406455\n",
      "it 1, train loss = 1.110357e+01\n",
      "0.05758889558132174 0.04774019898931268 0.026638561192780314\n",
      "it 1, train loss = 1.068346e+01\n",
      "0.04596360056156597 0.04926290879093725 0.02397495364539868\n",
      "it 1, train loss = 1.077674e+01\n",
      "0.05862001423975107 0.04729849584627546 0.026413308792222773\n",
      "it 1, train loss = 1.157641e+01\n",
      "0.04480061971517158 0.049394869246335425 0.023674951242203472\n",
      "it 1, train loss = 1.116220e+01\n",
      "0.05776593484547669 0.047705791964794624 0.026687387143665395\n",
      "it 1, train loss = 1.027677e+01\n",
      "0.05802449272305199 0.04754807562564515 0.026557032986769986\n",
      "it 1, train loss = 1.089393e+01\n",
      "0.045226402095390805 0.049516703452731486 0.022203577096008835\n",
      "it 1, train loss = 1.225710e+01\n",
      "0.04550011842311697 0.04913401166722664 0.02410241583748585\n",
      "it 1, train loss = 1.079488e+01\n",
      "0.05922996900537955 0.04715751528614692 0.02660708442856924\n",
      "it 1, train loss = 1.053924e+01\n",
      "0.047507319442526785 0.04891573219927735 0.02451668496438\n",
      "it 1, train loss = 9.667487e+00\n",
      "0.0459594158564658 0.04896372128336611 0.024344830799841514\n",
      "it 1, train loss = 1.101248e+01\n",
      "0.04835479921917281 0.05119026181122779 0.024388533466524188\n",
      "it 1, train loss = 1.147724e+01\n",
      "0.04766682509085068 0.048965430367356955 0.024453419610822\n",
      "it 1, train loss = 1.138211e+01\n",
      "0.05818450455734224 0.0474900004958027 0.026486073894854776\n",
      "it 1, train loss = 1.207396e+01\n",
      "0.0470664894798766 0.04971025307711162 0.02415296622557517\n",
      "it 1, train loss = 1.073532e+01\n",
      "0.058331499713133034 0.047590183358086006 0.02634034475505117\n",
      "it 1, train loss = 1.148034e+01\n",
      "0.04581201838144274 0.04899700054546812 0.024270619054319726\n",
      "it 1, train loss = 1.061049e+01\n",
      "0.04813210219017352 0.050229934097614004 0.024034695120372283\n",
      "it 1, train loss = 1.049605e+01\n",
      "0.05742435187042469 0.047881732392346554 0.026749298759944307\n",
      "it 1, train loss = 1.131456e+01\n",
      "0.05834740740176084 0.04760925184818821 0.026331434789781206\n",
      "it 1, train loss = 1.215879e+01\n",
      "0.05849347452474916 0.047343359865404486 0.026461118876870002\n",
      "it 1, train loss = 1.066498e+01\n",
      "0.047580160461032917 0.049235082409143344 0.024291075869755447\n",
      "it 1, train loss = 1.044371e+01\n",
      "0.0585275635547821 0.04738242676674996 0.02652704256649901\n",
      "it 1, train loss = 1.089236e+01\n",
      "0.058415715394548935 0.047343809555036045 0.026420729571068162\n",
      "it 1, train loss = 1.088255e+01\n",
      "0.044770225603999556 0.04900227652351902 0.02421852762635267\n",
      "it 1, train loss = 1.068546e+01\n",
      "0.047521798749559835 0.05001591598500847 0.024041986882521422\n",
      "it 1, train loss = 1.039962e+01\n",
      "0.05658709618486526 0.04888205520690614 0.027254552156264673\n",
      "it 1, train loss = 1.019338e+01\n",
      "0.04756837023310968 0.048818238868373276 0.024520751878570692\n",
      "it 1, train loss = 1.065346e+01\n",
      "0.04772264965758713 0.04891922732088873 0.02450153448411561\n",
      "it 1, train loss = 1.088736e+01\n",
      "0.05691565003690577 0.048573970787097 0.027092072310055643\n",
      "it 1, train loss = 1.160602e+01\n",
      "0.0583946409109348 0.04740706713354224 0.026450230571443755\n",
      "it 1, train loss = 1.089817e+01\n",
      "0.045171067526759874 0.04956554333386056 0.024460692643018252\n",
      "it 1, train loss = 1.085094e+01\n",
      "0.04618457655437053 0.04880479362127287 0.024472492314081173\n",
      "it 1, train loss = 1.048030e+01\n",
      "0.058326222651301496 0.047384151683433406 0.0264472715717401\n",
      "it 1, train loss = 1.075947e+01\n",
      "0.04752513873873774 0.049300202686946015 0.02430608639126382\n",
      "it 1, train loss = 1.004503e+01\n",
      "0.058277162320972006 0.04740679111715451 0.026445573800187947\n",
      "it 1, train loss = 1.135901e+01\n",
      "0.0587832878206418 0.0475180761092251 0.026871832453599972\n",
      "it 1, train loss = 1.046930e+01\n",
      "0.05778155215887192 0.04773070252868645 0.02662553484317192\n",
      "it 1, train loss = 1.092634e+01\n",
      "0.05671002051403929 0.04831685319314343 0.02671287936334385\n",
      "it 1, train loss = 1.107207e+01\n",
      "0.04710471590793722 0.048838581601890876 0.024540420921316215\n",
      "it 1, train loss = 1.094580e+01\n",
      "0.057731095078066126 0.04770551055505647 0.0266555690991147\n",
      "52 [1, 3, 3, 1]\n",
      "it 1, train loss = 1.024727e+01\n",
      "0.04518034015796818 0.04917249268290834 0.022919729042062822\n",
      "it 1, train loss = 1.243733e+01\n",
      "0.04512713203850932 0.049212508203205094 0.02342702008952314\n",
      "it 1, train loss = 1.147191e+01\n",
      "0.05849879164170543 0.047349840962186675 0.026469191078786123\n",
      "it 1, train loss = 1.114862e+01\n",
      "0.0586653505598493 0.047342213559832376 0.026510803722034192\n",
      "it 1, train loss = 1.108245e+01\n",
      "0.05848526511430039 0.047360986655903284 0.02647599604679083\n",
      "it 1, train loss = 1.139328e+01\n",
      "0.04715670111001324 0.04874359855766699 0.024622056939525194\n",
      "it 1, train loss = 1.074920e+01\n",
      "0.04733789688510731 0.048940104134755406 0.024494883606929408\n",
      "it 1, train loss = 1.110259e+01\n",
      "0.05842345489879639 0.047408143402672874 0.02653419637001177\n",
      "it 1, train loss = 1.089714e+01\n",
      "0.043275832974234735 0.050165998113706735 0.023629995980737412\n",
      "it 1, train loss = 1.068206e+01\n",
      "0.04453572308186569 0.049149754103411866 0.02369968692886505\n",
      "it 1, train loss = 1.076043e+01\n",
      "0.04524153250012176 0.04896779463191597 0.024256598192048163\n",
      "it 1, train loss = 1.068447e+01\n",
      "0.05861741145249768 0.04727687077740164 0.026370041478635125\n",
      "it 1, train loss = 1.151317e+01\n",
      "0.045959241423046124 0.049253883950533206 0.024137550828214217\n",
      "it 1, train loss = 1.072182e+01\n",
      "0.047435933165075654 0.049538939279084246 0.02436971368869088\n",
      "it 1, train loss = 1.041669e+01\n",
      "0.04778875022036868 0.04931965925592522 0.024374908253800755\n",
      "it 1, train loss = 1.118597e+01\n",
      "0.04518310872501516 0.04901144735706825 0.022619556959117597\n",
      "it 1, train loss = 1.058144e+01\n",
      "0.04421896516735016 0.049020808256881244 0.023755733397192152\n",
      "it 1, train loss = 1.165312e+01\n",
      "0.0474820805863674 0.04888587104554691 0.024506770380249232\n",
      "it 1, train loss = 1.075663e+01\n",
      "0.05861575416615478 0.04734943185977101 0.02646053917340844\n",
      "it 1, train loss = 1.103636e+01\n",
      "0.04768909965262212 0.048877196745066666 0.024521144340484472\n",
      "it 1, train loss = 1.141091e+01\n",
      "0.057260233413423386 0.048187551339942396 0.02687253248537469\n",
      "it 1, train loss = 1.094187e+01\n",
      "0.047439765837982734 0.049101707698830765 0.02443690742283474\n",
      "it 1, train loss = 1.066277e+01\n",
      "0.05789204321266725 0.047678674251955204 0.02669734211758967\n",
      "it 1, train loss = 1.151167e+01\n",
      "0.058323125591560145 0.047474110734897844 0.02652015004289238\n",
      "it 1, train loss = 1.172473e+01\n",
      "0.0472532543411194 0.04865079898402385 0.024690004892549983\n",
      "it 1, train loss = 1.144959e+01\n",
      "0.05859805866328367 0.04729883758461851 0.026394323701079962\n",
      "it 1, train loss = 1.094680e+01\n",
      "0.05805782247368185 0.04815104359737339 0.026940291904361154\n",
      "it 1, train loss = 1.112691e+01\n",
      "0.05820143723441079 0.04755286612826645 0.026553738464853597\n",
      "it 1, train loss = 1.233053e+01\n",
      "0.0440282321614831 0.049268175488065336 0.024062007681079984\n",
      "it 1, train loss = 1.034671e+01\n",
      "0.04526828000491062 0.04922846542294059 0.023427190408267035\n",
      "it 1, train loss = 9.924405e+00\n",
      "0.04747873999528159 0.04896078200566937 0.02439434449905951\n",
      "it 1, train loss = 1.048052e+01\n",
      "0.05832864495226806 0.04745503643564473 0.026532082027514173\n",
      "it 1, train loss = 1.084098e+01\n",
      "0.043536097100368494 0.04972873990633916 0.023758955735327998\n",
      "it 1, train loss = 1.181941e+01\n",
      "0.05816537028728829 0.04754604624282321 0.026641777932223\n",
      "it 1, train loss = 9.901938e+00\n",
      "0.047765120979351365 0.04893722295825197 0.02450619858495822\n",
      "it 1, train loss = 1.078885e+01\n",
      "0.05857158650768274 0.04736038951749736 0.026488840399663285\n",
      "it 1, train loss = 1.144050e+01\n",
      "0.047938392125117434 0.0486572614454045 0.024743944081445073\n",
      "it 1, train loss = 1.125712e+01\n",
      "0.04458908848417138 0.049071040110182096 0.02388392344545231\n",
      "it 1, train loss = 1.099019e+01\n",
      "0.05862389840109875 0.04726939024476862 0.02633587778078619\n",
      "it 1, train loss = 1.160786e+01\n",
      "0.04490723480842195 0.04940598937417178 0.024276057680138605\n",
      "it 1, train loss = 1.068892e+01\n",
      "0.057742827271531974 0.04778435617335405 0.02666293898652784\n",
      "it 1, train loss = 1.070201e+01\n",
      "0.04774607408964668 0.04900515330763339 0.0245101567725235\n",
      "it 1, train loss = 1.094457e+01\n",
      "0.047095823657512866 0.04870085537235618 0.024592480526800024\n",
      "it 1, train loss = 1.052128e+01\n",
      "0.047090339397901075 0.04873775331228125 0.024636589961406937\n",
      "it 1, train loss = 1.100081e+01\n",
      "0.047396980133823964 0.04884794160430677 0.024532142233352566\n",
      "it 1, train loss = 1.031501e+01\n",
      "0.05831758527619203 0.04743490755223174 0.026470503465286697\n",
      "it 1, train loss = 1.087314e+01\n",
      "0.052736528037449715 0.04984047245605187 0.026052444036409023\n",
      "it 1, train loss = 1.090581e+01\n",
      "0.05822625509346006 0.04748884894784894 0.026566351613311563\n",
      "it 1, train loss = 1.118212e+01\n",
      "0.044230004744561534 0.04941711517232511 0.02351082909715601\n",
      "it 1, train loss = 1.114376e+01\n",
      "0.058406166866334444 0.047394731698176366 0.026522555918018813\n"
     ]
    }
   ],
   "source": [
    "# Next: find the standard deviation of R2 from 50 runs for each case.\n",
    "# Takes more than 3 hours to finish\n",
    "lamUT_vec = np.array(UTdata['F11'])\n",
    "lamET_vec = np.array(ETdata['F11'])\n",
    "lamPS_vec = np.array(PSdata['F11'])\n",
    "np_list = []\n",
    "r2_list_global = []\n",
    "mae_list_global = []\n",
    "for n in range(10,50):\n",
    "    n_params, layers = opt_arch_icnn(n)\n",
    "    if n_params in np_list:\n",
    "        continue\n",
    "    print(n_params, layers)\n",
    "    np_list.append(n_params)\n",
    "\n",
    "    r2_list_arch = []\n",
    "    mae_list_arch = []\n",
    "    for i in range(50): #50 runs for every architecture\n",
    "        key, subkey = random.split(key)\n",
    "        params = init_icnn(key, layers)\n",
    "\n",
    "        # Doesn't work for ICNN\n",
    "        # # Make sure you are starting at a good place\n",
    "        # loss = loss_P11_all(params, F11_data, 2)\n",
    "        # while loss>10.0:\n",
    "        #     key, subkey = random.split(key)\n",
    "        #     params = init_icnn(key, layers)\n",
    "        #     loss = loss_P11_all(params, F11_data, 2)\n",
    "\n",
    "\n",
    "        opt_init, opt_update, get_params = optimizers.adam(2.e-4) #Original: 2.e-4\n",
    "        opt_state = opt_init(params)\n",
    "        params, train_loss, val_loss = train_jp(loss_P11_all, 2, F11_data, opt_state, key, nIter = 100000, print_freq=200000)\n",
    "\n",
    "        model = ICNN_model(params[0], params[1], normalization)\n",
    "\n",
    "        lambdas     = [lamUT_vec,       lamET_vec,      lamPS_vec       ]\n",
    "        P11_gt_list = [UTdata['P11'],   ETdata['P11'],  PSdata['P11']   ]\n",
    "        P11funs     = [P11_UT,          P11_ET,         P11_PS          ]\n",
    "\n",
    "        r2 = []\n",
    "        mae = []\n",
    "        for lam, P11_gt, P11fun in zip(lambdas, P11_gt_list, P11funs):\n",
    "            P11 = P11fun(lam, model, normalization)\n",
    "            r2i = r2_score(P11_gt, P11)\n",
    "            r2i = np.clip(r2i, a_min=0.0, a_max=1.0)\n",
    "            r2.append(r2i)\n",
    "\n",
    "            maei = onp.mean(onp.abs(P11_gt-P11))\n",
    "            mae.append(maei)\n",
    "        # r2 = np.mean(np.array(r2))\n",
    "        # print(*r2)\n",
    "        print(*mae)\n",
    "        r2_list_arch.append(r2)\n",
    "        mae_list_arch.append(mae)\n",
    "    r2_list_global.append(r2_list_arch)\n",
    "    mae_list_global.append(mae_list_arch)\n",
    "\n",
    "icnn_r2 = np.array(r2_list_global)\n",
    "icnn_mae = np.array(mae_list_global)\n",
    "\n",
    "# icnn_r2 = np.expand_dims(icnn_r2,axis=2)\n",
    "# icnn_mae = np.expand_dims(icnn_mae,axis=2)\n",
    "with open('savednet/ICNN_r2_efficiency_ALL.npy', 'wb') as f:\n",
    "    pickle.dump([np_list, icnn_r2], f)\n",
    "\n",
    "with open('savednet/ICNN_mae_efficiency_ALL.npy', 'wb') as f:\n",
    "    pickle.dump([np_list, icnn_mae], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 1, train loss = 7.560359e+00\n",
      "0.02983575442628569 45108968060487.5 0.36112477926273356\n",
      "it 1, train loss = 8.246256e+00\n",
      "0.026701970741370203 1.547993928799463e+21 0.49582414226473787\n",
      "it 1, train loss = 6.468798e+00\n",
      "0.029836637041179205 47714168205416.36 0.3601343938283202\n",
      "it 1, train loss = 7.941649e+00\n",
      "0.02983333792319999 43022063205459.96 0.3653002013250592\n",
      "it 1, train loss = 8.304535e+00\n",
      "0.029832272004711155 37139392031659.99 0.36671333070520973\n",
      "it 1, train loss = 7.049740e+00\n",
      "0.029838963979315337 50146555284304.98 0.3564821660136338\n",
      "it 1, train loss = 7.581303e+00\n",
      "0.02711688769883924 3.799561783023112e+23 0.6085336549094154\n",
      "it 1, train loss = 2.453135e+13\n",
      "0.02825030637985872 2.1374226102039919e+21 0.5657088946862231\n",
      "it 1, train loss = 6.839754e+00\n",
      "0.029831812265040093 47192637748729.445 0.367941130336675\n",
      "it 1, train loss = 1.783890e+00\n",
      "0.029839448197098634 46376222618253.16 0.3548791886183333\n",
      "it 1, train loss = 2.043411e+00\n",
      "0.0294280587924708 2.4876726154245586e+21 0.48606783346553645\n",
      "it 1, train loss = 6.921527e+13\n",
      "0.02726796724671233 1.5872959675087622e+19 0.3570569390485633\n",
      "it 1, train loss = 4.372327e+00\n",
      "0.027854117619149462 2.743650801380039e+20 0.47436575111048596\n",
      "it 1, train loss = 7.810280e+00\n",
      "0.02836495087596453 9.247613862939255e+20 0.4779074701090433\n",
      "it 1, train loss = 5.891686e+00\n",
      "0.02736556795797408 4.838903109200521e+21 0.6516128010057349\n",
      "it 1, train loss = 7.303552e+00\n",
      "0.029836439514508477 47648602729189.99 0.3604437247983364\n",
      "it 1, train loss = 6.180409e+04\n",
      "0.028429385062726614 3.118965832311934e+22 0.6145965965021265\n",
      "it 1, train loss = 4.467558e+00\n",
      "0.029824947025820236 28126278802690.97 0.37796123519420205\n",
      "it 1, train loss = 6.111365e+00\n",
      "0.029095473383895173 6.334606831858758e+21 0.5267037792265338\n",
      "it 1, train loss = 3.499325e+25\n",
      "0.3039512564014911 0.3955366906816766 0.4302748560837092\n",
      "it 1, train loss = 4.193930e+05\n",
      "0.02729289078481677 2.7141947865689955e+20 0.5034585386504012\n",
      "it 1, train loss = 2.907482e+00\n",
      "0.02983392314731624 36356310515422.88 0.3640168302032502\n",
      "it 1, train loss = 1.441937e+03\n",
      "0.02760453473421904 2.707967337087981e+21 0.5683003266254291\n",
      "it 1, train loss = 1.145128e+02\n",
      "0.02862401653349992 9.798327411262048e+20 0.6013240962661192\n",
      "it 1, train loss = 6.239832e+15\n",
      "0.659354445625265 0.7226661293131367 0.6965375629537868\n",
      "it 1, train loss = 3.771603e-01\n",
      "0.028901845977691066 2.8689438685549964e+21 0.5449219143783687\n",
      "it 1, train loss = 3.836778e+05\n",
      "0.02981078318018171 577875521660812.6 0.3857944874928733\n",
      "it 1, train loss = 3.059195e+04\n",
      "0.027226336263577684 9.17322196505236e+20 0.48613626750628974\n",
      "it 1, train loss = 6.625360e+08\n",
      "0.02790464884816216 2.909962490786753e+22 0.6316478828056081\n",
      "it 1, train loss = 4.334098e+05\n",
      "0.02873154288987295 1.4271763445452682e+22 0.5977365169000262\n",
      "it 1, train loss = 1.906267e+03\n",
      "0.027259826637827844 1.0284315882736e+22 0.6110607572121484\n",
      "it 1, train loss = 6.343774e+03\n",
      "0.02983294725334367 172750065034555.38 0.36881256180869254\n",
      "it 1, train loss = 3.382503e+07\n",
      "0.029326094111396497 1.6633006973186514e+22 0.5321807487088531\n",
      "it 1, train loss = 4.356210e+18\n",
      "1.0240863336772847 0.8375856186356443 0.750184902438509\n",
      "it 1, train loss = 1.915368e+00\n",
      "0.029947664352268318 39007623456230.56 0.37187783870807956\n",
      "it 1, train loss = 6.377229e+00\n",
      "0.027211264522327316 1.5605498908271403e+23 0.5738633857847975\n",
      "it 1, train loss = 3.376766e+02\n",
      "0.027070502859153692 1.4985222861913817e+24 0.7662095822276703\n",
      "it 1, train loss = 6.971818e+00\n",
      "0.029835084447084003 48434897414681.79 0.362712265673641\n",
      "it 1, train loss = 8.542648e+00\n",
      "0.028532679741440647 7.676200298720229e+21 0.5506534072324953\n",
      "it 1, train loss = 5.035182e+00\n",
      "0.029832618729853246 41851525169442.99 0.36632599621917444\n",
      "it 1, train loss = 1.664014e+02\n",
      "0.02735094746919582 3.3447828081862225e+21 0.5488967873667377\n",
      "it 1, train loss = 4.083046e+00\n",
      "0.02662733444705253 1.0998659291536233e+23 0.9030854205720529\n",
      "it 1, train loss = 2.857076e+01\n",
      "0.02911858906925053 3.224632207113612e+21 0.5287582729783608\n",
      "it 1, train loss = 1.004818e+02\n",
      "0.02665169041253802 1.512770369853006e+23 0.7019136897332492\n",
      "it 1, train loss = 5.594906e+00\n",
      "0.02983466309123693 44059415339067.68 0.36320681766436946\n",
      "it 1, train loss = 1.463337e+03\n",
      "0.027530970729714967 7.455104109182576e+21 0.529293237639598\n",
      "it 1, train loss = 3.571742e+01\n",
      "0.02747735202736167 3.916180133238496e+20 0.6413333696148849\n",
      "it 1, train loss = 7.521370e+00\n",
      "0.02984070975622885 49439441231727.44 0.35365733288090745\n",
      "it 1, train loss = 9.016192e+02\n",
      "0.026816187341262916 1.6704597206206893e+22 0.4681917125303331\n",
      "it 1, train loss = 3.529430e+00\n",
      "0.029841621130414168 45656412888304.91 0.3520134216615041\n"
     ]
    }
   ],
   "source": [
    "lamUT_vec = np.array(UTdata['F11'])\n",
    "lamET_vec = np.array(ETdata['F11'])\n",
    "lamPS_vec = np.array(PSdata['F11'])\n",
    "r2_list = []\n",
    "mae_list = []\n",
    "for i in range(50): #50 runs for every architecture\n",
    "    key, subkey = random.split(key)\n",
    "    params = init_cann(key)\n",
    "\n",
    "    opt_init, opt_update, get_params = optimizers.adam(2.e-4) #Original: 1.e-4\n",
    "    opt_state = opt_init(params)\n",
    "    params, train_loss, val_loss = train_jp(loss_P11_UT, 1, F11_data, opt_state, key, nIter = 100000, print_freq=1000000)\n",
    "\n",
    "    model = CANN_model(params[0], params[1], normalization)\n",
    "\n",
    "    lambdas     = [lamUT_vec,       lamET_vec,      lamPS_vec       ]\n",
    "    P11_gt_list = [UTdata['P11'],   ETdata['P11'],  PSdata['P11']   ]\n",
    "    P11funs     = [P11_UT,          P11_ET,         P11_PS          ]\n",
    "\n",
    "    r2 = []\n",
    "    mae = []\n",
    "    for lam, P11_gt, P11fun in zip(lambdas, P11_gt_list, P11funs):\n",
    "        P11 = P11fun(lam, model, normalization)\n",
    "        r2i = r2_score(P11_gt, P11)\n",
    "        r2i = np.clip(r2i, a_min=0.0, a_max=1.0)\n",
    "        r2.append(r2i)\n",
    "\n",
    "        maei = onp.mean(onp.abs(P11_gt-P11))\n",
    "        mae.append(maei)\n",
    "    # r2 = np.mean(np.array(r2))\n",
    "    print(*mae)\n",
    "    r2_list.append(r2)\n",
    "    mae_list.append(mae)\n",
    "cann_r2 = np.array(r2_list)\n",
    "cann_mae = np.array(mae_list)\n",
    "with open('savednet/CANN_r2_efficiency_UT.npy', 'wb') as f:\n",
    "    pickle.dump([[24], cann_r2], f)\n",
    "\n",
    "with open('savednet/CANN_mae_efficiency_UT.npy', 'wb') as f:\n",
    "    pickle.dump([[24], cann_mae], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 [1 1 2 1]\n",
      "it 1, train loss = 6.775323e-01\n",
      "0.07249911045745248 0.20844797620124306 0.0452637882866115\n",
      "it 1, train loss = 1.842115e+00\n",
      "0.07249920252094985 0.20844398550167728 0.045266004083310385\n",
      "it 1, train loss = 1.284460e+00\n",
      "0.07249443845069937 0.20844878888080606 0.04527554990964422\n",
      "it 1, train loss = 2.076778e+00\n",
      "0.07249824579081618 0.208443033198207 0.04526794739295979\n",
      "it 1, train loss = 9.797781e-01\n",
      "0.07265852275657536 0.20826239805008237 0.044586990309991754\n",
      "it 1, train loss = 1.820764e+00\n",
      "0.0724981745162183 0.2084479105849071 0.04526836323133052\n",
      "it 1, train loss = 1.762222e+00\n",
      "0.07244079091476519 0.208530312185181 0.04535083545693709\n",
      "it 1, train loss = 7.782333e-01\n",
      "0.07238995277685559 0.20863223114885443 0.04539517079583787\n",
      "it 1, train loss = 1.286305e-01\n",
      "0.07250401850716191 0.20845380932486812 0.045241180804512005\n",
      "it 1, train loss = 2.013894e+00\n",
      "0.07249817125789662 0.20844494114309253 0.04526811231588417\n",
      "it 1, train loss = 2.039344e+00\n",
      "0.07249975414636706 0.2084475869495241 0.04526094077128717\n",
      "it 1, train loss = 7.627252e-01\n",
      "0.03981968540563877 3.4349880921839695 0.21095931194607218\n",
      "it 1, train loss = 2.848959e+00\n",
      "0.07249655210040838 0.20844028791114055 0.04527374203911884\n",
      "it 1, train loss = 8.119874e-01\n",
      "0.04004667277332984 3.3988805189899276 0.20833520144171633\n",
      "it 1, train loss = 9.542156e-01\n",
      "0.07250028051392642 0.20844631003234562 0.04525990812032031\n",
      "it 1, train loss = 4.365119e-01\n",
      "0.07250317833756031 0.20845327170229935 0.045247279930603665\n",
      "it 1, train loss = 8.800030e-02\n",
      "0.07250934041374536 0.20845816148002363 0.04521488698763978\n",
      "it 1, train loss = 3.898115e-01\n",
      "0.07250216651822797 0.20844934789105113 0.045249531867259586\n",
      "it 1, train loss = 1.877630e+00\n",
      "0.07266178016218744 0.20827674818432762 0.044598521442355425\n",
      "it 1, train loss = 5.580991e-01\n",
      "0.03982595962573163 3.433409074983454 0.21085587477233977\n",
      "it 1, train loss = 2.915954e+00\n",
      "0.07249643699193184 0.208438896231974 0.045275642790153565\n",
      "it 1, train loss = 2.351624e+00\n",
      "0.07244268873805991 0.20853074128739604 0.0453462535321547\n",
      "it 1, train loss = 2.636330e+00\n",
      "0.07249330788511431 0.20845199705393824 0.04527944974701266\n",
      "it 1, train loss = 1.636292e+00\n",
      "0.0724720500833629 0.2085018064026095 0.04528541301959272\n",
      "it 1, train loss = 2.092948e+00\n",
      "0.07266282869471183 0.2082640541728804 0.04458491157660512\n",
      "it 1, train loss = 2.264522e+00\n",
      "0.07249862391961327 0.20845028759888085 0.045267024938122645\n",
      "it 1, train loss = 1.941191e+00\n",
      "0.07250106308559448 0.20844897409388152 0.045254982256378806\n",
      "it 1, train loss = 2.323745e-01\n",
      "0.07252285999465548 0.20842310381866969 0.04521658681902234\n",
      "it 1, train loss = 8.711274e-01\n",
      "0.07249896141680551 0.20845462846390803 0.04525641998899723\n",
      "it 1, train loss = 1.162752e+00\n",
      "0.07249980028432473 0.20844722684141243 0.045260763940380956\n",
      "it 1, train loss = 2.405610e+00\n",
      "0.04018706931765822 3.3748213541756287 0.206820884886949\n",
      "it 1, train loss = 3.877890e-01\n",
      "0.07250266090524998 0.20845140834856352 0.045247735187743394\n",
      "it 1, train loss = 1.958804e+00\n",
      "0.07249821930168066 0.20844323580653448 0.04526783847370826\n",
      "it 1, train loss = 8.258739e-01\n",
      "0.07249799622188022 0.20844447895702506 0.0452689095046946\n",
      "it 1, train loss = 1.446429e+00\n",
      "0.04009422831081691 3.3893499623693986 0.20782706693939826\n",
      "it 1, train loss = 5.531954e-01\n",
      "0.03991719244373626 3.4196390695806573 0.2098972986212848\n",
      "it 1, train loss = 7.343289e-01\n",
      "0.04002383264714649 3.4016631984308474 0.20866562156301877\n",
      "it 1, train loss = 3.840983e-02\n",
      "0.07250513229268432 0.20845874372946974 0.04523274734140418\n",
      "it 1, train loss = 5.157880e-01\n",
      "0.0724950484152588 0.20846715733632423 0.04525698562375708\n",
      "it 1, train loss = 1.544307e-01\n",
      "0.07250167835536449 0.20845027307497976 0.04525199351908412\n",
      "it 1, train loss = 6.896688e-01\n",
      "0.04008753441913394 3.3903832601406427 0.2078990415632351\n",
      "it 1, train loss = 1.378232e+00\n",
      "0.07249990046080262 0.20844734965707779 0.04526034945130038\n",
      "it 1, train loss = 1.188564e+00\n",
      "0.07266038301815649 0.20826183331752884 0.0445860515619469\n",
      "it 1, train loss = 1.721090e+00\n",
      "0.07266047417842277 0.20826224570903915 0.044586296532286025\n",
      "it 1, train loss = 2.853846e+00\n",
      "0.07249440972365956 0.20844378948593847 0.04528488693981086\n",
      "it 1, train loss = 2.356367e-01\n",
      "0.07250394786022336 0.20845291465701538 0.0452420097526376\n",
      "it 1, train loss = 1.286568e+00\n",
      "0.07250746360676805 0.20845426065159484 0.04522379585455787\n",
      "it 1, train loss = 5.361812e-01\n",
      "0.07249799284184161 0.20845226963792776 0.04526120475401157\n",
      "it 1, train loss = 2.200286e+00\n",
      "0.07266560214313275 0.20826461170215438 0.04458353689137749\n",
      "it 1, train loss = 2.602532e+00\n",
      "0.07249915654443076 0.20844954944362376 0.04526383662837612\n",
      "16 [1, 2, 2, 1]\n",
      "it 1, train loss = 2.951057e+00\n",
      "0.07262324538135033 0.2082522760156472 0.04458087508798033\n",
      "it 1, train loss = 1.025871e+00\n",
      "0.03494577964344031 4.203069419447487 0.26360153910899436\n",
      "it 1, train loss = 2.585386e+00\n",
      "0.07263069373533498 0.20819452414652637 0.044517294914423626\n",
      "it 1, train loss = 2.056535e+00\n",
      "0.0435302165621918 2.9301723990346007 0.1764802196915936\n",
      "it 1, train loss = 3.716138e-01\n",
      "0.03522299932690959 4.166378574387691 0.26109306957355644\n",
      "it 1, train loss = 1.720058e+00\n",
      "0.030844102481424388 2.923095502465109 0.16299086996920398\n",
      "it 1, train loss = 2.014841e+00\n",
      "0.07262343125840708 0.20825332034092195 0.04458088011454992\n",
      "it 1, train loss = 2.310146e+00\n",
      "0.07236451314285014 0.2083018824591264 0.04561588093328517\n",
      "it 1, train loss = 2.790387e-01\n",
      "0.07262263389797416 0.20825628728202955 0.044581365786753605\n",
      "it 1, train loss = 8.197996e-01\n",
      "0.03517308039700317 4.193078642072329 0.26301150154760483\n",
      "it 1, train loss = 3.952081e-01\n",
      "0.030406568146697323 3.1742606857773197 0.18129489443457242\n",
      "it 1, train loss = 2.952546e-02\n",
      "0.03478775399431555 4.231286665155036 0.2655376145533925\n",
      "it 1, train loss = 7.469152e-01\n",
      "0.03324241341572114 4.504859875559468 0.2842245097555679\n",
      "it 1, train loss = 4.065520e-01\n",
      "0.03305127901760364 4.550259801861581 0.2873876559051106\n",
      "it 1, train loss = 1.651144e+00\n",
      "0.07262358744319516 0.20825321021642676 0.04458082061573839\n",
      "it 1, train loss = 2.539535e+00\n",
      "0.034992071900405375 4.193916436219811 0.26297774951624\n",
      "it 1, train loss = 1.548365e+00\n",
      "0.0723450826330951 0.20807410186182834 0.04565369951101274\n",
      "it 1, train loss = 1.405318e+00\n",
      "0.033726031285334045 4.428254981261435 0.2790457941156876\n",
      "it 1, train loss = 2.346181e+00\n",
      "0.04027796344249261 3.3602474579471626 0.2058144570708001\n",
      "it 1, train loss = 1.656608e+00\n",
      "0.034709726362702684 4.252985052283899 0.2670378249376853\n",
      "it 1, train loss = 1.224713e+00\n",
      "0.07262326850840382 0.2082544246671174 0.04458159772170774\n",
      "it 1, train loss = 2.183538e+00\n",
      "0.03503942781013924 4.189186428817877 0.262654246387758\n",
      "it 1, train loss = 2.425977e+00\n",
      "0.07230111118080414 0.20798726919359592 0.04571449793320754\n",
      "it 1, train loss = 2.791537e+00\n",
      "0.07262413877287498 0.20825300789576648 0.044580600249583276\n",
      "it 1, train loss = 2.130846e+00\n",
      "0.07262463744864867 0.20825304177712828 0.04458040295571008\n",
      "it 1, train loss = 2.420235e+00\n",
      "0.03429856423337931 4.321518149066711 0.2717291216998892\n",
      "it 1, train loss = 7.109040e-01\n",
      "0.03739202621634543 3.787072359096689 0.23507850959388468\n",
      "it 1, train loss = 1.192282e-01\n",
      "0.034999131102622254 4.195059258486575 0.2630746342964579\n",
      "it 1, train loss = 2.357680e+00\n",
      "0.07262481781370962 0.20825272132751074 0.04458031035313557\n",
      "it 1, train loss = 1.063613e-01\n",
      "0.03396768869490438 4.3172312259910095 0.2711995781621349\n",
      "it 1, train loss = 2.161126e+00\n",
      "0.03495059950274371 4.20333423025543 0.2636230741787525\n",
      "it 1, train loss = 1.127434e-01\n",
      "0.03375504210191926 4.4225077249633316 0.27864056489877936\n",
      "it 1, train loss = 5.594547e-02\n",
      "0.07262025352328833 0.20825932607513967 0.04458221122447586\n",
      "it 1, train loss = 1.322085e+00\n",
      "0.04353813007176164 2.929496221048299 0.17643260931405866\n",
      "it 1, train loss = 1.316025e+00\n",
      "0.07262399941624151 0.20825529566846993 0.044580817122730994\n",
      "it 1, train loss = 8.158724e-02\n",
      "0.03370882859293919 4.433897006043418 0.27942992794654803\n",
      "it 1, train loss = 3.920861e-01\n",
      "0.07262235605777213 0.2082551394587001 0.044581372612419685\n",
      "it 1, train loss = 2.783874e+00\n",
      "0.034913782061415016 4.210848317948098 0.2641491108240391\n",
      "it 1, train loss = 1.119027e-01\n",
      "0.03468409029140918 4.249525969366839 0.2667886769377641\n",
      "it 1, train loss = 1.824865e+00\n",
      "0.07235587800250277 0.20810500175346305 0.04563432043919625\n",
      "it 1, train loss = 2.038493e+00\n",
      "0.06335026905182485 3.039387553054391 0.17801164720058768\n",
      "it 1, train loss = 1.199066e-01\n",
      "0.030691710483389114 3.0285390836716677 0.17073934723221112\n",
      "it 1, train loss = 1.225353e+00\n",
      "0.07262247567718792 0.20825390142256844 0.04458125329207661\n",
      "it 1, train loss = 3.951387e-01\n",
      "0.03518232121569685 4.1691602583037835 0.26127894916639355\n",
      "it 1, train loss = 1.772560e-01\n",
      "0.03508576533085797 4.182240533190224 0.26217716385985507\n",
      "it 1, train loss = 8.887463e-01\n",
      "0.035120783261078 4.180573525645637 0.2620779718387151\n",
      "it 1, train loss = 7.281924e-01\n",
      "0.07262385281491764 0.20825388339585513 0.044580767449711775\n",
      "it 1, train loss = 5.849245e-01\n",
      "0.03365326996759924 4.427577107400062 0.2789355409129897\n",
      "it 1, train loss = 5.805325e-01\n",
      "0.03491410889050934 4.211367977996407 0.2641748753994974\n",
      "it 1, train loss = 2.791840e+00\n",
      "0.035077145637065134 4.185835830096673 0.2624292065221889\n",
      "22 [1 3 2 1]\n",
      "it 1, train loss = 6.118870e-01\n",
      "0.07261435279786638 0.20825166332671785 0.04457764718595851\n",
      "it 1, train loss = 3.148153e-01\n",
      "0.07261294021129822 0.2082522260561749 0.044578065420995815\n",
      "it 1, train loss = 2.823793e+00\n",
      "0.07261423923134847 0.2082505641679822 0.04457760593885772\n",
      "it 1, train loss = 2.444788e+00\n",
      "0.07261441847898151 0.2082499601874693 0.04457759272410122\n",
      "it 1, train loss = 1.349682e+00\n",
      "0.03378683639365395 4.4277304953188725 0.2789987880378736\n",
      "it 1, train loss = 1.599567e-01\n",
      "0.0726118593708734 0.20826342172347753 0.04458977688575113\n",
      "it 1, train loss = 1.621486e+00\n",
      "0.07261654297877668 0.20822424743061307 0.04454702597974562\n",
      "it 1, train loss = 1.738003e+00\n",
      "0.0726134377418636 0.20825023964939052 0.04457864475252288\n",
      "it 1, train loss = 2.464997e-01\n",
      "0.03053481959877191 3.0415934652711174 0.17147393208276873\n",
      "it 1, train loss = 7.479558e-01\n",
      "0.07261310370568061 0.20825459177683034 0.044581701189061136\n",
      "it 1, train loss = 2.455670e+00\n",
      "0.0726149313438874 0.20824995255684253 0.04457733884599923\n",
      "it 1, train loss = 1.316453e+00\n",
      "0.07261389002610812 0.20825093197074898 0.044577449388822236\n",
      "it 1, train loss = 1.385056e+00\n",
      "0.07261413183502369 0.20825268122082197 0.044577783093479324\n",
      "it 1, train loss = 3.562622e-01\n",
      "0.032969870673734934 4.550604081414069 0.2873733671454054\n",
      "it 1, train loss = 2.226387e+00\n",
      "0.03174263646903959 4.80754059055948 0.30494533095399085\n",
      "it 1, train loss = 2.315295e+00\n",
      "0.03228531679004518 4.682772548938382 0.2964467780341897\n",
      "it 1, train loss = 2.300636e+00\n",
      "0.03301793822568809 4.55145790224555 0.28745063140822225\n",
      "it 1, train loss = 4.335090e-01\n",
      "0.0310453926539078 2.8265315202608456 0.15609901915888794\n",
      "it 1, train loss = 2.545373e+00\n",
      "0.028687146268306474 4.106131521016608 0.24765360136101996\n",
      "it 1, train loss = 2.756332e+00\n",
      "0.028229754960960477 6.14969687214154 0.3941884929401041\n",
      "it 1, train loss = 2.197221e-01\n",
      "0.030160978232972403 3.263470088552479 0.18752743276002878\n",
      "it 1, train loss = 2.191431e+00\n",
      "0.034545404924709006 4.267647926894221 0.2680043704238612\n",
      "it 1, train loss = 6.659588e-01\n",
      "0.03440184533884734 4.291960352324816 0.2696638440062464\n",
      "it 1, train loss = 4.449335e-01\n",
      "0.03098013686913708 2.8561292862676804 0.15826403822998938\n",
      "it 1, train loss = 4.214862e-01\n",
      "0.03471359295369397 4.247565353771682 0.2666555535914556\n",
      "it 1, train loss = 1.137535e+00\n",
      "0.033680218323351066 4.4218941304594885 0.2785702814717885\n",
      "it 1, train loss = 1.911135e-01\n",
      "0.035068858163159876 4.17790876185771 0.26187510415349097\n",
      "it 1, train loss = 5.013241e-01\n",
      "0.07261344186222307 0.20825191740826235 0.04457792296288189\n",
      "it 1, train loss = 1.177912e+00\n",
      "0.03044527550011134 3.1119567618209496 0.1766313941790218\n",
      "it 1, train loss = 2.828662e+00\n",
      "0.07261467334063695 0.20825248108692265 0.04457764252475306\n",
      "it 1, train loss = 5.714789e-01\n",
      "0.030628846529597364 3.025037533085079 0.17040005543296927\n",
      "it 1, train loss = 8.362450e-01\n",
      "0.03454884497994026 4.268360025504333 0.2680636959126793\n",
      "it 1, train loss = 1.221795e-01\n",
      "0.03365846243844579 4.440822835927925 0.27989030744094606\n",
      "it 1, train loss = 5.033589e-02\n",
      "0.03502014044698856 4.168926852909667 0.26119017219063606\n",
      "it 1, train loss = 5.236755e-01\n",
      "0.03441525089723095 4.293151797151933 0.26974462623485845\n",
      "it 1, train loss = 6.870515e-02\n",
      "0.03077889289595456 2.951418541806255 0.16512518223657255\n",
      "it 1, train loss = 2.555210e+00\n",
      "0.07261495269904363 0.20824967428984056 0.044577317069643754\n",
      "it 1, train loss = 2.014255e+00\n",
      "0.07261400143834024 0.20824989025763532 0.044577603238786456\n",
      "it 1, train loss = 1.195307e+00\n",
      "0.035386673241919624 4.131728116359587 0.25873936429910643\n",
      "it 1, train loss = 9.246381e-01\n",
      "0.030773029045399317 2.9499866791541516 0.1650043731901937\n",
      "it 1, train loss = 1.641838e+00\n",
      "0.03538087928564773 4.127252211048121 0.25841832341299253\n",
      "it 1, train loss = 1.529553e-01\n",
      "0.02910854539100023 4.199769154561778 0.25408766014102785\n",
      "it 1, train loss = 9.896261e-01\n",
      "0.07261375346209638 0.20825041977791697 0.04457773886253743\n",
      "it 1, train loss = 2.591310e+00\n",
      "0.0340334206048103 4.361634048861491 0.2744499737608063\n",
      "it 1, train loss = 5.416518e-02\n",
      "0.07261191783673816 0.2082544096921436 0.044578422893064865\n",
      "it 1, train loss = 1.004699e+00\n",
      "0.034217719994440454 4.334614776847457 0.2726055186762946\n",
      "it 1, train loss = 2.236535e+00\n",
      "0.03465344572947303 4.24356016233002 0.2663683903219187\n",
      "it 1, train loss = 2.245171e-01\n",
      "0.033810756384384806 4.30430080794898 0.27015060490815607\n",
      "it 1, train loss = 1.668730e+00\n",
      "0.03176509407542376 4.790099299006294 0.30376347115830793\n",
      "it 1, train loss = 8.600043e-02\n",
      "0.0303242583254645 3.1746890636025737 0.181112057931819\n",
      "30 [1, 3, 3, 1]\n",
      "it 1, train loss = 1.353834e+00\n",
      "0.03498339564014291 4.202681598550475 0.26356731950727813\n",
      "it 1, train loss = 2.600899e-01\n",
      "0.03430555612675256 4.321405623698231 0.27168886780953455\n",
      "it 1, train loss = 6.986079e-01\n",
      "0.029846006225455735 3.431074259992293 0.19976686017250295\n",
      "it 1, train loss = 1.111614e+00\n",
      "0.03332061306946522 4.497728875693171 0.28375368658366346\n",
      "it 1, train loss = 1.095880e+00\n",
      "0.03398762296030313 4.3752176616099 0.27535695981591884\n",
      "it 1, train loss = 2.819189e+00\n",
      "0.03500045839856865 4.198230819392622 0.2632726032704296\n",
      "it 1, train loss = 2.632475e+00\n",
      "0.029477073173431546 3.6160383056092544 0.21281329271365276\n",
      "it 1, train loss = 2.579440e+00\n",
      "0.03398757045503076 4.373497042018417 0.27524970760946665\n",
      "it 1, train loss = 1.334322e+00\n",
      "0.034409638002723716 4.304331907931614 0.27050908279455044\n",
      "it 1, train loss = 1.012723e+00\n",
      "0.028618689483226035 4.990967982676399 0.3109588295898968\n",
      "it 1, train loss = 2.192166e+00\n",
      "0.03427263720119239 4.3230317539164576 0.2717847555064755\n",
      "it 1, train loss = 1.850385e-01\n",
      "0.03480268285852588 4.23015321634599 0.2654315651609737\n",
      "it 1, train loss = 3.830882e-01\n",
      "0.03356647724556308 4.460398498030981 0.28120455016296375\n",
      "it 1, train loss = 2.966218e+00\n",
      "0.03329072177142973 4.519241210165567 0.2852761076722671\n",
      "it 1, train loss = 2.914555e+00\n",
      "0.035173795946821976 4.168708000915376 0.26123706056903473\n",
      "it 1, train loss = 4.411204e-01\n",
      "0.028538365581124055 4.034120821472636 0.24264927830987393\n",
      "it 1, train loss = 1.925599e+00\n",
      "0.034343189185215166 4.312250702396791 0.27105184284702133\n",
      "it 1, train loss = 2.190534e+00\n",
      "0.030310849571643032 3.1911286628792075 0.1823567404873009\n",
      "it 1, train loss = 4.018030e-02\n",
      "0.03081782049256546 2.95257560048916 0.16558611398376724\n",
      "it 1, train loss = 1.506008e-01\n",
      "0.033412396746927274 4.479426002828461 0.28249260974073176\n",
      "it 1, train loss = 1.708890e+00\n",
      "0.03344024820981477 4.484721564319507 0.28287465761359815\n",
      "it 1, train loss = 1.215883e-01\n",
      "0.06655542481957938 1.9971488697637942 0.11100091295700072\n",
      "it 1, train loss = 2.917241e-01\n",
      "0.034775371850156035 4.235201817977135 0.2657879016670481\n",
      "it 1, train loss = 3.468062e-01\n",
      "0.03484877081503118 4.2274485029519555 0.26525886757742706\n",
      "it 1, train loss = 1.983604e+00\n",
      "0.030678068355714044 2.9809378729465816 0.16714743989778127\n",
      "it 1, train loss = 5.757542e-02\n",
      "0.027881973580999482 4.733210751410917 0.2917042971727154\n",
      "it 1, train loss = 1.946982e+00\n",
      "0.03504733075541827 4.1929568457097615 0.26290637927294114\n",
      "it 1, train loss = 6.678964e-02\n",
      "0.0349302091361546 4.208367520375447 0.2639419873130198\n",
      "it 1, train loss = 2.676331e+00\n",
      "0.03471812428522123 4.19078050653438 0.2625345310874686\n",
      "it 1, train loss = 1.299363e-01\n",
      "0.03462935697894987 4.265541584667244 0.2678825358683502\n",
      "it 1, train loss = 1.323761e+00\n",
      "0.03250255645220773 4.6494933028010825 0.2941608780594128\n",
      "it 1, train loss = 5.128746e-01\n",
      "0.035091113367283065 4.137375322063923 0.25890741240430515\n",
      "it 1, train loss = 2.280323e+00\n",
      "0.03069776191500655 2.9944635407511564 0.16794339341069672\n",
      "it 1, train loss = 1.104189e+00\n",
      "0.03498666053506753 4.203493890158275 0.2636211823175319\n",
      "it 1, train loss = 1.319854e+00\n",
      "0.03364203700035461 4.437262105922574 0.2796050573967044\n",
      "it 1, train loss = 1.680598e+00\n",
      "0.035064383075829306 4.189922618505996 0.26269123863215205\n",
      "it 1, train loss = 5.467363e-01\n",
      "0.03414700306983293 4.348038784912264 0.27350495507308176\n",
      "it 1, train loss = 2.722128e-02\n",
      "0.03397983573765721 4.3760881975706 0.2754251881736983\n",
      "it 1, train loss = 3.741117e-01\n",
      "0.034579154705145755 4.274005843485346 0.26844248975499463\n",
      "it 1, train loss = 8.188880e-01\n",
      "0.03299342370032366 4.562114949817879 0.288173728042751\n",
      "it 1, train loss = 1.672404e+00\n",
      "0.03368341361985907 4.423271454360742 0.2786365127425125\n",
      "it 1, train loss = 2.279456e+00\n",
      "0.03363125795142682 4.440639610069051 0.27985462807089123\n",
      "it 1, train loss = 2.569341e-01\n",
      "0.029430411742970577 3.613966826762228 0.2123423170347871\n",
      "it 1, train loss = 1.718623e+00\n",
      "0.03403208086813913 4.36122446430154 0.2743979518231171\n",
      "it 1, train loss = 2.228276e+00\n",
      "0.03487527411083482 4.2016419389483985 0.26342797717453054\n",
      "it 1, train loss = 1.162682e-01\n",
      "0.03397820101888047 4.3775165937267335 0.2755165143608035\n",
      "it 1, train loss = 2.520157e+00\n",
      "0.02888210441643798 3.946667828895665 0.23652509944613595\n",
      "it 1, train loss = 8.603804e-01\n",
      "0.0303255720784993 3.1890874629981742 0.18220887766256025\n",
      "it 1, train loss = 2.163191e+00\n",
      "0.03522789596299062 4.161943559561456 0.260779288278884\n",
      "it 1, train loss = 2.047649e+00\n",
      "0.03506042700769234 4.190938573469669 0.2627672361073076\n",
      "38 [1 4 3 1]\n",
      "it 1, train loss = 2.104969e+00\n",
      "0.024552799113857136 5.650799231329113 0.36248678884658153\n",
      "it 1, train loss = 8.810062e-02\n",
      "0.03239728615136203 4.665461644012757 0.29523004724313884\n",
      "it 1, train loss = 2.023084e+00\n",
      "0.02709078449906577 5.511721189825528 0.34699646989302163\n",
      "it 1, train loss = 2.357231e-01\n",
      "0.02949750651701493 3.6138186604787514 0.2127353625896132\n",
      "it 1, train loss = 2.317952e+00\n",
      "0.03072382545482707 2.90238098316426 0.16144562432072762\n",
      "it 1, train loss = 2.809802e+00\n",
      "0.03372352114301859 4.424586794677113 0.2787555036277586\n",
      "it 1, train loss = 2.419584e+00\n",
      "0.029044293367188497 3.7654109666514293 0.2232282322567937\n",
      "it 1, train loss = 1.836264e-01\n",
      "0.034318247002068626 4.313584495747712 0.271149808110866\n",
      "it 1, train loss = 2.518994e+00\n",
      "0.07261048250810048 0.20824929775627693 0.04457571545221899\n",
      "it 1, train loss = 2.004618e-01\n",
      "0.03422043927456248 4.328382150127035 0.27215383735021376\n",
      "it 1, train loss = 2.756176e+00\n",
      "0.03384161572285541 4.387860749993223 0.2761862732454034\n",
      "it 1, train loss = 3.323866e-01\n",
      "0.03021304991967477 3.2604751798726306 0.1876033789605754\n",
      "it 1, train loss = 8.317487e-01\n",
      "0.03027543214674764 3.188864988306647 0.1822092200285438\n",
      "it 1, train loss = 4.645217e-01\n",
      "0.028541946859164728 4.034973695855185 0.24233920917963067\n",
      "it 1, train loss = 2.143834e+00\n",
      "0.03409000570508472 4.3498610614855515 0.2736046202236188\n",
      "it 1, train loss = 3.766328e-01\n",
      "0.03522441743624686 4.158792661734074 0.2605637558683455\n",
      "it 1, train loss = 7.677478e-01\n",
      "0.03328657866650493 4.501068951063509 0.28398489827632317\n",
      "it 1, train loss = 8.059451e-02\n",
      "0.03411482796485195 4.352332576718623 0.2737913851783804\n",
      "it 1, train loss = 2.339122e+00\n",
      "0.033957602040082746 4.380759512515961 0.27575637879491627\n",
      "it 1, train loss = 1.928759e+00\n",
      "0.03397084238168723 4.37444012039076 0.27530407497659304\n",
      "it 1, train loss = 4.341282e-01\n",
      "0.03361750108455559 4.438708698161878 0.27969676265116733\n",
      "it 1, train loss = 1.513659e-01\n",
      "0.034824579623863434 4.227909695035344 0.265274796710014\n",
      "it 1, train loss = 9.069795e-01\n",
      "0.033824522790119944 4.404025463184103 0.27732993243611714\n",
      "it 1, train loss = 2.842273e+00\n",
      "0.03340114109840783 4.481035060159204 0.28260827627087576\n",
      "it 1, train loss = 7.629515e-02\n",
      "0.0342277359700929 4.326433157885145 0.2720227146990367\n",
      "it 1, train loss = 1.590411e-01\n",
      "0.03365394079937617 4.349787227881027 0.2733046077240827\n",
      "it 1, train loss = 1.953513e+00\n",
      "0.02854308381371327 5.2308218199260015 0.3285901430724242\n",
      "it 1, train loss = 4.337706e-02\n",
      "0.02994383434256801 3.320631260431418 0.19156478483960557\n",
      "it 1, train loss = 2.640893e+00\n",
      "0.03267539368211589 4.615661964826162 0.29182510530175304\n",
      "it 1, train loss = 1.494961e-01\n",
      "0.030241942311433023 3.188786298882237 0.1820696717324583\n",
      "it 1, train loss = 8.784983e-02\n",
      "0.07260495371367326 0.20829910070068222 0.04462719659512458\n",
      "it 1, train loss = 1.318216e+00\n",
      "0.03040315347435717 3.1016154845022044 0.17583703839759002\n",
      "it 1, train loss = 6.290300e-01\n",
      "0.03435511185953251 4.306865739261949 0.270679527053373\n",
      "it 1, train loss = 1.534253e+00\n",
      "0.029795450765631573 3.4159457806186277 0.198209761121885\n",
      "it 1, train loss = 2.091238e+00\n",
      "0.07261034771472201 0.20825070953601615 0.04457739461035659\n",
      "it 1, train loss = 2.951133e+00\n",
      "0.03242901466947324 4.659652929648804 0.29483046058959816\n",
      "it 1, train loss = 1.048104e+00\n",
      "0.03312958573132455 4.533658285139908 0.28621219625798305\n",
      "it 1, train loss = 1.228309e+00\n",
      "0.03026759590104454 3.18688613610405 0.18192929797279866\n",
      "it 1, train loss = 2.472335e+00\n",
      "0.033350242872829755 4.49705368626898 0.2837036170443875\n",
      "it 1, train loss = 2.521000e+00\n",
      "0.03272708067035889 4.604992501543444 0.2910992455135931\n",
      "it 1, train loss = 2.795171e+00\n",
      "0.03514053354331431 4.177173878040293 0.26182366574476906\n",
      "it 1, train loss = 1.997504e+00\n",
      "0.032950248006051355 4.558649730194574 0.28791257351462296\n",
      "it 1, train loss = 2.688684e+00\n",
      "0.0344901409844113 4.254901456670822 0.2670142992983068\n",
      "it 1, train loss = 1.699979e+00\n",
      "0.029653250346931385 3.5437171514656924 0.2076730523486776\n",
      "it 1, train loss = 1.898768e+00\n",
      "0.033681416488395996 4.414385341821937 0.2779969267441423\n",
      "it 1, train loss = 1.387142e+00\n",
      "0.034622660019571525 4.355369821630371 0.2743555316894291\n",
      "it 1, train loss = 1.040514e+00\n",
      "0.027091865415565772 5.656614159529087 0.3573561656889899\n",
      "it 1, train loss = 2.705916e+00\n",
      "0.03376303901295061 4.416848404435699 0.27823317107305845\n",
      "it 1, train loss = 1.823898e-01\n",
      "0.030124699109692125 3.222870958130948 0.18434539225757865\n",
      "it 1, train loss = 2.521241e-02\n",
      "0.03438446810318669 4.302405014422848 0.2703746936176624\n",
      "48 [1, 4, 4, 1]\n",
      "it 1, train loss = 6.319749e-01\n",
      "0.03334167648627351 4.493047677805481 0.28341349139143934\n",
      "it 1, train loss = 1.766014e+00\n",
      "0.03327356086513724 4.528759278208046 0.2859256733599205\n",
      "it 1, train loss = 1.929862e+00\n",
      "0.03193441065446608 4.74776545829569 0.30083756164490344\n",
      "it 1, train loss = 1.915357e+00\n",
      "0.02891157399735456 5.00737649285138 0.3125030295651699\n",
      "it 1, train loss = 2.693651e+00\n",
      "0.029267017761823354 3.754763037768058 0.22275542686966862\n",
      "it 1, train loss = 5.911547e-02\n",
      "0.03363011333126676 4.36089119937903 0.27407524745409556\n",
      "it 1, train loss = 1.444847e+00\n",
      "0.03385232100352628 4.411859365856138 0.2778932716522278\n",
      "it 1, train loss = 2.381699e+00\n",
      "0.03382573845316841 4.407633556287383 0.2775691309817992\n",
      "it 1, train loss = 2.461855e+00\n",
      "0.031853885702784496 4.76540384887955 0.3020386294138461\n",
      "it 1, train loss = 2.078334e+00\n",
      "0.033242604493586146 4.5097162150087815 0.2845563565797532\n",
      "it 1, train loss = 6.870977e-02\n",
      "0.029142019484349843 7.53462762637591 0.4944608572687345\n",
      "it 1, train loss = 1.939818e+00\n",
      "0.03382864036250934 4.416005225756327 0.2781394043810627\n",
      "it 1, train loss = 4.949075e-01\n",
      "0.029114845750140627 3.802230813915591 0.22570619414252255\n",
      "it 1, train loss = 3.332258e-01\n",
      "0.033033780758460175 4.549898130047911 0.28729592842289653\n",
      "it 1, train loss = 1.274932e+00\n",
      "0.033771943193389456 4.36030404989082 0.2741302224016233\n",
      "it 1, train loss = 2.422484e+00\n",
      "0.02463826059266521 5.577501584664957 0.3570832487579526\n",
      "it 1, train loss = 2.565765e+00\n",
      "0.033727914947796964 4.42695809824514 0.2788813677800312\n",
      "it 1, train loss = 2.924262e+00\n",
      "0.030124050491419155 3.2874087981827054 0.1895689017557863\n",
      "it 1, train loss = 8.477530e-01\n",
      "0.03279420459336126 4.5881042031550505 0.28988116650357215\n",
      "it 1, train loss = 1.875736e+00\n",
      "0.03033236480061201 3.153013381864923 0.17953333745134317\n",
      "it 1, train loss = 1.832323e+00\n",
      "0.03456949521984094 4.25109953947949 0.2667904096149228\n",
      "it 1, train loss = 8.236663e-01\n",
      "0.0338244962468871 4.383312210306349 0.27584066834290427\n",
      "it 1, train loss = 1.994340e+00\n",
      "0.03386608185239704 4.396700297169148 0.2768102167326233\n",
      "it 1, train loss = 5.645961e-01\n",
      "0.029837624304855533 3.416115496638536 0.19822213892277027\n",
      "it 1, train loss = 1.068001e+00\n",
      "0.033590271422840344 4.45391641488793 0.280734188869317\n",
      "it 1, train loss = 1.846299e+00\n",
      "0.03394614305402908 4.384550648000207 0.27598253618779056\n",
      "it 1, train loss = 6.276812e-01\n",
      "0.03022699419867096 3.2180206097466897 0.18421090842766055\n",
      "it 1, train loss = 1.539709e+00\n",
      "0.028535131358883845 4.082616770724364 0.24562857381040018\n",
      "it 1, train loss = 1.183993e-01\n",
      "0.03385377451631575 4.389592352910221 0.2763034825931196\n",
      "it 1, train loss = 2.073277e+00\n",
      "0.034119351905098114 4.351745351306873 0.2737422928267897\n",
      "it 1, train loss = 8.193408e-01\n",
      "0.029640634400937227 3.506993413753571 0.20476243804608657\n",
      "it 1, train loss = 6.207101e-01\n",
      "0.027969856541406966 4.514163434091593 0.2766358487973034\n",
      "it 1, train loss = 1.401873e+00\n",
      "0.03069118596097564 2.9826623158603707 0.16735380857102763\n",
      "it 1, train loss = 1.684034e+00\n",
      "0.03403492474340288 4.285650646371624 0.2689207816114726\n",
      "it 1, train loss = 1.818068e+00\n",
      "0.0331341542185129 4.524248870393114 0.28553148165135195\n",
      "it 1, train loss = 2.322534e+00\n",
      "0.03357961029237484 4.450013347521392 0.28045609779192343\n",
      "it 1, train loss = 2.720086e-01\n",
      "0.03384944946765014 4.404419233018454 0.27734593454258244\n",
      "it 1, train loss = 6.949028e-01\n",
      "0.030384446648836754 3.129615048874704 0.17781323672142063\n",
      "it 1, train loss = 1.678507e+00\n",
      "0.030386445577131677 3.1635526870348034 0.1804814648186331\n",
      "it 1, train loss = 1.868510e-01\n",
      "0.0330276510462993 4.557771059563962 0.2878230985823415\n",
      "it 1, train loss = 2.274863e+00\n",
      "0.03307374528778045 4.544192766894741 0.28690751165978573\n",
      "it 1, train loss = 2.388813e+00\n",
      "0.030849848228974297 2.9019485596824874 0.16153593750620152\n",
      "it 1, train loss = 5.884965e-01\n",
      "0.03385812008138487 4.388555681178577 0.2762293800558516\n",
      "it 1, train loss = 2.378530e+00\n",
      "0.029016325583630834 3.9076426235657102 0.23374939001451647\n",
      "it 1, train loss = 1.601348e-01\n",
      "0.03403174127030074 4.368419176419733 0.2748821933178976\n",
      "it 1, train loss = 2.062199e+00\n",
      "0.03325722499281158 4.50881904659714 0.2844992399503333\n",
      "it 1, train loss = 3.266754e-01\n",
      "0.03413194559158111 4.353666632115681 0.2738662216586999\n",
      "it 1, train loss = 2.284455e+00\n",
      "0.03311837001193805 4.534378793021166 0.28622920037885413\n",
      "it 1, train loss = 2.498402e-01\n",
      "0.03048629997009154 3.1161685544787563 0.17706454064073204\n",
      "it 1, train loss = 1.194441e+00\n",
      "0.0344367265320811 4.2958527549881085 0.2699021536357642\n"
     ]
    }
   ],
   "source": [
    "lamUT_vec = np.array(UTdata['F11'])\n",
    "lamET_vec = np.array(ETdata['F11'])\n",
    "lamPS_vec = np.array(PSdata['F11'])\n",
    "np_list = []\n",
    "r2_list_global = []\n",
    "mae_list_global = []\n",
    "for n in range(10,50):\n",
    "    n_params, layers = opt_arch_node(n)\n",
    "    if n_params in np_list:\n",
    "        continue\n",
    "    print(n_params, layers)\n",
    "    np_list.append(n_params)\n",
    "\n",
    "    r2_list_arch = []\n",
    "    mae_list_arch = []\n",
    "    for i in range(50): #50 runs for every architecture\n",
    "        key, subkey = random.split(key)\n",
    "        params = init_node(key, layers)\n",
    "\n",
    "        # Make sure you are starting at a good place\n",
    "        loss = loss_P11_UT(params, F11_data, 3)\n",
    "        while loss>3.0:\n",
    "            key, subkey = random.split(key)\n",
    "            params = init_node(key, layers)\n",
    "            loss = loss_P11_UT(params, F11_data, 3)\n",
    "\n",
    "        opt_init, opt_update, get_params = optimizers.adam(2.e-4) #Original: 2.e-4\n",
    "        opt_state = opt_init(params)\n",
    "        params, train_loss, val_loss = train_jp(loss_P11_UT, 3, F11_data, opt_state, key, nIter = 100000, print_freq=1000000)\n",
    "\n",
    "        model = NODE_model(params[0], params[1])\n",
    "\n",
    "        lambdas     = [lamUT_vec,       lamET_vec,      lamPS_vec       ]\n",
    "        P11_gt_list = [UTdata['P11'],   ETdata['P11'],  PSdata['P11']   ]\n",
    "        P11funs     = [P11_UT,          P11_ET,         P11_PS          ]\n",
    "\n",
    "        r2 = []\n",
    "        mae = []\n",
    "        for lam, P11_gt, P11fun in zip(lambdas, P11_gt_list, P11funs):\n",
    "            P11 = P11fun(lam, model, normalization)\n",
    "            r2i = r2_score(P11_gt, P11)\n",
    "            r2i = np.clip(r2i, a_min=0.0, a_max=1.0)\n",
    "            r2.append(r2i)\n",
    "\n",
    "            maei = onp.mean(onp.abs(P11_gt-P11))\n",
    "            mae.append(maei)\n",
    "        # r2 = np.mean(np.array(r2))\n",
    "        # print(*r2)\n",
    "        print(*mae)\n",
    "        r2_list_arch.append(r2)\n",
    "        mae_list_arch.append(mae)\n",
    "    r2_list_global.append(r2_list_arch)\n",
    "    mae_list_global.append(mae_list_arch)\n",
    "node_r2 = np.array(r2_list_global)\n",
    "node_mae = np.array(mae_list_global)\n",
    "with open('savednet/NODE_r2_efficiency_UT.npy', 'wb') as f:\n",
    "    pickle.dump([np_list, node_r2], f)\n",
    "with open('savednet/NODE_mae_efficiency_UT.npy', 'wb') as f:\n",
    "    pickle.dump([np_list, node_mae], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 [1 1 1]\n",
      "it 1, train loss = 7.771423e+00\n",
      "0.047969638977718485 1236.5120234485305 1.30186720004801\n",
      "it 1, train loss = 8.768878e+00\n",
      "0.0537932626837995 761.6109498437485 2.5943973517555476\n",
      "it 1, train loss = 8.805664e+00\n",
      "0.05023720677779796 57.83032574658819 0.27830941780373586\n",
      "it 1, train loss = 8.819682e+00\n",
      "0.050057088112786856 86.3157546641736 0.34996392522211606\n",
      "it 1, train loss = 9.211406e+00\n",
      "0.050444487072279616 28.521434739005546 0.1899067063773011\n",
      "it 1, train loss = 7.545916e+00\n",
      "0.1691386929283886 7489.907237558917 9.83818052714552\n",
      "it 1, train loss = 7.200437e+00\n",
      "0.045258106683746205 3718.6002053544457 3.3197587240601973\n",
      "it 1, train loss = 7.615540e+00\n",
      "0.046316944088758756 2894.239481197378 2.3986312939486503\n",
      "it 1, train loss = 9.107901e+00\n",
      "0.1669286235557261 29.340134034160364 1.649427875116302\n",
      "it 1, train loss = 1.035384e+01\n",
      "0.4527365285328241 218.01894202588178 3.358700754538025\n",
      "it 1, train loss = 1.088638e+01\n",
      "0.8679989271213054 0.2592080410119464 0.03993661450523446\n",
      "it 1, train loss = 9.030709e+00\n",
      "0.7275955463563087 0.1398081710402185 0.09841072651869767\n",
      "it 1, train loss = 8.107347e+00\n",
      "0.17098375058621337 3356.8074794776135 9.165274523421393\n",
      "it 1, train loss = 9.087015e+00\n",
      "0.1667200598053208 28.838498974028624 1.639955854620606\n",
      "it 1, train loss = 9.119740e+00\n",
      "0.1821935026062919 1599.1749786958787 8.380077374426342\n",
      "it 1, train loss = 8.843342e+00\n",
      "0.17475899521991017 2357.51960885245 9.522930067156873\n",
      "it 1, train loss = 9.181070e+00\n",
      "0.05044472478575036 29.325507729997774 0.19260716785874044\n",
      "it 1, train loss = 8.937612e+00\n",
      "0.1776536397726288 2007.864976738727 9.066034313083993\n",
      "it 1, train loss = 8.287194e+00\n",
      "0.04936930007658816 267.3859622871677 0.6292467369951511\n",
      "it 1, train loss = 8.738130e+00\n",
      "0.04934796414832292 250.7178981970929 0.6862520843028559\n",
      "it 1, train loss = 8.004669e+00\n",
      "0.048459012682283295 749.1629877525381 1.048540897879195\n",
      "it 1, train loss = 8.681674e+00\n",
      "0.04885125937133114 402.19206564552655 0.9474476707779379\n",
      "it 1, train loss = 9.972172e+00\n",
      "0.16691993178031475 31.19307689431111 1.658963757891517\n",
      "it 1, train loss = 9.701533e+00\n",
      "0.8247293097442092 57.58383579242181 2.224120984428647\n",
      "it 1, train loss = 7.982231e+00\n",
      "0.17351753368810705 4048.8793698694267 10.82757777725754\n",
      "it 1, train loss = 8.098114e+00\n",
      "0.04721492561071179 1403.8052053944082 1.8596505981200728\n",
      "it 1, train loss = 9.422660e+00\n",
      "0.16691461380164305 29.7342472142045 1.651044230570766\n",
      "it 1, train loss = 8.737476e+00\n",
      "0.1812030342158595 1862.5782333072905 8.789135543865596\n",
      "it 1, train loss = 8.601462e+00\n",
      "0.049642951742841196 176.82319799296195 0.5174447955010241\n",
      "it 1, train loss = 8.247009e+00\n",
      "0.04865454669905323 562.5291571939327 0.9709899357023347\n",
      "it 1, train loss = 7.798120e+00\n",
      "0.04683504767497654 2113.0940701125182 2.049052687651778\n",
      "it 1, train loss = 9.414684e+00\n",
      "0.16699780331958133 30.04678911333652 1.6552253578435288\n",
      "it 1, train loss = 8.990224e+00\n",
      "0.200429381172135 1088.6537562706046 7.207017037654401\n",
      "it 1, train loss = 8.023878e+00\n",
      "0.048986208193338286 439.84010728213934 0.794159986452869\n",
      "it 1, train loss = 7.793382e+00\n",
      "0.04648845737064214 2290.082136043434 2.3256920677825894\n",
      "it 1, train loss = 8.877533e+00\n",
      "0.04954494221757793 200.6046743696704 0.6160269380833502\n",
      "it 1, train loss = 9.095262e+00\n",
      "0.05045783592685323 27.62686674517125 0.18545709374218836\n",
      "it 1, train loss = 8.123372e+00\n",
      "0.17125113357709115 3312.1107868216045 9.27301986434104\n",
      "it 1, train loss = 8.914129e+00\n",
      "0.04956639713441257 195.32715252679748 0.6121846765991784\n",
      "it 1, train loss = 8.039793e+00\n",
      "0.048977886473825054 429.5510566452124 0.7858851300985182\n",
      "it 1, train loss = 8.682917e+00\n",
      "0.048880208983239914 392.4719841528725 0.9295116347435678\n",
      "it 1, train loss = 9.491516e+00\n",
      "0.1668575301780404 29.720847136617778 1.649228824290425\n",
      "it 1, train loss = 8.183452e+00\n",
      "0.17327552890826348 3404.9403549075523 10.249557313646369\n",
      "it 1, train loss = 9.389125e+00\n",
      "0.16695250361854888 29.812040036757026 1.6526047862721611\n",
      "it 1, train loss = 7.859690e+00\n",
      "0.17117971662482284 4342.2486557534385 10.005277707136214\n",
      "it 1, train loss = 9.763402e+00\n",
      "0.16704267804472855 31.035144192526083 1.6616601764390972\n",
      "it 1, train loss = 9.658780e+00\n",
      "0.5828857046056648 118.76459378145319 2.5948578082609637\n",
      "it 1, train loss = 7.754801e+00\n",
      "0.17184223322537392 5328.786048876907 10.89939393729914\n",
      "it 1, train loss = 7.738496e+00\n",
      "0.04624402552150195 2358.406035498191 2.5444163533448667\n",
      "it 1, train loss = 9.048292e+00\n",
      "0.0500163410122902 101.47002876341342 0.40665071195164765\n",
      "16 [1, 2, 1]\n",
      "it 1, train loss = 8.400468e+00\n",
      "0.028281765894358976 10.112023840204225 0.3052342333234462\n",
      "it 1, train loss = 8.669247e+00\n",
      "0.027853467819525773 10.899169321021388 0.3187424140777618\n",
      "it 1, train loss = 8.969360e+00\n",
      "0.028808360525337792 12.882665879697258 0.41585865182868276\n",
      "it 1, train loss = 9.292912e+00\n",
      "0.02819170330217949 10.411909571386854 0.3084732943703384\n",
      "it 1, train loss = 8.514559e+00\n",
      "0.028141148615009767 10.589273280724386 0.31271525062253447\n",
      "it 1, train loss = 8.184228e+00\n",
      "0.02848763241904765 11.0123542304636 0.29457944998924857\n",
      "it 1, train loss = 8.914554e+00\n",
      "0.028302732799461622 10.73416242482272 0.299411987959835\n",
      "it 1, train loss = 8.834505e+00\n",
      "0.028094794776692994 11.013617000277767 0.32121007877611235\n",
      "it 1, train loss = 8.624855e+00\n",
      "0.028054675202589655 10.884281116745552 0.31822363278689086\n",
      "it 1, train loss = 9.134622e+00\n",
      "0.02791407012741818 11.089727112738101 0.3360254671241165\n",
      "it 1, train loss = 8.827513e+00\n",
      "0.028302054821680488 10.754424203676317 0.2976229558586416\n",
      "it 1, train loss = 8.219168e+00\n",
      "0.028160211816923172 10.30419553992116 0.30933469425854515\n",
      "it 1, train loss = 9.561417e+00\n",
      "0.02822541383709906 11.39368248383826 0.3129881212341575\n",
      "it 1, train loss = 8.296337e+00\n",
      "0.028269361194695232 10.471064884408394 0.30161644989159947\n",
      "it 1, train loss = 8.928704e+00\n",
      "0.028160719985455746 12.868198216262506 0.32769059702172487\n",
      "it 1, train loss = 9.582582e+00\n",
      "0.02799906563036983 11.782647684030314 0.33308634311794794\n",
      "it 1, train loss = 7.477009e+00\n",
      "0.028249582683942773 10.706948554431026 0.3003527800657943\n",
      "it 1, train loss = 7.696245e+00\n",
      "0.02840884814986918 12.67473340682207 0.30598382829286247\n",
      "it 1, train loss = 8.493255e+00\n",
      "0.028233990090902578 10.946167878643841 0.31206561555906626\n",
      "it 1, train loss = 8.075288e+00\n",
      "0.12581858547011923 1147.848880885452 10.608230404705141\n",
      "it 1, train loss = 9.353849e+00\n",
      "0.027803187250825277 11.29279511254954 0.3503622116183182\n",
      "it 1, train loss = 8.196038e+00\n",
      "0.028368738537735548 10.468805806192536 0.29851164505985045\n",
      "it 1, train loss = 8.849696e+00\n",
      "0.028010967857546268 10.700846226152859 0.32312068623640267\n",
      "it 1, train loss = 8.198624e+00\n",
      "0.02846741768562511 11.103894477278226 0.29581615588451704\n",
      "it 1, train loss = 8.166706e+00\n",
      "0.027975581594226293 10.817176334742962 0.3328265972217796\n",
      "it 1, train loss = 7.802312e+00\n",
      "0.12259528118053506 1154.8706159349904 10.835382863376342\n",
      "it 1, train loss = 9.571141e+00\n",
      "0.02803120682181402 10.76996006594433 0.3114035350759873\n",
      "it 1, train loss = 8.907089e+00\n",
      "0.028148037863982164 10.81534947180079 0.3086289634555565\n",
      "it 1, train loss = 8.389106e+00\n",
      "0.0281854499207569 10.657467616588132 0.3107375024039656\n",
      "it 1, train loss = 9.113208e+00\n",
      "0.028206089259464953 10.731966459693313 0.31157829164278467\n",
      "it 1, train loss = 9.231330e+00\n",
      "0.028228027047753783 10.805456613326204 0.3104241428240814\n",
      "it 1, train loss = 8.548165e+00\n",
      "0.027775791108166806 10.754112043134299 0.3199492453346502\n",
      "it 1, train loss = 8.721745e+00\n",
      "0.02828860963620083 10.820053830798326 0.3085777514562706\n",
      "it 1, train loss = 9.062774e+00\n",
      "0.12136383106079254 1176.5522636258172 11.145344943136276\n",
      "it 1, train loss = 7.329783e+00\n",
      "0.028696699887963705 10.760192290200644 0.28678583194394563\n",
      "it 1, train loss = 9.346822e+00\n",
      "0.02793299913764967 10.28610945593775 0.31804299368923966\n",
      "it 1, train loss = 8.157956e+00\n",
      "0.02832101578975572 10.471668445893696 0.29901084813553197\n",
      "it 1, train loss = 9.733742e+00\n",
      "0.027773508302010388 11.911927792502468 0.35716422657862595\n",
      "it 1, train loss = 9.155923e+00\n",
      "0.028035262291537313 11.59687497388666 0.3305326307736016\n",
      "it 1, train loss = 7.957867e+00\n",
      "0.028297464464430506 10.172602390429267 0.3013018592205584\n",
      "it 1, train loss = 8.914532e+00\n",
      "0.02796402057339411 11.054878954031873 0.3315131965784933\n",
      "it 1, train loss = 8.793704e+00\n",
      "0.028246765934100613 11.251263711219202 0.31303170345431947\n",
      "it 1, train loss = 8.906103e+00\n",
      "0.028131818685068935 10.784240579383667 0.314028937387091\n",
      "it 1, train loss = 7.901127e+00\n",
      "0.02823795458230938 10.27552001584581 0.30166235536864733\n",
      "it 1, train loss = 8.959493e+00\n",
      "0.027706395342928757 10.598789724742627 0.3260559178774073\n",
      "it 1, train loss = 8.563386e+00\n",
      "0.02836109809652747 10.908224708978581 0.3019917348654501\n",
      "it 1, train loss = 8.235677e+00\n",
      "0.028406967973264897 10.614029930403873 0.2962069914425526\n",
      "it 1, train loss = 9.173554e+00\n",
      "0.028115866192951176 11.279571208776428 0.32128736321157003\n",
      "it 1, train loss = 9.891809e+00\n",
      "0.027873183980077404 11.724092359038561 0.3453726063893532\n",
      "it 1, train loss = 9.258561e+00\n",
      "0.027915996483289397 11.62247960137671 0.33892542046296226\n",
      "22 [1, 3, 1]\n",
      "it 1, train loss = 8.522150e+00\n",
      "0.026268376273172155 12.010736373913732 0.3998739738041707\n",
      "it 1, train loss = 7.915594e+00\n",
      "0.02756759674990919 10.860943036420643 0.3254895751911634\n",
      "it 1, train loss = 8.663514e+00\n",
      "0.027716471961215596 10.465812928846567 0.31555155182485467\n",
      "it 1, train loss = 8.610130e+00\n",
      "0.027344914082001428 10.987557956416223 0.34433066619394775\n",
      "it 1, train loss = 9.185480e+00\n",
      "0.02709653787870396 11.208355198281659 0.34396290305496297\n",
      "it 1, train loss = 9.144715e+00\n",
      "0.02711134423951744 11.035745532886923 0.3522013956764842\n",
      "it 1, train loss = 8.466120e+00\n",
      "0.02760102947874437 9.655700837357145 0.3216924487173796\n",
      "it 1, train loss = 8.566128e+00\n",
      "0.02731467170393932 10.820850032190135 0.33584759288033783\n",
      "it 1, train loss = 8.700931e+00\n",
      "0.027551571746498662 10.33055403660038 0.32879243657597507\n",
      "it 1, train loss = 8.732466e+00\n",
      "0.02744722619310066 10.407141380944992 0.3361149031211363\n",
      "it 1, train loss = 8.354146e+00\n",
      "0.027729889597670886 10.659598561672514 0.3189144136113778\n",
      "it 1, train loss = 8.625171e+00\n",
      "0.02757005767717792 10.765394535025624 0.3301985579430983\n",
      "it 1, train loss = 9.049859e+00\n",
      "0.027947595127871677 9.413853203766545 0.30645023975470453\n",
      "it 1, train loss = 8.941729e+00\n",
      "0.027491820882915005 11.013683517451447 0.33910510277423894\n",
      "it 1, train loss = 8.413228e+00\n",
      "0.02768552384495666 9.935911224000947 0.3190959414004209\n",
      "it 1, train loss = 8.683007e+00\n",
      "0.026319389022659803 11.859188480046912 0.3994857383763693\n",
      "it 1, train loss = 9.273522e+00\n",
      "0.025878028161641684 12.966361810396872 0.4236899702636827\n",
      "it 1, train loss = 8.264337e+00\n",
      "0.027561821080898895 11.35112063705075 0.32170180074445404\n",
      "it 1, train loss = 8.777314e+00\n",
      "0.027534934592256487 10.39392406681967 0.32820359618032185\n",
      "it 1, train loss = 9.293278e+00\n",
      "0.026973524649050836 10.867316208731129 0.3567385109410048\n",
      "it 1, train loss = 8.849395e+00\n",
      "0.02752292868761156 10.863615797782572 0.3335826366174013\n",
      "it 1, train loss = 7.820611e+00\n",
      "0.027809225988868258 10.328836109489671 0.3137994792718309\n",
      "it 1, train loss = 9.408237e+00\n",
      "0.027471268625250976 10.989395020768333 0.34096923819284236\n",
      "it 1, train loss = 7.814155e+00\n",
      "0.03032149299395897 14.43459107055941 0.5157039371270057\n",
      "it 1, train loss = 8.772838e+00\n",
      "0.027579242936006408 10.605413006169055 0.330631033773373\n",
      "it 1, train loss = 8.621713e+00\n",
      "0.026020176138914372 12.451649449260216 0.4174390006096732\n",
      "it 1, train loss = 9.502400e+00\n",
      "0.027467409000255693 9.97219843880767 0.32819733609857943\n",
      "it 1, train loss = 8.725026e+00\n",
      "0.02768750199830209 9.990135900609225 0.32023392388110095\n",
      "it 1, train loss = 8.759141e+00\n",
      "0.027388804928106816 10.235426270605142 0.33141001654317387\n",
      "it 1, train loss = 8.532018e+00\n",
      "0.02804274434152365 10.925582358940545 0.306595325826421\n",
      "it 1, train loss = 8.857611e+00\n",
      "0.027537983165760886 10.323542673801596 0.3273870120808723\n",
      "it 1, train loss = 8.910638e+00\n",
      "0.02748195887619063 10.331856434981601 0.3367700319843344\n",
      "it 1, train loss = 9.425137e+00\n",
      "0.027195235912580006 11.283703006850446 0.36236783684791607\n",
      "it 1, train loss = 9.342527e+00\n",
      "0.027468734236552033 10.321765950874564 0.3344631849086901\n",
      "it 1, train loss = 8.919739e+00\n",
      "0.0277325404470391 10.637635903809757 0.3250205615524139\n",
      "it 1, train loss = 9.211950e+00\n",
      "0.02761985674090101 9.902980856903369 0.33172386065969295\n",
      "it 1, train loss = 7.505500e+00\n",
      "0.027907375221223017 11.486455714522972 0.3099327429216956\n",
      "it 1, train loss = 9.107645e+00\n",
      "0.025880681614587498 12.80974280759709 0.43628162500631545\n",
      "it 1, train loss = 8.868482e+00\n",
      "0.02723324689743123 10.936637949211516 0.34454234495968167\n",
      "it 1, train loss = 8.519023e+00\n",
      "0.026944078081429776 11.126617888091308 0.3595300161069453\n",
      "it 1, train loss = 9.259832e+00\n",
      "0.02711057993244275 11.304571011554103 0.3469016991632084\n",
      "it 1, train loss = 8.907943e+00\n",
      "0.027563156592830883 10.115853648468509 0.3270576035137565\n",
      "it 1, train loss = 9.455329e+00\n",
      "0.027631584370554643 9.739887238486949 0.31990698803832857\n",
      "it 1, train loss = 8.875802e+00\n",
      "0.028042508157055952 10.584747905611282 0.3548983397969823\n",
      "it 1, train loss = 8.462338e+00\n",
      "0.027439475912052355 10.811732603177344 0.3256540875944077\n",
      "it 1, train loss = 8.093015e+00\n",
      "0.027691985606902857 10.174856417035956 0.3249189523572263\n",
      "it 1, train loss = 9.094427e+00\n",
      "0.027536414078720043 10.610368044631674 0.3319048245854992\n",
      "it 1, train loss = 8.261015e+00\n",
      "0.027520166190429567 10.300110643063121 0.324485533306165\n",
      "it 1, train loss = 8.284691e+00\n",
      "0.02582133244768424 13.190027298146033 0.4375898391349177\n",
      "it 1, train loss = 9.086019e+00\n",
      "0.02654180894425352 11.966657221317886 0.38586511048783345\n",
      "28 [1, 4, 1]\n",
      "it 1, train loss = 8.286720e+00\n",
      "0.026757865642471783 11.470455721493158 0.3693164341109162\n",
      "it 1, train loss = 9.035061e+00\n",
      "0.027547389007500395 10.052184276064423 0.33175665082439126\n",
      "it 1, train loss = 9.171499e+00\n",
      "0.027402695202599272 10.326164663025414 0.3314031837701989\n",
      "it 1, train loss = 7.741783e+00\n",
      "0.027559667374963424 11.024724147661342 0.3237049807006743\n",
      "it 1, train loss = 8.536847e+00\n",
      "0.027537885255759433 10.64297183354918 0.3278800244990849\n",
      "it 1, train loss = 9.310576e+00\n",
      "0.02516339538701839 15.194913697464093 0.5248544441181326\n",
      "it 1, train loss = 8.977933e+00\n",
      "0.027590021505359833 10.023117333007391 0.32249713749236825\n",
      "it 1, train loss = 8.420703e+00\n",
      "0.027546116656806518 10.320960394026724 0.3248127202580742\n",
      "it 1, train loss = 8.481454e+00\n",
      "0.026635141056603785 11.44689129412685 0.385179851580684\n",
      "it 1, train loss = 8.968400e+00\n",
      "0.02640207933735323 11.938420280345653 0.389644520794411\n",
      "it 1, train loss = 9.411909e+00\n",
      "0.027541997712299503 10.118313326878623 0.3269982290636392\n",
      "it 1, train loss = 8.737924e+00\n",
      "0.02531272026559642 14.436107880812134 0.496608400425468\n",
      "it 1, train loss = 8.310332e+00\n",
      "0.027345090874591565 11.141960261982318 0.32966749966768194\n",
      "it 1, train loss = 7.816894e+00\n",
      "0.026685922496786967 11.829256660557588 0.3862967071455941\n",
      "it 1, train loss = 8.873013e+00\n",
      "0.02535813427793438 15.666890108798569 0.5278455335244928\n",
      "it 1, train loss = 8.588158e+00\n",
      "0.027620634106494978 10.806986738091195 0.31827739800179905\n",
      "it 1, train loss = 8.400108e+00\n",
      "0.027200999255182667 10.724567391376178 0.34425057373416523\n",
      "it 1, train loss = 8.082226e+00\n",
      "0.02552876790037517 14.787690328031788 0.48523387328567125\n",
      "it 1, train loss = 9.149700e+00\n",
      "0.0274104788529334 10.883398239829967 0.33166999415539544\n",
      "it 1, train loss = 8.080282e+00\n",
      "0.0277937553793401 10.17314654737177 0.31275508271756103\n",
      "it 1, train loss = 9.107907e+00\n",
      "0.027679699741509575 9.697695721312936 0.31613111350615947\n",
      "it 1, train loss = 8.465889e+00\n",
      "0.02736891037907046 10.204433760590613 0.34126614742895034\n",
      "it 1, train loss = 8.310759e+00\n",
      "0.027644700600883514 9.76233135482116 0.3232092479875847\n",
      "it 1, train loss = 9.596194e+00\n",
      "0.02737107770104237 10.355179792570487 0.33362758252568164\n",
      "it 1, train loss = 9.321641e+00\n",
      "0.026260366051625655 12.470798607660734 0.4038384396519081\n",
      "it 1, train loss = 8.775163e+00\n",
      "0.027275793434054688 10.995734552099199 0.337865280998021\n",
      "it 1, train loss = 8.042682e+00\n",
      "0.027141637198872706 10.97286558160764 0.34087762298966623\n",
      "it 1, train loss = 8.201830e+00\n",
      "0.02741377812064491 10.748785721296175 0.3300511230045803\n",
      "it 1, train loss = 9.250623e+00\n",
      "0.02732315234829078 10.187548397365319 0.3363312537937047\n",
      "it 1, train loss = 7.519488e+00\n",
      "0.02720926795658655 11.39123959786283 0.3376024624284927\n",
      "it 1, train loss = 8.843374e+00\n",
      "0.02734643766066437 10.664835764873345 0.33796447090425213\n",
      "it 1, train loss = 8.579812e+00\n",
      "0.027334390803506525 10.587656782713935 0.347447453730873\n",
      "it 1, train loss = 8.616242e+00\n",
      "0.02742963358456704 10.604260219370353 0.3297744266188697\n",
      "it 1, train loss = 8.970604e+00\n",
      "0.025185198677634184 15.766967386072608 0.5351697784380999\n",
      "it 1, train loss = 8.744619e+00\n",
      "0.02739699192459721 10.517732557725747 0.3324107511897605\n",
      "it 1, train loss = 8.945641e+00\n",
      "0.027467258813513674 10.261258272626376 0.32969644963156275\n",
      "it 1, train loss = 8.800113e+00\n",
      "0.025804864804705875 12.915186121167189 0.432402465901074\n",
      "it 1, train loss = 8.651612e+00\n",
      "0.027201218699195423 10.541402270699269 0.349919096411659\n",
      "it 1, train loss = 9.055722e+00\n",
      "0.026627122157538027 11.518217196430623 0.3880852679022922\n",
      "it 1, train loss = 9.136059e+00\n",
      "0.02717809649899337 10.594787666763906 0.33769303962906594\n",
      "it 1, train loss = 8.704368e+00\n",
      "0.025857841018096076 12.798314299815198 0.42662085014071105\n",
      "it 1, train loss = 8.850643e+00\n",
      "0.027727860534850803 9.6946051808536 0.3149838880617171\n",
      "it 1, train loss = 8.505583e+00\n",
      "0.027096802454620796 10.624413279978484 0.3502627087646496\n",
      "it 1, train loss = 8.899857e+00\n",
      "0.027245430975512054 10.394197060708375 0.3430114865161225\n",
      "it 1, train loss = 8.619071e+00\n",
      "0.025111054806616975 15.956857482936966 0.5149268018374406\n",
      "it 1, train loss = 9.166454e+00\n",
      "0.026640706975678018 11.873599341963933 0.373214722663122\n",
      "it 1, train loss = 8.321987e+00\n",
      "0.02513650327933716 15.037628759080643 0.5136359444995284\n",
      "it 1, train loss = 9.327328e+00\n",
      "0.02758713375805598 10.156417891868806 0.3237437124294843\n",
      "it 1, train loss = 8.902274e+00\n",
      "0.02754426073656085 10.480185439050539 0.33124868171919136\n",
      "it 1, train loss = 8.337269e+00\n",
      "0.025317964416946212 14.87308765100778 0.4988536481684688\n",
      "32 [1, 2, 2, 1]\n",
      "it 1, train loss = 8.065811e+00\n",
      "0.02954683622050612 69.0668908925093 0.6437089187666383\n",
      "it 1, train loss = 8.280376e+00\n",
      "0.02035602353139132 86.76059088659943 0.5860814666271589\n",
      "it 1, train loss = 8.366936e+00\n",
      "0.020191170342401598 280.3273431927249 0.63473610768765\n",
      "it 1, train loss = 8.339222e+00\n",
      "0.020230489073192234 74.95767344203605 0.5346751932348094\n",
      "it 1, train loss = 8.139552e+00\n",
      "0.019995039033442626 106.21381207743607 0.5456478687908547\n",
      "it 1, train loss = 8.903318e+00\n",
      "0.020150360286280318 152.8459619483992 0.599658447431932\n",
      "it 1, train loss = 9.474805e+00\n",
      "0.022295273587317678 7840.439168383926 2.0117349951282555\n",
      "it 1, train loss = 8.857214e+00\n",
      "0.08738243467821726 5300.892888491397 21.339730822484377\n",
      "it 1, train loss = 9.033519e+00\n",
      "0.026809694636851192 6728.92759299663 1.169106931642066\n",
      "it 1, train loss = 8.152366e+00\n",
      "0.019974022341840866 242.97592813250753 0.6321187017030285\n",
      "it 1, train loss = 8.380318e+00\n",
      "0.026490420679612922 16089.29686119179 2.2742541413695934\n",
      "it 1, train loss = 8.569512e+00\n",
      "0.020197624923762706 143.25938529128211 0.5771950016723537\n",
      "it 1, train loss = 8.575105e+00\n",
      "0.020458500537400857 198.06591622658232 0.6401157037532977\n",
      "it 1, train loss = 9.113020e+00\n",
      "0.02022065893984572 120.39771544099388 0.5675413805322971\n",
      "it 1, train loss = 7.833652e+00\n",
      "0.020081477607129684 138.77723768764372 0.5637273660010169\n",
      "it 1, train loss = 7.581789e+00\n",
      "0.019684534269780757 104.75658025937157 0.575222420204876\n",
      "it 1, train loss = 9.328506e+00\n",
      "0.025283145563163537 736.7205513457563 0.9464197957984991\n",
      "it 1, train loss = 9.220690e+00\n",
      "0.0864884603993585 4434.255347690322 20.072896925794964\n",
      "it 1, train loss = 9.158515e+00\n",
      "0.02749867563880672 159.2468158103003 0.6062972433372167\n",
      "it 1, train loss = 8.552860e+00\n",
      "0.019857053029957043 99.18623830726003 0.47923671863140177\n",
      "it 1, train loss = 8.691142e+00\n",
      "0.020098575305610895 98.30510998022257 0.5734230062578435\n",
      "it 1, train loss = 7.831594e+00\n",
      "0.019995800400339173 129.5493217722923 0.5795922503194791\n",
      "it 1, train loss = 8.756973e+00\n",
      "0.020083016848780014 114.84798093702736 0.558783532360363\n",
      "it 1, train loss = 7.895705e+00\n",
      "0.020187754752133222 92.66941413108898 0.5549628801038916\n",
      "it 1, train loss = 9.208716e+00\n",
      "0.02035069709950174 161.3300753663753 0.6159235080865356\n",
      "it 1, train loss = 8.258155e+00\n",
      "0.020228877383783178 108.585524807419 0.5596743094555733\n",
      "it 1, train loss = 8.743812e+00\n",
      "0.020133734868749836 108.33518021282404 0.594296408316978\n",
      "it 1, train loss = 8.972333e+00\n",
      "0.02060914522433169 2459.782603304448 1.0538618331002652\n",
      "it 1, train loss = 8.747151e+00\n",
      "0.021092806184549406 326.25431597998164 0.6770944720367587\n",
      "it 1, train loss = 8.586262e+00\n",
      "0.01974397718391368 378.91520257007613 0.6892425019897322\n",
      "it 1, train loss = 7.949440e+00\n",
      "0.019999415887626017 135.61710364237115 0.573534577232379\n",
      "it 1, train loss = 9.360949e+00\n",
      "0.020058807143981606 327.1959756185759 0.7027424071261328\n",
      "it 1, train loss = 8.935936e+00\n",
      "0.026064940364119438 1338.9874295318737 1.1691830827802936\n",
      "it 1, train loss = 9.152464e+00\n",
      "0.020371447916323077 97.55094670705913 0.5664052172706946\n",
      "it 1, train loss = 8.550371e+00\n",
      "0.020185357636252163 106.54839061320651 0.5536393411143686\n",
      "it 1, train loss = 7.995781e+00\n",
      "0.02127766480931083 8140.888380700893 1.7999866972273362\n",
      "it 1, train loss = 8.305809e+00\n",
      "0.030636922748268763 40210.39036993374 6.615181843552186\n",
      "it 1, train loss = 8.592246e+00\n",
      "0.019782536277329624 3725.013460335474 1.6273526499694042\n",
      "it 1, train loss = 9.111404e+00\n",
      "0.027431579069456643 5737.020820082903 1.6666083703326884\n",
      "it 1, train loss = 9.148632e+00\n",
      "0.02768995927762563 1919.867779004646 0.8627826296857061\n",
      "it 1, train loss = 8.176462e+00\n",
      "0.020394112225735648 153.76679165485493 0.5738138104678063\n",
      "it 1, train loss = 9.001118e+00\n",
      "0.020048939857097377 133.75061877503555 0.5502507717165513\n",
      "it 1, train loss = 9.245150e+00\n",
      "0.02007154328182093 167.31973243643205 0.6330388524355588\n",
      "it 1, train loss = 8.135723e+00\n",
      "0.020167747163521602 136.65781651410245 0.5996374184320847\n",
      "it 1, train loss = 8.716459e+00\n",
      "0.0201719786898792 132.6598920391837 0.5807719044150945\n",
      "it 1, train loss = 8.319341e+00\n",
      "0.020208416681628906 85.26529033027242 0.5367328876433889\n",
      "it 1, train loss = 8.634982e+00\n",
      "0.020072361726795888 333.9887391395746 0.6960810452883894\n",
      "it 1, train loss = 8.982932e+00\n",
      "0.02023538765399917 110.68824705690801 0.4979390889150813\n",
      "it 1, train loss = 8.876440e+00\n",
      "0.019960489499508488 130.00007563958238 0.5049082541032125\n",
      "it 1, train loss = 8.915765e+00\n",
      "0.020628317636892563 284.30571452637736 0.6707492568215859\n",
      "40 [1 3 2 1]\n",
      "it 1, train loss = 8.866568e+00\n",
      "0.020615226895653416 173.14820012507542 0.6124625144258138\n",
      "it 1, train loss = 8.797147e+00\n",
      "0.020192096170764454 147.45136079257497 0.5817874511180433\n",
      "it 1, train loss = 9.375474e+00\n",
      "0.019807435757502087 159.30171996460996 0.5971993415286752\n",
      "it 1, train loss = 8.760143e+00\n",
      "0.029446027947363448 114.9920256019194 0.6895338508481428\n",
      "it 1, train loss = 8.794221e+00\n",
      "0.020319462241143987 165.96036167189976 0.5956267606915212\n",
      "it 1, train loss = 8.169560e+00\n",
      "0.01987170541473098 101.90636077983736 0.549937455738847\n",
      "it 1, train loss = 8.652528e+00\n",
      "0.01955007268055594 122.20006663083113 0.5767372302305444\n",
      "it 1, train loss = 8.771383e+00\n",
      "0.028727393031764078 27208.250780523165 7.0426667724112155\n",
      "it 1, train loss = 9.231776e+00\n",
      "0.020441647160735883 94.1876744485709 0.5486824993043523\n",
      "it 1, train loss = 8.115870e+00\n",
      "0.018379293930785082 300.0974795463419 1.2769770063567905\n",
      "it 1, train loss = 8.563736e+00\n",
      "0.019915050685506537 126.94083604078426 0.5733818303197227\n",
      "it 1, train loss = 9.273419e+00\n",
      "0.020907011408370244 331.4977903673113 0.6690559543704028\n",
      "it 1, train loss = 8.772265e+00\n",
      "0.020504372760865853 178.7017155960249 0.6118032506210705\n",
      "it 1, train loss = 8.721341e+00\n",
      "0.019658755335181386 288.94076089528045 0.6505296730541301\n",
      "it 1, train loss = 8.337919e+00\n",
      "0.020112568089842266 108.16600717261016 0.5638636741478402\n",
      "it 1, train loss = 9.195810e+00\n",
      "0.01984865955868481 168.72336677888973 0.5965338147100134\n",
      "it 1, train loss = 8.781286e+00\n",
      "0.020225114545190808 154.6002259320985 0.6041599202436966\n",
      "it 1, train loss = 8.913993e+00\n",
      "0.01992825675556499 113.77387657460504 0.5525927497806827\n",
      "it 1, train loss = 8.765574e+00\n",
      "0.0191946321677571 168.15360274427417 0.8128844381236215\n",
      "it 1, train loss = 8.633790e+00\n",
      "0.019756927221285275 490.60499654131587 0.7157127756557523\n",
      "it 1, train loss = 8.115160e+00\n",
      "0.030900186714095525 18821.230664795356 5.3303947914175875\n",
      "it 1, train loss = 9.029980e+00\n",
      "0.019996937385854355 143.74843299367998 0.5902699614103332\n",
      "it 1, train loss = 8.422508e+00\n",
      "0.020134126442434466 163.96129864851198 0.6056779271409379\n",
      "it 1, train loss = 8.441327e+00\n",
      "0.01957916667064784 147.1353166531942 0.5244657117528725\n",
      "it 1, train loss = 8.923620e+00\n",
      "0.020099521368966165 139.4696773187819 0.5814635203452729\n",
      "it 1, train loss = 8.748694e+00\n",
      "0.01874826202870347 92.12767306699729 0.5193689732074973\n",
      "it 1, train loss = 8.818467e+00\n",
      "0.020045652156441 123.84744565264522 0.6043343455860273\n",
      "it 1, train loss = 7.867767e+00\n",
      "0.020387520204726787 111.73930911814028 0.48156109108577283\n",
      "it 1, train loss = 8.641943e+00\n",
      "0.020124394168972853 173.00188604721643 0.5389621163256754\n",
      "it 1, train loss = 8.427641e+00\n",
      "0.0197255101399716 219.1004155278263 0.620771799159179\n",
      "it 1, train loss = 9.523518e+00\n",
      "0.02038071198629027 111.87615571671058 0.5808086477985841\n",
      "it 1, train loss = 8.354752e+00\n",
      "0.02034978573381216 130.2376359664814 0.5570490708386485\n",
      "it 1, train loss = 8.889161e+00\n",
      "0.01982703753776518 273.0686490968508 0.6431370823361491\n",
      "it 1, train loss = 9.949797e+00\n",
      "0.02041840307112448 188.19725046286203 0.6290050048797535\n",
      "it 1, train loss = 9.224577e+00\n",
      "0.019948360123047845 219.51086698088906 0.6373848258003879\n",
      "it 1, train loss = 8.140409e+00\n",
      "0.020063883865348516 115.6982278242538 0.5667684871472042\n",
      "it 1, train loss = 9.271057e+00\n",
      "0.020131412289113016 156.51447272219207 0.5687463377726475\n",
      "it 1, train loss = 8.564230e+00\n",
      "0.01979527067209337 203.6685756528002 0.6078355478400909\n",
      "it 1, train loss = 8.514573e+00\n",
      "0.019964653943061298 112.21040542398529 0.5634922486054249\n",
      "it 1, train loss = 8.692677e+00\n",
      "0.0204017067453412 158.31165910137224 0.6072260226986383\n",
      "it 1, train loss = 8.353950e+00\n",
      "0.020023330112068027 121.31468593246112 0.5338716196709694\n",
      "it 1, train loss = 8.522230e+00\n",
      "0.020124054150694393 133.017008547402 0.5564445556997666\n",
      "it 1, train loss = 8.178213e+00\n",
      "0.019958827823001804 130.8706377125824 0.5868024524893103\n",
      "it 1, train loss = 9.375231e+00\n",
      "0.020290952534127688 72.13313843960908 0.5135874470500124\n",
      "it 1, train loss = 8.415510e+00\n",
      "0.020049376024977247 106.50185083999028 0.521775903622406\n",
      "it 1, train loss = 9.029003e+00\n",
      "0.020230285370079096 212.07505030307053 0.6110240689263883\n",
      "it 1, train loss = 8.478342e+00\n",
      "0.02006956259239356 175.3065757760717 0.6057051924434805\n",
      "it 1, train loss = 9.103909e+00\n",
      "0.019963320050353794 141.8554818757329 0.5824374311301007\n",
      "it 1, train loss = 8.615433e+00\n",
      "0.020552846105191147 286.6526880205135 0.6503208190000433\n",
      "it 1, train loss = 9.312576e+00\n",
      "0.020236301780511612 209.40785545441207 0.6315985807203097\n",
      "42 [1 2 3 1]\n",
      "it 1, train loss = 9.105536e+00\n",
      "0.017390224166204817 79.14670447513862 0.5063424508371047\n",
      "it 1, train loss = 8.556660e+00\n",
      "0.019711253040746925 86.73123189956362 0.5433550364830484\n",
      "it 1, train loss = 8.320323e+00\n",
      "0.019909511203238035 157.17492377824738 0.5598032291454498\n",
      "it 1, train loss = 9.038399e+00\n",
      "0.016941683958694834 105.14613480310942 0.5349363953415419\n",
      "it 1, train loss = 8.946786e+00\n",
      "0.019801401891727875 120.48628585438371 0.5471771820309275\n",
      "it 1, train loss = 8.346462e+00\n",
      "0.01967553629992743 53.22447970586513 0.5099210195112053\n",
      "it 1, train loss = 8.619306e+00\n",
      "0.0197495224874499 119.58217799435546 0.5605627772623694\n",
      "it 1, train loss = 9.247161e+00\n",
      "0.019965617272663747 109.2184377950926 0.5449467555468974\n",
      "it 1, train loss = 7.706874e+00\n",
      "0.02543823241862708 104.07874581357764 0.6632495040375785\n",
      "it 1, train loss = 8.612697e+00\n",
      "0.01967045578755707 68.96425777237634 0.49832842954904516\n",
      "it 1, train loss = 8.397373e+00\n",
      "0.01837410054989048 95.85275106884251 0.5118955786098681\n",
      "it 1, train loss = 9.093008e+00\n",
      "0.02649405304633333 1860.7174138154787 1.425512578332683\n",
      "it 1, train loss = 8.854930e+00\n",
      "0.021026753740091966 149.98657972096908 0.6643935276301308\n",
      "it 1, train loss = 8.439322e+00\n",
      "0.02018970633958772 102.20284068394695 0.5560241266909066\n",
      "it 1, train loss = 8.992945e+00\n",
      "0.019897537102600952 72.32451261174336 0.49241293685677684\n",
      "it 1, train loss = 8.765749e+00\n",
      "0.018767692550109932 230.91173932986416 0.6713833192562804\n",
      "it 1, train loss = 8.174138e+00\n",
      "0.01972741071172342 76.5435032991243 0.48864807292053314\n",
      "it 1, train loss = 8.581598e+00\n",
      "0.019609873720947343 99.72013555508586 0.5151029229487978\n",
      "it 1, train loss = 9.019140e+00\n",
      "0.019549630969140377 136.797199693905 0.5822823153647603\n",
      "it 1, train loss = 7.892655e+00\n",
      "0.019330017910602813 146.82165161893252 0.5952372785677598\n",
      "it 1, train loss = 9.198881e+00\n",
      "0.018020075034372706 1013.1530602962603 1.0164907227734001\n",
      "it 1, train loss = 8.725627e+00\n",
      "0.01986534337794353 65.38364566544114 0.5007241114016591\n",
      "it 1, train loss = 8.985476e+00\n",
      "0.02009824347250573 127.35566200703566 0.574215102133692\n",
      "it 1, train loss = 8.422860e+00\n",
      "0.019686313704594564 122.22459190864508 0.5603077463445255\n",
      "it 1, train loss = 8.980978e+00\n",
      "0.019765229983673548 72.87262244430507 0.5033653193417338\n",
      "it 1, train loss = 8.630383e+00\n",
      "0.018410229109003556 62.88281194038216 0.46104096855537996\n",
      "it 1, train loss = 7.935664e+00\n",
      "0.01966888675583336 64.4013738816625 0.5039714046528798\n",
      "it 1, train loss = 8.491846e+00\n",
      "0.020271066596401825 92.39068896946303 0.5221332257522453\n",
      "it 1, train loss = 8.673765e+00\n",
      "0.01767162779193369 81.05881260654083 0.5062370184238679\n",
      "it 1, train loss = 8.600286e+00\n",
      "0.019901307816627637 80.41838387926683 0.5200286914054927\n",
      "it 1, train loss = 8.834978e+00\n",
      "0.02026512786162912 95.99559505177824 0.5876780160896524\n",
      "it 1, train loss = 8.940331e+00\n",
      "0.019407360290728454 154.4220448629575 0.6091244548417174\n",
      "it 1, train loss = 8.991252e+00\n",
      "0.02727941847748637 68.35231138900629 0.4330537541378964\n",
      "it 1, train loss = 9.519361e+00\n",
      "0.018592393527160946 138.26856702450127 0.55540176521312\n",
      "it 1, train loss = 8.920704e+00\n",
      "0.01958977763182779 94.32617296424735 0.5681949561073203\n",
      "it 1, train loss = 8.721401e+00\n",
      "0.01757317668220838 115.41961776443328 0.5256261071712577\n",
      "it 1, train loss = 8.274183e+00\n",
      "0.01987021834004786 130.1683062750862 0.5418573700701886\n",
      "it 1, train loss = 8.030212e+00\n",
      "0.019298703270015486 67.63260101824838 0.5254875380909807\n",
      "it 1, train loss = 9.098900e+00\n",
      "0.020332441433679422 111.193316600308 0.6075414120812964\n",
      "it 1, train loss = 8.451129e+00\n",
      "0.01979700932885337 82.21022634222716 0.5197234356301316\n",
      "it 1, train loss = 9.592290e+00\n",
      "0.019536228845071803 164.37455157330373 0.5640484298563258\n",
      "it 1, train loss = 8.632598e+00\n",
      "0.025970696908935754 100.7095625482478 0.6867753705110954\n",
      "it 1, train loss = 8.373180e+00\n",
      "0.019991793858530692 95.20336816257569 0.5420096013248829\n",
      "it 1, train loss = 9.769252e+00\n",
      "0.02019238216999726 137.55254764937865 0.5689328506602677\n",
      "it 1, train loss = 9.106214e+00\n",
      "0.019180564021731107 277.291535909624 0.6752104905887535\n",
      "it 1, train loss = 8.862665e+00\n",
      "0.019885812828750262 85.58971488570664 0.5383377287901049\n",
      "it 1, train loss = 8.605210e+00\n",
      "0.02005473020327365 114.37994310091884 0.5535903295790179\n",
      "it 1, train loss = 9.010903e+00\n",
      "0.015797988040612925 107.10126588632515 0.5974676456664489\n",
      "it 1, train loss = 8.594433e+00\n",
      "0.019955792161597572 121.07846346481723 0.5562457684286781\n",
      "it 1, train loss = 8.949427e+00\n",
      "0.019826962495803 147.61010164802838 0.5962062815956554\n",
      "52 [1, 3, 3, 1]\n",
      "it 1, train loss = 8.843658e+00\n",
      "0.01922086615815219 154.11755742767278 0.6164836244541595\n",
      "it 1, train loss = 8.944222e+00\n",
      "0.0200532057400963 119.93558594647037 0.5626105847201363\n",
      "it 1, train loss = 8.903202e+00\n",
      "0.01878418014395399 288.5648757892914 0.667845889202054\n",
      "it 1, train loss = 8.131722e+00\n",
      "0.017738143693534297 158.49417273153713 0.5464458376221512\n",
      "it 1, train loss = 9.246144e+00\n",
      "0.019717816479034672 180.01738601769006 0.6211426228022788\n",
      "it 1, train loss = 9.551737e+00\n",
      "0.019871153482064786 163.31322884893828 0.5890679962665596\n",
      "it 1, train loss = 9.054406e+00\n",
      "0.01926998679710931 132.75965004390102 0.5492426013678889\n",
      "it 1, train loss = 8.541638e+00\n",
      "0.01972789724837168 126.31860330865344 0.5535495556597009\n",
      "it 1, train loss = 8.319757e+00\n",
      "0.019844184682215923 127.78149469991001 0.5478259433640019\n",
      "it 1, train loss = 8.676434e+00\n",
      "0.019942469943072017 165.96685841909516 0.6029914540995265\n",
      "it 1, train loss = 9.518488e+00\n",
      "0.01799415973236194 201.85367082710255 0.6710165943651353\n",
      "it 1, train loss = 8.929399e+00\n",
      "0.019282521656588537 142.4173271128211 0.5729268065003447\n",
      "it 1, train loss = 8.903149e+00\n",
      "0.01959096126006483 112.7884783515809 0.5638273406746614\n",
      "it 1, train loss = 8.324602e+00\n",
      "0.018893672466012538 156.1825182324204 0.6982883294248529\n",
      "it 1, train loss = 8.503030e+00\n",
      "0.017672491831011536 183.54150393917644 0.6478064729011394\n",
      "it 1, train loss = 8.885175e+00\n",
      "0.019774519711574613 193.69185800939798 0.589483958818688\n",
      "it 1, train loss = 8.435554e+00\n",
      "0.019783269585095752 101.48042069912914 0.5276628492292446\n",
      "it 1, train loss = 9.110671e+00\n",
      "0.01988501899293094 112.84015284793833 0.5328676325025077\n",
      "it 1, train loss = 8.447112e+00\n",
      "0.019861912584747052 129.37569516745614 0.5603227099545968\n",
      "it 1, train loss = 8.550415e+00\n",
      "0.019692415673895028 183.20186502739688 0.609942874932463\n",
      "it 1, train loss = 8.820165e+00\n",
      "0.020042019988082815 116.73988220682693 0.5535870568314297\n",
      "it 1, train loss = 8.932353e+00\n",
      "0.018389789427190897 97.63801587412976 0.7002001786598578\n",
      "it 1, train loss = 8.549207e+00\n",
      "0.019557270372250725 131.8679271182388 0.541735278868143\n",
      "it 1, train loss = 8.180980e+00\n",
      "0.0197574353251398 195.18381275202705 0.6683020615480599\n",
      "it 1, train loss = 9.797458e+00\n",
      "0.019530892370432488 189.49095440427882 0.6093879270299591\n",
      "it 1, train loss = 8.662056e+00\n",
      "0.018583160945922838 160.07816646373564 0.6595627080162799\n",
      "it 1, train loss = 9.124494e+00\n",
      "0.01990366061419027 235.7729475880654 0.6484535396417923\n",
      "it 1, train loss = 9.162319e+00\n",
      "0.018228346468256048 95.72464249462807 0.4820837185271545\n",
      "it 1, train loss = 7.874577e+00\n",
      "0.019490800150156237 125.4259216120487 0.5713620019684609\n",
      "it 1, train loss = 8.554330e+00\n",
      "0.019858517294675186 133.19421894960763 0.5638674605106753\n",
      "it 1, train loss = 8.588469e+00\n",
      "0.020037031999994622 122.53609667922386 0.5218819949646272\n",
      "it 1, train loss = 8.525352e+00\n",
      "0.016734639230893446 120.04743762448345 0.6677707990897804\n",
      "it 1, train loss = 8.660188e+00\n",
      "0.01968772534405725 96.31177614333694 0.5222500034762639\n",
      "it 1, train loss = 9.082514e+00\n",
      "0.019666364474684447 107.73940361757298 0.5311184247967748\n",
      "it 1, train loss = 8.844080e+00\n",
      "0.018443839543157217 120.73747716752463 0.5094078439693533\n",
      "it 1, train loss = 8.717693e+00\n",
      "0.01906803652897916 163.59058614106527 0.5750385644626387\n",
      "it 1, train loss = 9.567650e+00\n",
      "0.0199440644651738 131.5116427796611 0.5868763295713864\n",
      "it 1, train loss = 8.304647e+00\n",
      "0.017812667019217066 119.67324562206998 0.5295499146816965\n",
      "it 1, train loss = 8.668913e+00\n",
      "0.019716620146009018 120.98699337724086 0.5404721145428303\n",
      "it 1, train loss = 8.614137e+00\n",
      "0.019170620660875156 88.2227554388036 0.5014198557551788\n",
      "it 1, train loss = 8.753844e+00\n",
      "0.01976138145421755 231.74854825052256 0.6280624937897292\n",
      "it 1, train loss = 9.143215e+00\n",
      "0.019707575468333516 123.97723393556076 0.5561098836369976\n",
      "it 1, train loss = 8.511384e+00\n",
      "0.01975513124996548 149.77627079963094 0.5783000804871927\n",
      "it 1, train loss = 9.272818e+00\n",
      "0.019514502377348326 125.0203937088009 0.5348998992641522\n",
      "it 1, train loss = 8.954136e+00\n",
      "0.01642644327008401 120.10145649636495 0.6600160866264332\n",
      "it 1, train loss = 8.470746e+00\n",
      "0.01987504161063496 106.58394213391671 0.5382168943807784\n",
      "it 1, train loss = 9.127405e+00\n",
      "0.019050316829331178 95.18922550870903 0.6671569923150829\n",
      "it 1, train loss = 8.428009e+00\n",
      "0.01987739564275439 124.82395042327568 0.5869541207838536\n",
      "it 1, train loss = 7.958188e+00\n",
      "0.01846573057310178 122.59434589054345 0.6030774443241917\n",
      "it 1, train loss = 9.338990e+00\n",
      "0.01833940913195553 102.10128844579532 0.5100192623994044\n"
     ]
    }
   ],
   "source": [
    "# Next: find the standard deviation of R2 from 50 runs for each case.\n",
    "# Takes more than 3 hours to finish\n",
    "lamUT_vec = np.array(UTdata['F11'])\n",
    "lamET_vec = np.array(ETdata['F11'])\n",
    "lamPS_vec = np.array(PSdata['F11'])\n",
    "np_list = []\n",
    "r2_list_global = []\n",
    "mae_list_global = []\n",
    "for n in range(10,50):\n",
    "    n_params, layers = opt_arch_icnn(n)\n",
    "    if n_params in np_list:\n",
    "        continue\n",
    "    print(n_params, layers)\n",
    "    np_list.append(n_params)\n",
    "\n",
    "    r2_list_arch = []\n",
    "    mae_list_arch = []\n",
    "    for i in range(50): #50 runs for every architecture\n",
    "        key, subkey = random.split(key)\n",
    "        params = init_icnn(key, layers)\n",
    "\n",
    "        opt_init, opt_update, get_params = optimizers.adam(2.e-4) #Original: 2.e-4\n",
    "        opt_state = opt_init(params)\n",
    "        params, train_loss, val_loss = train_jp(loss_P11_UT, 2, F11_data, opt_state, key, nIter = 100000, print_freq=200000)\n",
    "\n",
    "        model = ICNN_model(params[0], params[1], normalization)\n",
    "\n",
    "        lambdas     = [lamUT_vec,       lamET_vec,      lamPS_vec       ]\n",
    "        P11_gt_list = [UTdata['P11'],   ETdata['P11'],  PSdata['P11']   ]\n",
    "        P11funs     = [P11_UT,          P11_ET,         P11_PS          ]\n",
    "\n",
    "        r2 = []\n",
    "        mae = []\n",
    "        for lam, P11_gt, P11fun in zip(lambdas, P11_gt_list, P11funs):\n",
    "            P11 = P11fun(lam, model, normalization)\n",
    "            r2i = r2_score(P11_gt, P11)\n",
    "            r2i = np.clip(r2i, a_min=0.0, a_max=1.0)\n",
    "            r2.append(r2i)\n",
    "\n",
    "            maei = onp.mean(onp.abs(P11_gt-P11))\n",
    "            mae.append(maei)\n",
    "        # r2 = np.mean(np.array(r2))\n",
    "        # print(*r2)\n",
    "        print(*mae)\n",
    "        r2_list_arch.append(r2)\n",
    "        mae_list_arch.append(mae)\n",
    "    r2_list_global.append(r2_list_arch)\n",
    "    mae_list_global.append(mae_list_arch)\n",
    "\n",
    "icnn_r2 = np.array(r2_list_global)\n",
    "icnn_mae = np.array(mae_list_global)\n",
    "with open('savednet/ICNN_r2_efficiency_UT.npy', 'wb') as f:\n",
    "    pickle.dump([np_list, icnn_r2], f)\n",
    "with open('savednet/ICNN_mae_efficiency_UT.npy', 'wb') as f:\n",
    "    pickle.dump([np_list, icnn_mae], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 1, train loss = 4.836975e-01\n",
      "0.5207915931201567 0.011176370172270689 0.13800152849422834\n",
      "it 1, train loss = 1.043110e+00\n",
      "0.5207974129779029 0.011177617699051897 0.13799508124223486\n",
      "it 1, train loss = 2.485455e-01\n",
      "0.2556066842028803 0.012800265149798686 0.16350402936151184\n",
      "it 1, train loss = 1.173598e+00\n",
      "0.5207923654499592 0.011176559498555266 0.1380005567961921\n",
      "it 1, train loss = 1.409967e+00\n",
      "0.5207929387927877 0.01117658083916159 0.13800043004705523\n",
      "it 1, train loss = 3.518570e+01\n",
      "0.2763238990580559 0.012761509465076874 0.16300190605007633\n",
      "it 1, train loss = 8.210497e-01\n",
      "0.5207926625978525 0.011176551856727801 0.1380005764858093\n",
      "it 1, train loss = 1.031412e+00\n",
      "0.5207915769719971 0.01117637035677084 0.13800152853599526\n",
      "it 1, train loss = 1.320198e+00\n",
      "0.520791528038849 0.011176370465625654 0.13800153086205497\n",
      "it 1, train loss = 1.373412e+00\n",
      "0.5207915328771203 0.01117637045725001 0.1380015306204109\n",
      "it 1, train loss = 1.324601e+00\n",
      "0.5207918895160841 0.011176389016090195 0.1380014191241389\n",
      "it 1, train loss = 1.385398e+00\n",
      "0.5207915353929111 0.0111763704530598 0.1380015304941235\n",
      "it 1, train loss = 1.422755e+00\n",
      "0.5207916167896041 0.011176370172801505 0.13800152711604666\n",
      "it 1, train loss = 1.417094e+00\n",
      "0.5207917591449962 0.011176382807549498 0.13800145688652682\n",
      "it 1, train loss = 1.415682e+00\n",
      "0.5207983635412936 0.0111777832992677 0.13799423019464876\n",
      "it 1, train loss = 1.278491e+00\n",
      "0.5207915255065356 0.011176370469481431 0.1380015309911747\n",
      "it 1, train loss = 6.874495e-01\n",
      "0.520791531994568 0.011176370459059977 0.13800153066279894\n",
      "it 1, train loss = 1.094820e+00\n",
      "0.5207915235682133 0.011176370457359822 0.13800153116420918\n",
      "it 1, train loss = 1.325574e+00\n",
      "0.5207915267263321 0.011176370467466256 0.13800153093016057\n",
      "it 1, train loss = 1.347381e+00\n",
      "0.5207915766313138 0.011176370335147956 0.13800152866199694\n",
      "it 1, train loss = 1.393928e+00\n",
      "0.5207916723176259 0.011176371079693765 0.13800151941722344\n",
      "it 1, train loss = 1.207905e+00\n",
      "0.5207915294618359 0.01117637046381193 0.13800153078855013\n",
      "it 1, train loss = 1.342818e+00\n",
      "0.5207915330674022 0.011176370457078633 0.13800153061019169\n",
      "it 1, train loss = 1.377596e+00\n",
      "0.5207915665577757 0.01117637040790569 0.13800152889462816\n",
      "it 1, train loss = 1.055188e+00\n",
      "0.5207987064750612 0.011177964264878968 0.13799331784848995\n",
      "it 1, train loss = 1.226415e+00\n",
      "0.5207917059116629 0.011176371070762026 0.1380015175310816\n",
      "it 1, train loss = 8.490031e+03\n",
      "0.5207915314502611 0.01117637045983626 0.1380015306913124\n",
      "it 1, train loss = 1.317059e+00\n",
      "0.5207915277417957 0.011176370423793547 0.13800153108169771\n",
      "it 1, train loss = 1.135066e+00\n",
      "0.5207915361787099 0.011176372990473997 0.13800151801697697\n",
      "it 1, train loss = 1.280877e+00\n",
      "0.5207915744588248 0.011176370370136794 0.1380015286174334\n",
      "it 1, train loss = 4.886537e-01\n",
      "0.5207915270200065 0.011176370466762503 0.13800153091676134\n",
      "it 1, train loss = 4.462667e-01\n",
      "0.5207915292948156 0.011176370463603234 0.13800153079844435\n",
      "it 1, train loss = 7.221758e-01\n",
      "0.5207915327524862 0.01117637045754562 0.13800153062626777\n",
      "it 1, train loss = 1.024611e+00\n",
      "0.5207915358835681 0.011176370451966575 0.13800153047167488\n",
      "it 1, train loss = 1.226284e+00\n",
      "0.5207915331790379 0.01117637045863148 0.1380015306143785\n",
      "it 1, train loss = 5.007778e-01\n",
      "0.5207815965122636 0.011174131410397907 0.1380130771238293\n",
      "it 1, train loss = 1.031064e+00\n",
      "0.5207916503784066 0.011176368684032698 0.13800153242946092\n",
      "it 1, train loss = 1.226462e+00\n",
      "0.5207915390324555 0.011176370448432638 0.1380015303200167\n",
      "it 1, train loss = 1.345911e+00\n",
      "0.5207885499137326 0.011175601549929318 0.1380054812707629\n",
      "it 1, train loss = 1.377843e+00\n",
      "0.5207915396487137 0.011176370444982391 0.13800153028493528\n",
      "it 1, train loss = 1.341373e+00\n",
      "0.5207915877572681 0.011176370434072918 0.1380015275264673\n",
      "it 1, train loss = 1.049486e+00\n",
      "0.5207912939541722 0.01117626163095496 0.13800207752500263\n",
      "it 1, train loss = 1.367413e+00\n",
      "0.5207915264698952 0.011176370459630698 0.13800153098467868\n",
      "it 1, train loss = 1.360031e+00\n",
      "0.5207922111014482 0.011176468962686772 0.13800100774627275\n",
      "it 1, train loss = 1.365280e+00\n",
      "0.5207915416700177 0.011176370441906539 0.13800153018180208\n",
      "it 1, train loss = 3.861865e+01\n",
      "0.2763238977357338 0.012761509465554866 0.16300190605797774\n",
      "it 1, train loss = 6.732795e-01\n",
      "0.5207916420062277 0.011176370477795372 0.13800152414257288\n",
      "it 1, train loss = 5.449492e-01\n",
      "0.5207915576047655 0.011176370420482147 0.13800152935555565\n",
      "it 1, train loss = 1.116527e+00\n",
      "0.5207917332550229 0.011176378260815849 0.13800148077309415\n",
      "it 1, train loss = 1.089781e+00\n",
      "0.5207918424682899 0.011176398222443333 0.1380013766219763\n"
     ]
    }
   ],
   "source": [
    "lamUT_vec = np.array(UTdata['F11'])\n",
    "lamET_vec = np.array(ETdata['F11'])\n",
    "lamPS_vec = np.array(PSdata['F11'])\n",
    "r2_list = []\n",
    "mae_list = []\n",
    "for i in range(50): #50 runs for every architecture\n",
    "    key, subkey = random.split(key)\n",
    "    params = init_cann(key)\n",
    "\n",
    "    opt_init, opt_update, get_params = optimizers.adam(2.e-4) #Original: 1.e-4\n",
    "    opt_state = opt_init(params)\n",
    "    params, train_loss, val_loss = train_jp(loss_P11_ET, 1, F11_data, opt_state, key, nIter = 100000, print_freq=1000000)\n",
    "\n",
    "    model = CANN_model(params[0], params[1], normalization)\n",
    "\n",
    "    lambdas     = [lamUT_vec,       lamET_vec,      lamPS_vec       ]\n",
    "    P11_gt_list = [UTdata['P11'],   ETdata['P11'],  PSdata['P11']   ]\n",
    "    P11funs     = [P11_UT,          P11_ET,         P11_PS          ]\n",
    "\n",
    "    r2 = []\n",
    "    mae = []\n",
    "    for lam, P11_gt, P11fun in zip(lambdas, P11_gt_list, P11funs):\n",
    "        P11 = P11fun(lam, model, normalization)\n",
    "        r2i = r2_score(P11_gt, P11)\n",
    "        r2i = np.clip(r2i, a_min=0.0, a_max=1.0)\n",
    "        r2.append(r2i)\n",
    "\n",
    "        maei = onp.mean(onp.abs(P11_gt-P11))\n",
    "        mae.append(maei)\n",
    "    # r2 = np.mean(np.array(r2))\n",
    "    print(*mae)\n",
    "    r2_list.append(r2)\n",
    "    mae_list.append(mae)\n",
    "cann_r2 = np.array(r2_list)\n",
    "cann_mae = np.array(mae_list)\n",
    "with open('savednet/CANN_r2_efficiency_ET.npy', 'wb') as f:\n",
    "    pickle.dump([[24], cann_r2], f)\n",
    "with open('savednet/CANN_mae_efficiency_ET.npy', 'wb') as f:\n",
    "    pickle.dump([[24], cann_mae], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 [1 1 2 1]\n",
      "it 1, train loss = 2.043378e+00\n",
      "0.3538260148078883 0.010348760058485899 0.16439756485189924\n",
      "it 1, train loss = 2.453687e+00\n",
      "0.37728738480184665 0.010999080200852296 0.1649827557324491\n",
      "it 1, train loss = 1.358538e+00\n",
      "0.388475582788393 0.011220860279129032 0.1652706141239937\n",
      "it 1, train loss = 3.264843e-01\n",
      "0.6823316604932842 0.007956320735305429 0.13165912146105244\n",
      "it 1, train loss = 2.088349e+00\n",
      "0.3983786836044797 0.011525450840550996 0.1655410336055349\n",
      "it 1, train loss = 1.400940e+00\n",
      "0.37432267331010793 0.010913070344284855 0.16489147585682937\n",
      "it 1, train loss = 2.170448e+00\n",
      "0.3833299107357938 0.011122859063754749 0.16514345537255373\n",
      "it 1, train loss = 2.832625e+00\n",
      "0.6801641224385634 0.007966068911231396 0.13182447774456935\n",
      "it 1, train loss = 2.677568e+00\n",
      "0.37195702406184766 0.01085943294074219 0.16481084047274688\n",
      "it 1, train loss = 1.812568e+00\n",
      "0.38509743669324226 0.011169557814258338 0.16518453277398656\n",
      "it 1, train loss = 2.676838e+00\n",
      "0.3857446514321221 0.011168772409258157 0.1652024764441816\n",
      "it 1, train loss = 1.198793e+00\n",
      "0.36779425341662486 0.010744022227283103 0.16469307594475713\n",
      "it 1, train loss = 9.072794e-01\n",
      "0.375240973505498 0.010939861406678056 0.1649130533958867\n",
      "it 1, train loss = 2.043655e+00\n",
      "0.3719445598145895 0.010862019035849794 0.1648099321780296\n",
      "it 1, train loss = 1.973091e+00\n",
      "0.379898086026234 0.011078877665233081 0.16506263274630184\n",
      "it 1, train loss = 1.370082e+00\n",
      "0.380585645591989 0.011073057872714436 0.16508120728229028\n",
      "it 1, train loss = 6.992924e-01\n",
      "0.701905016407753 0.010212586251720734 0.12248727561390393\n",
      "it 1, train loss = 2.182574e+00\n",
      "0.38606307036597143 0.011174050561395288 0.1652098185275014\n",
      "it 1, train loss = 2.831592e+00\n",
      "0.3923201624377123 0.011329206706429654 0.16536968351642095\n",
      "it 1, train loss = 4.367536e-01\n",
      "0.37699517970103175 0.010975723606822904 0.16496982204978858\n",
      "it 1, train loss = 2.176719e-01\n",
      "0.35738831880742594 0.0104212080208141 0.16455473337076457\n",
      "it 1, train loss = 2.184025e+00\n",
      "0.3835555158941642 0.011129122347169458 0.16514872287778573\n",
      "it 1, train loss = 1.381191e+00\n",
      "0.3597917009592953 0.010529449940741148 0.16452222529615287\n",
      "it 1, train loss = 1.906291e+00\n",
      "0.39144921089019247 0.0113095417293479 0.16534766631432568\n",
      "it 1, train loss = 8.804315e-01\n",
      "0.3982668922047404 0.011507583522403423 0.16553540638479042\n",
      "it 1, train loss = 6.227569e-01\n",
      "0.3967356439098936 0.011085685817455466 0.16492627894925146\n",
      "it 1, train loss = 1.990257e+00\n",
      "0.3491623324452345 0.010181809506985814 0.16430078286918828\n",
      "it 1, train loss = 1.068806e+00\n",
      "0.3932201973583481 0.011324423782480521 0.16551682713185945\n",
      "it 1, train loss = 1.076100e+00\n",
      "0.386853347430993 0.011184834993607297 0.1652286864326434\n",
      "it 1, train loss = 8.111637e-01\n",
      "0.3477085831763347 0.010136554518897336 0.16427165130305937\n",
      "it 1, train loss = 1.035436e+00\n",
      "0.3658600359341564 0.01068944589023756 0.16464477224295088\n",
      "it 1, train loss = 2.855622e+00\n",
      "0.37586112870785016 0.010979712425079773 0.1649310400248827\n",
      "it 1, train loss = 1.923975e+00\n",
      "0.3741785171293445 0.010916845026185356 0.16487998102712442\n",
      "it 1, train loss = 2.341045e+00\n",
      "0.37120908923858636 0.01083706243903677 0.1647854613247193\n",
      "it 1, train loss = 2.985752e+00\n",
      "0.3405981160002372 0.009834919626936566 0.164120901882127\n",
      "it 1, train loss = 2.895850e+00\n",
      "0.33932144477502874 0.009778032513859104 0.16409400454117243\n",
      "it 1, train loss = 1.492029e+00\n",
      "0.3517787848325203 0.010279751146105697 0.1643563498174027\n",
      "it 1, train loss = 1.540147e+00\n",
      "0.3414928491996494 0.00987660077504386 0.16413489080207028\n",
      "it 1, train loss = 2.252255e+00\n",
      "0.3878250630277483 0.011312010124152708 0.16501793667928083\n",
      "it 1, train loss = 2.982996e+00\n",
      "0.3792678850122211 0.011055052979624244 0.1650524477678545\n",
      "it 1, train loss = 2.953298e+00\n",
      "0.38483525425842413 0.011146456449913964 0.16517677321169902\n",
      "it 1, train loss = 2.574504e+00\n",
      "0.38116393846071595 0.011086835397288164 0.16509453639634333\n",
      "it 1, train loss = 2.775163e+00\n",
      "0.3729196144248793 0.010887148213223093 0.1648388268393237\n",
      "it 1, train loss = 2.500027e+00\n",
      "0.3662952080132301 0.010715998944969458 0.16466723076373\n",
      "it 1, train loss = 9.600786e-01\n",
      "0.392467780286153 0.011332060255299124 0.165372567891648\n",
      "it 1, train loss = 2.635183e+00\n",
      "0.6796208594709889 0.007968495352505371 0.13186517603874195\n",
      "it 1, train loss = 2.829249e+00\n",
      "0.3444402287015339 0.01000506534092072 0.16420314177495593\n",
      "it 1, train loss = 2.674994e+00\n",
      "0.37089260102702853 0.010847029117238938 0.1647772741321129\n",
      "it 1, train loss = 2.802101e+00\n",
      "0.3659558957515657 0.010694871826109106 0.1646474064586562\n",
      "it 1, train loss = 1.872936e+00\n",
      "0.3638770748006671 0.010643092040439325 0.16460190881838735\n",
      "32 [1, 2, 2, 1]\n",
      "it 1, train loss = 7.692793e-02\n",
      "0.34711696464443753 0.010092651847822105 0.164314852478509\n",
      "it 1, train loss = 2.544298e-01\n",
      "0.359880520492802 0.010082356199293338 0.1625300377542361\n",
      "it 1, train loss = 1.804776e+00\n",
      "0.3463865342407518 0.010080144674525357 0.16429946718840158\n",
      "it 1, train loss = 1.683043e+00\n",
      "0.3455848090218683 0.010039340447884848 0.16428000520318947\n",
      "it 1, train loss = 2.385418e+00\n",
      "0.34581669592308417 0.010051345179998158 0.16428566039368628\n",
      "it 1, train loss = 7.895586e-01\n",
      "0.3457909725652395 0.01005050413787019 0.16428454593868777\n",
      "it 1, train loss = 1.974494e+00\n",
      "0.3548869953566857 0.010276358286593734 0.16408526936724263\n",
      "it 1, train loss = 2.940122e-01\n",
      "0.3461361411493773 0.010063686029369736 0.16429335341433773\n",
      "it 1, train loss = 7.348327e-01\n",
      "0.34452322953263603 0.009995608340878414 0.16422035461439258\n",
      "it 1, train loss = 1.916182e+00\n",
      "0.6881972928016734 0.008307872003954764 0.13008400107529006\n",
      "it 1, train loss = 1.666238e+00\n",
      "0.3454098128034053 0.010030603729569206 0.16427582848851335\n",
      "it 1, train loss = 2.292326e+00\n",
      "0.3544611624289135 0.009893450926999973 0.163847511541493\n",
      "it 1, train loss = 2.366377e+00\n",
      "0.34609365623685145 0.010068073036699928 0.16428689578586475\n",
      "it 1, train loss = 1.118686e+00\n",
      "0.34556098688905246 0.010037262719498678 0.16427998336846267\n",
      "it 1, train loss = 1.140899e+00\n",
      "0.3462784207259429 0.010074270758224124 0.1642968438713107\n",
      "it 1, train loss = 2.012321e+00\n",
      "0.348826058259362 0.010182343473994467 0.164356705244843\n",
      "it 1, train loss = 2.441624e+00\n",
      "0.3495608890262665 0.01040017911826144 0.16450972110939696\n",
      "it 1, train loss = 2.031276e+00\n",
      "0.3474975011212241 0.00999001849293893 0.1646049549260346\n",
      "it 1, train loss = 8.966782e-01\n",
      "0.34602135312244114 0.01006106874846799 0.16429060300688647\n",
      "it 1, train loss = 2.162972e+00\n",
      "0.34713244446492114 0.010115980869015122 0.1643135706802242\n",
      "it 1, train loss = 1.743500e-01\n",
      "0.685622814792445 0.00779719416325797 0.1322376639028655\n",
      "it 1, train loss = 2.481994e+00\n",
      "0.3519092507637343 0.010331451942616818 0.16443214636817288\n",
      "it 1, train loss = 1.289216e+00\n",
      "0.3493574966315673 0.009831515550877216 0.1632034108333964\n",
      "it 1, train loss = 2.953719e+00\n",
      "0.6788447138258908 0.007965741806349746 0.13186312361415498\n",
      "it 1, train loss = 2.504851e+00\n",
      "0.3457048900821066 0.010045371995923121 0.16428310117191255\n",
      "it 1, train loss = 3.264944e-01\n",
      "0.3495204444143107 0.009976708238267929 0.16397683867804605\n",
      "it 1, train loss = 2.056122e+00\n",
      "0.3480603511188856 0.010160038594775803 0.16433795588039507\n",
      "it 1, train loss = 2.170146e-01\n",
      "0.35816993558109156 0.009776131203851911 0.1641521693035252\n",
      "it 1, train loss = 1.975987e+00\n",
      "0.34683213500751153 0.010104198207489342 0.16430970168926604\n",
      "it 1, train loss = 1.651288e+00\n",
      "0.6832840942208 0.007816519041633338 0.1323723729642588\n",
      "it 1, train loss = 4.601260e-01\n",
      "0.6806110176454808 0.008297726261073883 0.13129902790870962\n",
      "it 1, train loss = 1.563643e-01\n",
      "0.3550152542352215 0.009943067248670268 0.16402726451268343\n",
      "it 1, train loss = 1.272694e-01\n",
      "0.3457298129626343 0.010046976692011074 0.16428403769910963\n",
      "it 1, train loss = 9.366173e-01\n",
      "0.352607566433706 0.01015841110343084 0.16396332011071144\n",
      "it 1, train loss = 2.059694e+00\n",
      "0.34555573595802536 0.010037459432280802 0.16428006195830733\n",
      "it 1, train loss = 2.596869e+00\n",
      "0.6796221607844217 0.007964090375513813 0.1318227886984383\n",
      "it 1, train loss = 2.633067e+00\n",
      "0.34806538016209143 0.010146753244746868 0.16433855918005\n",
      "it 1, train loss = 7.067816e-01\n",
      "0.3472638265978435 0.010109779112324288 0.16431523106987503\n",
      "it 1, train loss = 1.562383e+00\n",
      "0.3468435474192664 0.010103803165519426 0.16431100059989007\n",
      "it 1, train loss = 2.476595e+00\n",
      "0.34598545983739454 0.01005999170777014 0.16428953543651253\n",
      "it 1, train loss = 2.015156e+00\n",
      "0.34617201329838226 0.010068702791800544 0.1642947849939866\n",
      "it 1, train loss = 1.901102e+00\n",
      "0.34965991444525996 0.010075746464369326 0.16419105365583672\n",
      "it 1, train loss = 2.678633e+00\n",
      "0.34610347296544886 0.010065586127165783 0.1642929374677509\n",
      "it 1, train loss = 2.398740e+00\n",
      "0.3463987375795681 0.010077477306642034 0.1642988529429829\n",
      "it 1, train loss = 3.215343e-02\n",
      "0.6805816847654202 0.00796290642593743 0.1317958654621361\n",
      "it 1, train loss = 2.525240e+00\n",
      "0.3466105002138923 0.010223998732260495 0.16420753201836907\n",
      "it 1, train loss = 2.034278e+00\n",
      "0.3561322816245624 0.010322732499465456 0.16451406127125484\n",
      "it 1, train loss = 2.878008e+00\n",
      "0.6873049685681751 0.00735995984333825 0.12967472637701105\n",
      "it 1, train loss = 1.867713e+00\n",
      "0.35162786811182534 0.010098156431961628 0.16397014504179858\n",
      "it 1, train loss = 1.114694e+00\n",
      "0.3457916814419849 0.01004956391497746 0.1642853166314671\n",
      "44 [1 3 2 1]\n",
      "it 1, train loss = 1.922800e+00\n",
      "0.3592195653333702 0.010402894070181353 0.16430304902506046\n",
      "it 1, train loss = 5.302872e-01\n",
      "0.6833617222462103 0.007778272395344007 0.13230377316760988\n",
      "it 1, train loss = 1.734007e+00\n",
      "0.34639215264380935 0.010265303833923914 0.1639094827247059\n",
      "it 1, train loss = 2.469702e+00\n",
      "0.3535784227258929 0.01027161793485536 0.16387440745895215\n",
      "it 1, train loss = 2.975919e+00\n",
      "0.32211227552613536 0.009608518742116975 0.16384454525921585\n",
      "it 1, train loss = 1.305716e+00\n",
      "0.34622867260206547 0.010070063689631363 0.16429517817131628\n",
      "it 1, train loss = 2.953964e+00\n",
      "0.3143437460868977 0.007992722190218536 0.16211699052139764\n",
      "it 1, train loss = 1.798130e-01\n",
      "0.34643525130826475 0.01007788441675747 0.16429764722992823\n",
      "it 1, train loss = 7.772379e-01\n",
      "0.3654866940032018 0.010615571338513638 0.16423083548718892\n",
      "it 1, train loss = 1.283698e+00\n",
      "0.6736612838851187 0.008023256093008762 0.1322384700502607\n",
      "it 1, train loss = 1.583403e+00\n",
      "0.31096841246114765 0.008143257982350402 0.1627719006982718\n",
      "it 1, train loss = 2.987301e+00\n",
      "0.35059938573025917 0.009871065318033974 0.1632050575328092\n",
      "it 1, train loss = 2.241223e+00\n",
      "0.34256832427659545 0.010278420711455424 0.1641901282359876\n",
      "it 1, train loss = 1.369891e-01\n",
      "0.3458905856426131 0.010052515581377642 0.16428780882036587\n",
      "it 1, train loss = 8.351367e-01\n",
      "0.3469549843138074 0.010105928037204129 0.1643099024120616\n",
      "it 1, train loss = 5.350749e-01\n",
      "0.3549696472640551 0.009683914102372421 0.16429367744685333\n",
      "it 1, train loss = 1.201442e+00\n",
      "0.3555420677330726 0.009894906333911971 0.16421575714308154\n",
      "it 1, train loss = 1.239524e-01\n",
      "0.38180823374632533 0.010834663178856035 0.16496936169886883\n",
      "it 1, train loss = 2.989311e+00\n",
      "0.6813266992303284 0.007783174892465974 0.13242828287044087\n",
      "it 1, train loss = 3.207090e-01\n",
      "0.3307305289003406 0.009288967623654228 0.16318285575495883\n",
      "it 1, train loss = 2.959014e+00\n",
      "0.3474541230170505 0.010115599624951427 0.16435367592884\n",
      "it 1, train loss = 3.245432e-01\n",
      "0.6817086155600617 0.007789603699558674 0.1325676779754197\n",
      "it 1, train loss = 1.668668e+00\n",
      "0.34637420959903337 0.010078128147072308 0.16429918890829942\n",
      "it 1, train loss = 2.862790e+00\n",
      "0.31172995482116705 0.008611641871349041 0.16313677875218385\n",
      "it 1, train loss = 2.202616e+00\n",
      "0.3466963053184435 0.010095176223726732 0.16430699332337698\n",
      "it 1, train loss = 1.141607e+00\n",
      "0.6750979741598172 0.007971643621148777 0.1320492507852433\n",
      "it 1, train loss = 2.813916e+00\n",
      "0.3525796445166162 0.00973688500188993 0.16394194556012862\n",
      "it 1, train loss = 1.080650e+00\n",
      "0.35443142226171337 0.009749630516381327 0.16399318210028127\n",
      "it 1, train loss = 1.614037e+00\n",
      "0.3513868441189664 0.010204062857742375 0.16416907775817746\n",
      "it 1, train loss = 1.362847e+00\n",
      "0.36002004108235736 0.010407476301569085 0.16421237559027316\n",
      "it 1, train loss = 1.052976e+00\n",
      "0.35233711041818816 0.01030904506246266 0.164138142627424\n",
      "it 1, train loss = 1.472686e+00\n",
      "0.34786537998197203 0.010152252420150195 0.1643296959407692\n",
      "it 1, train loss = 2.628081e+00\n",
      "0.6847833953976036 0.007245773274437754 0.13011003304272956\n",
      "it 1, train loss = 2.392822e+00\n",
      "0.31638665663237053 0.00817092920665176 0.16290512630527015\n",
      "it 1, train loss = 2.769071e+00\n",
      "0.34672570488377263 0.01009680996710148 0.1643080090913434\n",
      "it 1, train loss = 8.183417e-01\n",
      "0.6810116052380214 0.007797851230265531 0.132417740209985\n",
      "it 1, train loss = 2.258033e+00\n",
      "0.3204216759326692 0.009739449702576492 0.16375096094255603\n",
      "it 1, train loss = 2.338034e+00\n",
      "0.34663242964279783 0.010083920323399517 0.16431912596918008\n",
      "it 1, train loss = 9.092423e-01\n",
      "0.3472677039084576 0.010121311919917374 0.16431728405938348\n",
      "it 1, train loss = 2.591914e+00\n",
      "0.31964761112539497 0.00976016508199432 0.16372316364688697\n",
      "it 1, train loss = 1.667887e+00\n",
      "0.3229161369273567 0.0096902509390706 0.16388979330485434\n",
      "it 1, train loss = 2.467977e+00\n",
      "0.3473243204970492 0.010129711489445206 0.16432191724221834\n",
      "it 1, train loss = 2.469708e+00\n",
      "0.32359026258781237 0.009246268017016375 0.16338263824531776\n",
      "it 1, train loss = 1.064456e+00\n",
      "0.3463880740329897 0.010079383470761392 0.1642993317085251\n",
      "it 1, train loss = 1.410781e+00\n",
      "0.3127402588282805 0.008895473808619521 0.16330495205450715\n",
      "it 1, train loss = 1.297032e+00\n",
      "0.34831226293855294 0.010263510962661234 0.164164435019955\n",
      "it 1, train loss = 2.811130e+00\n",
      "0.3547877712715711 0.010239344652363192 0.16459463557338402\n",
      "it 1, train loss = 2.538008e+00\n",
      "0.3703965938911874 0.010738048058281118 0.1646378141384218\n",
      "it 1, train loss = 1.986166e+00\n",
      "0.7004881735262157 0.007161184728783638 0.1293059117336598\n",
      "it 1, train loss = 1.756948e+00\n",
      "0.33637859815322074 0.009958248113300002 0.1640864712863456\n",
      "60 [1, 3, 3, 1]\n",
      "it 1, train loss = 1.885487e+00\n",
      "0.694128084628173 0.007282400062751736 0.1295815586791609\n",
      "it 1, train loss = 2.021301e+00\n",
      "0.34376803712293963 0.009972786434264106 0.16424583154646902\n",
      "it 1, train loss = 1.840104e+00\n",
      "0.35455927357223904 0.010307048104247866 0.16358199197847947\n",
      "it 1, train loss = 1.575056e-01\n",
      "0.6938220782635197 0.007219600125057202 0.12965138385819064\n",
      "it 1, train loss = 2.855892e+00\n",
      "0.6940162617334008 0.007276486018072571 0.12966075536901725\n",
      "it 1, train loss = 1.372448e+00\n",
      "0.3244528728313603 0.009508809334449038 0.1638588229486346\n",
      "it 1, train loss = 1.943483e+00\n",
      "0.6926645373402062 0.0072634535595677485 0.12964351022767626\n",
      "it 1, train loss = 8.717464e-01\n",
      "0.6935137687384687 0.007274189111879156 0.12973545946751147\n",
      "it 1, train loss = 7.137837e-01\n",
      "0.6922660611349343 0.0071530379990898466 0.12980733875997083\n",
      "it 1, train loss = 2.729848e+00\n",
      "0.3787515091203551 0.011015763868995686 0.1646132545802985\n",
      "it 1, train loss = 2.390054e+00\n",
      "0.693058565197687 0.007190043405740975 0.1297124728138796\n",
      "it 1, train loss = 1.895553e+00\n",
      "0.33893033881775847 0.009775528537701394 0.16308190727994828\n",
      "it 1, train loss = 5.444178e-01\n",
      "0.6932627283766913 0.00716622710561239 0.12950315578950267\n",
      "it 1, train loss = 2.104855e+00\n",
      "0.3227603338615353 0.009665747727865784 0.16643438103648855\n",
      "it 1, train loss = 1.596286e-02\n",
      "0.3633146620434217 0.010764670775439977 0.16385275628436544\n",
      "it 1, train loss = 1.546459e-01\n",
      "0.3565306385907974 0.009976490433200329 0.16483110461493905\n",
      "it 1, train loss = 1.602721e+00\n",
      "0.3673688020300303 0.010732293593497396 0.1645980970666698\n",
      "it 1, train loss = 2.136193e+00\n",
      "0.6941856412890571 0.007102530831898953 0.12942261342627384\n",
      "it 1, train loss = 6.595916e-01\n",
      "0.35819855370461906 0.010035620455596542 0.16355770907440764\n",
      "it 1, train loss = 1.812489e+00\n",
      "0.3427898106459037 0.009919731246714232 0.16422216353339997\n",
      "it 1, train loss = 2.454873e+00\n",
      "0.36384647719331153 0.009988811257206695 0.16519178290474137\n",
      "it 1, train loss = 2.467724e+00\n",
      "0.3328426160849678 0.009837481941537168 0.16258800766301912\n",
      "it 1, train loss = 9.073757e-01\n",
      "0.35307187573333737 0.010329960735457359 0.16314981753371827\n",
      "it 1, train loss = 1.474729e+00\n",
      "0.34341577207993285 0.009875563504657256 0.16348581562586792\n",
      "it 1, train loss = 2.385953e+00\n",
      "0.3413399998865917 0.009949728277373941 0.16348853472584343\n",
      "it 1, train loss = 1.103614e+00\n",
      "0.6906686906617846 0.007231592560241988 0.12987624701535336\n",
      "it 1, train loss = 2.065925e+00\n",
      "0.3431268842851 0.009935294114399203 0.16422936593146062\n",
      "it 1, train loss = 2.547699e+00\n",
      "0.34636753377653656 0.009710126875939435 0.16447678770020144\n",
      "it 1, train loss = 1.971377e+00\n",
      "0.36688983054260715 0.01045569287636293 0.16473634170833146\n",
      "it 1, train loss = 9.059655e-01\n",
      "0.3426286737674906 0.009911236936493968 0.16421610377663448\n",
      "it 1, train loss = 2.464314e+00\n",
      "0.34425047192431024 0.010301145336721721 0.1643770139133934\n",
      "it 1, train loss = 8.890172e-01\n",
      "0.37515730226242633 0.010545597520365706 0.16442457326883994\n",
      "it 1, train loss = 1.756219e+00\n",
      "0.6934532097257633 0.0071331532152957445 0.12947635627739248\n",
      "it 1, train loss = 8.366171e-01\n",
      "0.6938939919959172 0.007279108670828481 0.12958208742170532\n",
      "it 1, train loss = 6.708024e-01\n",
      "0.31753866130324904 0.008715257452929668 0.16370072830730673\n",
      "it 1, train loss = 2.441061e+00\n",
      "0.35219508454487747 0.009765016684184051 0.16375655333330794\n",
      "it 1, train loss = 1.625297e+00\n",
      "0.6934823049953621 0.00727622846730723 0.12965681081203098\n",
      "it 1, train loss = 2.516237e-01\n",
      "0.6939228327269121 0.007262235460574003 0.12960907533072855\n",
      "it 1, train loss = 1.540987e+00\n",
      "0.6934704206158421 0.007276624576427067 0.12960240239721826\n",
      "it 1, train loss = 4.457616e-01\n",
      "0.6937899479372193 0.007271349485228889 0.1298055879389188\n",
      "it 1, train loss = 2.115155e+00\n",
      "0.3658966901565971 0.011040296858179774 0.16386454823372412\n",
      "it 1, train loss = 1.412479e-01\n",
      "0.6934097967037681 0.007208422316190845 0.12979342144578068\n",
      "it 1, train loss = 3.960121e-01\n",
      "0.6941056359640071 0.007239575574904099 0.1294687836302383\n",
      "it 1, train loss = 2.574819e+00\n",
      "0.36536164572084856 0.010157294093264173 0.1645501827625899\n",
      "it 1, train loss = 2.831616e+00\n",
      "0.6920679726316962 0.007276554907409034 0.1297115048986482\n",
      "it 1, train loss = 1.127932e-02\n",
      "0.6940256587069671 0.007213191777228165 0.12963403654122765\n",
      "it 1, train loss = 5.055403e-01\n",
      "0.6935090953354885 0.007211816795873602 0.12975451968514848\n",
      "it 1, train loss = 1.293406e+00\n",
      "0.37328279625309563 0.010793470256035966 0.1646468687530168\n",
      "it 1, train loss = 3.028742e-01\n",
      "0.6932016200222364 0.007267657092390164 0.1296876138927984\n",
      "it 1, train loss = 2.896597e-01\n",
      "0.6944570018953468 0.007118967001410709 0.1294204566759065\n",
      "76 [1 4 3 1]\n",
      "it 1, train loss = 1.010125e+00\n",
      "0.6927283176619113 0.0072702666429889165 0.12977325425920602\n",
      "it 1, train loss = 2.409324e+00\n",
      "0.6932286246972725 0.007267264708150346 0.1297960595763728\n",
      "it 1, train loss = 2.105328e+00\n",
      "0.6874258712749947 0.007375869239517513 0.1301573304326953\n",
      "it 1, train loss = 2.906256e+00\n",
      "0.6931858299403383 0.00716407259209147 0.12967496779367185\n",
      "it 1, train loss = 1.718744e-01\n",
      "0.6938621138422261 0.007081076562214731 0.1295536921294823\n",
      "it 1, train loss = 2.625665e+00\n",
      "0.31477272108760973 0.009243931343506814 0.16326795285018816\n",
      "it 1, train loss = 1.926626e+00\n",
      "0.3693619893111306 0.0110011608552401 0.16304708155471417\n",
      "it 1, train loss = 2.026442e+00\n",
      "0.34332512134316756 0.00992909954318532 0.16368882547519686\n",
      "it 1, train loss = 1.983374e+00\n",
      "0.6924723754769257 0.007177991376769637 0.12974433357901524\n",
      "it 1, train loss = 1.499781e+00\n",
      "0.363313932677535 0.01006526762689857 0.16417769296966161\n",
      "it 1, train loss = 1.904150e+00\n",
      "0.3812161893142594 0.010734577732211 0.1641931946720924\n",
      "it 1, train loss = 1.208657e+00\n",
      "0.31545598456038965 0.008192354575567905 0.16290592496002518\n",
      "it 1, train loss = 1.972033e+00\n",
      "0.6933011065585717 0.007164160447533166 0.1294744771954229\n",
      "it 1, train loss = 2.987661e-02\n",
      "0.6928173532214075 0.007164011139707892 0.1296694797650217\n",
      "it 1, train loss = 1.394916e+00\n",
      "0.6931582913602773 0.007067375976687538 0.12957412875386304\n",
      "it 1, train loss = 2.184335e+00\n",
      "0.6929076643071415 0.007270699315593668 0.1297467621892297\n",
      "it 1, train loss = 2.603494e+00\n",
      "0.37433173256978497 0.011046912773334364 0.16480840887399317\n",
      "it 1, train loss = 1.746643e+00\n",
      "0.6916915678889312 0.0073031662608846895 0.12983410190330336\n",
      "it 1, train loss = 1.430294e+00\n",
      "0.6904110673408266 0.007304570534920686 0.12986754153805893\n",
      "it 1, train loss = 7.504379e-01\n",
      "0.31568410986449696 0.008216275352800102 0.1628392394325191\n",
      "it 1, train loss = 8.774384e-01\n",
      "0.346749515559972 0.00980625610574682 0.16425265875335418\n",
      "it 1, train loss = 2.697840e+00\n",
      "0.7006952795334058 0.007094289255605768 0.12943202442861904\n",
      "it 1, train loss = 1.657949e+00\n",
      "0.31526255864998165 0.008170716997921405 0.1628957567283022\n",
      "it 1, train loss = 1.838197e+00\n",
      "0.33039369079916653 0.009606183671550375 0.1635959288767474\n",
      "it 1, train loss = 2.298444e+00\n",
      "0.6916939029282474 0.00721265194636813 0.1298899742088907\n",
      "it 1, train loss = 1.259700e+00\n",
      "0.6919993702046214 0.007261190101546032 0.1297864062203777\n",
      "it 1, train loss = 2.488187e+00\n",
      "0.692647244486591 0.007289478517150618 0.1298517775610158\n",
      "it 1, train loss = 2.982955e-02\n",
      "0.6939966703327205 0.007011440320575683 0.12950375922408766\n",
      "it 1, train loss = 1.699025e+00\n",
      "0.7007655235804576 0.007010574349748507 0.12915658609549854\n",
      "it 1, train loss = 2.556303e+00\n",
      "0.6908521612040728 0.007204980665897914 0.1297726069060215\n",
      "it 1, train loss = 4.127817e-01\n",
      "0.693485529909 0.0071068580945992214 0.12954673237413297\n",
      "it 1, train loss = 6.419387e-01\n",
      "0.36750647009760146 0.010976178765004022 0.16399482008558725\n",
      "it 1, train loss = 2.196999e+00\n",
      "0.3478971915828461 0.009830235597393985 0.16382996063425473\n",
      "it 1, train loss = 2.082707e+00\n",
      "0.6929495902502881 0.007069222321121908 0.12962219462726018\n",
      "it 1, train loss = 1.246277e+00\n",
      "0.37210533256204265 0.010328463536794964 0.164693952871446\n",
      "it 1, train loss = 2.327030e+00\n",
      "0.3598881034421677 0.010189323061014496 0.16466499763399373\n",
      "it 1, train loss = 2.111457e+00\n",
      "0.6927204083205121 0.007157869094325432 0.12956989514935982\n",
      "it 1, train loss = 1.479783e+00\n",
      "0.259792936637467 0.00784269628455685 0.1569591937179753\n",
      "it 1, train loss = 2.168329e+00\n",
      "0.3209833367349588 0.009202735161698464 0.16372601574996778\n",
      "it 1, train loss = 1.843673e+00\n",
      "0.691295456370079 0.007160166438705634 0.1298973749771642\n",
      "it 1, train loss = 2.122564e+00\n",
      "0.3194910891933707 0.009196634919249252 0.1633801524843425\n",
      "it 1, train loss = 2.088079e+00\n",
      "0.6927955134452722 0.007222484754705392 0.12977497821126036\n",
      "it 1, train loss = 2.612785e+00\n",
      "0.33396423093605626 0.01031983412522357 0.16416346723799746\n",
      "it 1, train loss = 1.519921e+00\n",
      "0.6933305277011228 0.007164201011256523 0.1296734158407296\n",
      "it 1, train loss = 2.179641e+00\n",
      "0.37068932402469135 0.010525221394373549 0.16450010114865526\n",
      "it 1, train loss = 1.266847e+00\n",
      "0.6921669235631053 0.007257052980009034 0.12981300459629413\n",
      "it 1, train loss = 2.467894e+00\n",
      "0.36998290554326446 0.010518564861319066 0.1643057005435428\n",
      "it 1, train loss = 4.522434e-01\n",
      "0.6920062834100544 0.00725576835656446 0.12977719609600827\n",
      "it 1, train loss = 1.067990e-01\n",
      "0.3619760724604696 0.010153630165692606 0.16398380353727507\n",
      "it 1, train loss = 2.609391e+00\n",
      "0.6913152967270603 0.0071724502585136865 0.12982148300776802\n",
      "96 [1, 4, 4, 1]\n",
      "it 1, train loss = 1.827939e+00\n",
      "0.6945103904872622 0.007231233405217478 0.12967543332810905\n",
      "it 1, train loss = 2.758299e+00\n",
      "0.6939265155060019 0.0067531001676849126 0.12928972106573888\n",
      "it 1, train loss = 1.557220e+00\n",
      "0.6947176116625515 0.0072145912588437375 0.1297345102698695\n",
      "it 1, train loss = 7.757135e-01\n",
      "0.6958151888763845 0.007144650802201388 0.12946485641107192\n",
      "it 1, train loss = 7.273597e-01\n",
      "0.6945854181415859 0.00706366309909451 0.12956581148268936\n",
      "it 1, train loss = 1.392134e+00\n",
      "0.32884296368976146 0.009335605227431152 0.16707333227844964\n",
      "it 1, train loss = 2.977193e+00\n",
      "0.34543132857050296 0.010871030982204138 0.1647775035996538\n",
      "it 1, train loss = 2.666906e+00\n",
      "0.6947808680729913 0.007249746631242037 0.12969554698230873\n",
      "it 1, train loss = 1.680491e+00\n",
      "0.6953916786191476 0.00722856086669562 0.12966621161062952\n",
      "it 1, train loss = 1.015108e+00\n",
      "0.7009584642778861 0.00725800169561702 0.12943750368475196\n",
      "it 1, train loss = 2.044916e+00\n",
      "0.6956657404992066 0.007222548003717325 0.12948650111374385\n",
      "it 1, train loss = 1.612236e+00\n",
      "0.31972292944850417 0.008911691868850857 0.1659316374682347\n",
      "it 1, train loss = 1.694647e-01\n",
      "0.695233561370678 0.007105087416091486 0.12956777106359138\n",
      "it 1, train loss = 3.741667e-01\n",
      "0.694716928935826 0.007108654260171639 0.12971156184768043\n",
      "it 1, train loss = 2.265187e+00\n",
      "0.6951353877121106 0.006702443431302392 0.1292110476409358\n",
      "it 1, train loss = 1.401795e+00\n",
      "0.6951488377346089 0.0072162819811055915 0.12970321442902238\n",
      "it 1, train loss = 2.638953e+00\n",
      "0.3263623127737389 0.009547789046005978 0.1630901487890188\n",
      "it 1, train loss = 4.005635e-01\n",
      "0.6945573036089071 0.007131598096164966 0.129744159176916\n",
      "it 1, train loss = 1.639362e+00\n",
      "0.36461321167691507 0.010573644940382203 0.16353493615214068\n",
      "it 1, train loss = 2.377581e+00\n",
      "0.6943856840335494 0.0072531066370919986 0.12978012090984437\n",
      "it 1, train loss = 2.535438e+00\n",
      "0.6954710331192024 0.007214513244906427 0.12970561195705593\n",
      "it 1, train loss = 2.747437e+00\n",
      "0.35522321032341236 0.010391240238333951 0.1640470455236931\n",
      "it 1, train loss = 3.500626e-01\n",
      "0.6948959202780702 0.007162890900667692 0.12985222793485005\n",
      "it 1, train loss = 1.947731e+00\n",
      "0.6951060675667898 0.006954468838535017 0.12939741262445595\n",
      "it 1, train loss = 1.301373e+00\n",
      "0.693485056468642 0.0070804996322118225 0.12989491493962146\n",
      "it 1, train loss = 2.476363e+00\n",
      "0.32390174405198285 0.008127643835004875 0.1629722366647428\n",
      "it 1, train loss = 2.930959e+00\n",
      "0.6954813826600466 0.007246099223468834 0.12960395581030523\n",
      "it 1, train loss = 9.514930e-01\n",
      "0.3256313535939233 0.009841703748260197 0.16593156829927505\n",
      "it 1, train loss = 1.938435e-01\n",
      "0.3393457622648262 0.009665872123739918 0.1638902583875195\n",
      "it 1, train loss = 1.802383e+00\n",
      "0.6954682385405303 0.007257753469815971 0.12971044225222664\n",
      "it 1, train loss = 1.998717e+00\n",
      "0.6957227988724719 0.007074414659132074 0.1293875763259018\n",
      "it 1, train loss = 1.322097e+00\n",
      "0.366816754727883 0.010135884296083024 0.16463840940741767\n",
      "it 1, train loss = 2.490944e+00\n",
      "0.35470426254367365 0.010923766691014889 0.16589163504231694\n",
      "it 1, train loss = 1.287664e+00\n",
      "0.7010113883957989 0.00722615642834753 0.1293069498164218\n",
      "it 1, train loss = 9.703173e-01\n",
      "0.6951595621957238 0.0070380583445487895 0.1295112096192233\n",
      "it 1, train loss = 2.622709e+00\n",
      "0.6952232401045277 0.0070832810892872 0.12953770208649526\n",
      "it 1, train loss = 1.897869e-01\n",
      "0.3142235424523261 0.009722399953106782 0.16400843360882583\n",
      "it 1, train loss = 1.831191e+00\n",
      "0.6948184027502851 0.007137035649587055 0.1296895585387477\n",
      "it 1, train loss = 8.981630e-01\n",
      "0.34059569474594026 0.00975892634626892 0.1643418467190681\n",
      "it 1, train loss = 2.171938e+00\n",
      "0.694823448613337 0.007061922404863858 0.1294997719447895\n",
      "it 1, train loss = 1.742304e+00\n",
      "0.6952642597301464 0.006738298220056277 0.1291718922299507\n",
      "it 1, train loss = 8.456162e-03\n",
      "0.6958805876382477 0.007149788726388488 0.1296647562874442\n",
      "it 1, train loss = 2.273256e-01\n",
      "0.7007497037699555 0.007190159421081979 0.1294256747435744\n",
      "it 1, train loss = 2.902202e-01\n",
      "0.6949046459496725 0.007159989010329669 0.12959651304893458\n",
      "it 1, train loss = 2.947391e+00\n",
      "0.3357843248081507 0.00976780194120713 0.1634363878546326\n",
      "it 1, train loss = 1.043555e+00\n",
      "0.7009058615224915 0.007064714617005528 0.12933230161859038\n",
      "it 1, train loss = 1.880737e+00\n",
      "0.31798780650542285 0.009983273660749041 0.16409163469948965\n",
      "it 1, train loss = 1.257028e+00\n",
      "0.6947025640932462 0.007130566662908527 0.12946564514074574\n",
      "it 1, train loss = 8.301503e-01\n",
      "0.35831725069358095 0.010457481767414616 0.1643239328067482\n",
      "it 1, train loss = 2.488654e+00\n",
      "0.3387194051384806 0.00976454289870015 0.1633204233102022\n"
     ]
    }
   ],
   "source": [
    "lamUT_vec = np.array(UTdata['F11'])\n",
    "lamET_vec = np.array(ETdata['F11'])\n",
    "lamPS_vec = np.array(PSdata['F11'])\n",
    "np_list = []\n",
    "r2_list_global = []\n",
    "mae_list_global = []\n",
    "for n in range(10,50):\n",
    "    n_params, layers = opt_arch_node(n)\n",
    "    if n_params in np_list:\n",
    "        continue\n",
    "    print(n_params, layers)\n",
    "    np_list.append(n_params)\n",
    "\n",
    "    r2_list_arch = []\n",
    "    mae_list_arch = []\n",
    "    for i in range(50): #50 runs for every architecture\n",
    "        key, subkey = random.split(key)\n",
    "        params = init_node(key, layers)\n",
    "\n",
    "        # Make sure you are starting at a good place\n",
    "        loss = loss_P11_ET(params, F11_data, 3)\n",
    "        while loss>3.0:\n",
    "            key, subkey = random.split(key)\n",
    "            params = init_node(key, layers)\n",
    "            loss = loss_P11_ET(params, F11_data, 3)\n",
    "\n",
    "        opt_init, opt_update, get_params = optimizers.adam(2.e-4) #Original: 2.e-4\n",
    "        opt_state = opt_init(params)\n",
    "        params, train_loss, val_loss = train_jp(loss_P11_ET, 3, F11_data, opt_state, key, nIter = 100000, print_freq=1000000)\n",
    "\n",
    "        model = NODE_model(params[0], params[1])\n",
    "\n",
    "        lambdas     = [lamUT_vec,       lamET_vec,      lamPS_vec       ]\n",
    "        P11_gt_list = [UTdata['P11'],   ETdata['P11'],  PSdata['P11']   ]\n",
    "        P11funs     = [P11_UT,          P11_ET,         P11_PS          ]\n",
    "\n",
    "        r2 = []\n",
    "        mae = []\n",
    "        for lam, P11_gt, P11fun in zip(lambdas, P11_gt_list, P11funs):\n",
    "            P11 = P11fun(lam, model, normalization)\n",
    "            r2i = r2_score(P11_gt, P11)\n",
    "            r2i = np.clip(r2i, a_min=0.0, a_max=1.0)\n",
    "            r2.append(r2i)\n",
    "\n",
    "            maei = onp.mean(onp.abs(P11_gt-P11))\n",
    "            mae.append(maei)\n",
    "        # r2 = np.mean(np.array(r2))\n",
    "        # print(*r2)\n",
    "        print(*mae)\n",
    "        r2_list_arch.append(r2)\n",
    "        mae_list_arch.append(mae)\n",
    "    r2_list_global.append(r2_list_arch)\n",
    "    mae_list_global.append(mae_list_arch)\n",
    "node_r2 = np.array(r2_list_global)\n",
    "node_mae = np.array(mae_list_global)\n",
    "with open('savednet/NODE_r2_efficiency_ET.npy', 'wb') as f:\n",
    "    pickle.dump([np_list, node_r2], f)\n",
    "\n",
    "with open('savednet/NODE_mae_efficiency_ET.npy', 'wb') as f:\n",
    "    pickle.dump([np_list, node_mae], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 [1 1 1]\n",
      "it 1, train loss = 1.473255e+00\n",
      "0.6811328862648339 0.010978461505322775 0.13182750214792863\n",
      "it 1, train loss = 1.498649e+00\n",
      "0.6807276118724619 0.010978430280222874 0.1319408268483123\n",
      "it 1, train loss = 1.440350e+00\n",
      "0.6818994020155802 0.010977801339039412 0.1316332112339225\n",
      "it 1, train loss = 1.520532e+00\n",
      "0.6802415894568427 0.010979028489811339 0.1320592075237008\n",
      "it 1, train loss = 1.390548e+00\n",
      "0.4235871551578899 0.007028630912848214 0.27526473199768337\n",
      "it 1, train loss = 1.539730e+00\n",
      "0.6794716165453802 0.010978469784883357 0.13227871857519496\n",
      "it 1, train loss = 1.335473e+00\n",
      "0.3981167731889378 0.006977972880971861 0.2621121819986138\n",
      "it 1, train loss = 1.264217e+00\n",
      "0.6834851884977818 0.010977452450670755 0.13120422978484866\n",
      "it 1, train loss = 1.463971e+00\n",
      "0.6796285565073542 0.01097642700464562 0.13226912348499414\n",
      "it 1, train loss = 1.321672e+00\n",
      "0.6839037590665454 0.010974708169846028 0.13109070196580472\n",
      "it 1, train loss = 1.486101e+00\n",
      "0.6796590425803629 0.01097703970095979 0.1322509717299918\n",
      "it 1, train loss = 1.240902e+00\n",
      "0.6855164910417091 0.010978544442154022 0.13064446581654546\n",
      "it 1, train loss = 1.350914e+00\n",
      "0.6806359910468315 0.010975004902317093 0.13201462240140968\n",
      "it 1, train loss = 1.544367e+00\n",
      "0.6793963304355373 0.010978589392123006 0.13229844747970768\n",
      "it 1, train loss = 1.275844e+00\n",
      "0.40839211920470553 0.00697556997032841 0.26952300991244177\n",
      "it 1, train loss = 1.453589e+00\n",
      "0.6795564426570582 0.010976114262953847 0.13229400228906255\n",
      "it 1, train loss = 1.366725e+00\n",
      "0.6830597156746744 0.010978081580720962 0.13131084428624268\n",
      "it 1, train loss = 1.491702e+00\n",
      "0.6801134984547962 0.010978082998642912 0.13211422037639298\n",
      "it 1, train loss = 1.303887e+00\n",
      "0.6843524159924255 0.010977890215159198 0.13096595618074788\n",
      "it 1, train loss = 1.069618e+00\n",
      "0.6853998621410858 0.010994182153823025 0.1306317747923546\n",
      "it 1, train loss = 9.441999e-01\n",
      "0.6831025269738927 0.010975958914727557 0.1313293329482862\n",
      "it 1, train loss = 1.400681e+00\n",
      "0.6795405151785188 0.010964860715125294 0.13233915545402739\n",
      "it 1, train loss = 1.521334e+00\n",
      "0.680237687944171 0.010979110727655087 0.13205799379518365\n",
      "it 1, train loss = 1.272927e+00\n",
      "0.4107645316308609 0.006978139959683553 0.27109238586781054\n",
      "it 1, train loss = 1.488185e+00\n",
      "0.6788928686464275 0.010976368576327352 0.1324734417521503\n",
      "it 1, train loss = 1.310652e+00\n",
      "0.6842636387383567 0.010977834992656196 0.1309914342461374\n",
      "it 1, train loss = 1.522130e+00\n",
      "0.6787912766894929 0.010962281123737577 0.1325224408856725\n",
      "it 1, train loss = 1.577702e+00\n",
      "0.3481850697976337 0.04365855789827489 0.1780132998321977\n",
      "it 1, train loss = 1.317220e+00\n",
      "0.6830971467329087 0.10177707002593342 0.1577290639716605\n",
      "it 1, train loss = 1.473975e+00\n",
      "0.6798029607268716 0.01097687067771853 0.13221402952203132\n",
      "it 1, train loss = 1.457335e+00\n",
      "0.6815080267972773 0.01097809694439277 0.13173361051148752\n",
      "it 1, train loss = 1.308121e+00\n",
      "0.6840535189455187 0.010978291706294937 0.13103749048977903\n",
      "it 1, train loss = 1.607065e+00\n",
      "0.6769397175392889 0.010978936139759761 0.13298388987952378\n",
      "it 1, train loss = 1.577054e+00\n",
      "0.7587871344368893 0.17197593017352664 0.06349520614345702\n",
      "it 1, train loss = 1.490334e+00\n",
      "0.7322197099681305 0.1445658217905841 0.09301939901315326\n",
      "it 1, train loss = 1.613925e+00\n",
      "0.6785229561353071 0.010979688239242488 0.13253277167614316\n",
      "it 1, train loss = 1.311234e+00\n",
      "0.6816475007409943 0.10097961939491724 0.15998739378178303\n",
      "it 1, train loss = 1.399755e+00\n",
      "0.6809812641444851 0.010976381997755988 0.13189854095407505\n",
      "it 1, train loss = 1.238855e+00\n",
      "0.6849585404422756 0.010978828378619668 0.13078445964358304\n",
      "it 1, train loss = 1.432110e+00\n",
      "0.6794866600757368 0.010978300136837163 0.13231588368400657\n",
      "it 1, train loss = 1.473752e+00\n",
      "0.681099911576322 0.01097851189677626 0.13183476512902764\n",
      "it 1, train loss = 1.309886e+00\n",
      "0.6841279264428435 0.010978129415083216 0.13102175298007399\n",
      "it 1, train loss = 1.385448e+00\n",
      "0.6812111016421522 0.010976319449873163 0.13183672242494565\n",
      "it 1, train loss = 1.388619e+00\n",
      "0.6821623377888227 0.01094954499476084 0.13161759035077222\n",
      "it 1, train loss = 1.272666e+00\n",
      "0.6841582258924729 0.010978197506157503 0.13100882491186025\n",
      "it 1, train loss = 9.862308e-01\n",
      "0.6855680790998326 0.010978906203675556 0.13062059928685502\n",
      "it 1, train loss = 1.220469e+00\n",
      "0.6805678486250331 0.010973438692457907 0.13205834714740644\n",
      "it 1, train loss = 1.654488e+00\n",
      "0.6763900473844617 0.010979524720883403 0.1331212382096656\n",
      "it 1, train loss = 1.349022e+00\n",
      "0.691019550381429 0.10659635801195697 0.14539392668342657\n",
      "it 1, train loss = 1.366538e+00\n",
      "0.682029025191992 0.010976998479331892 0.1316033675990004\n",
      "32 [1, 2, 1]\n",
      "it 1, train loss = 1.441003e+00\n",
      "0.6827210242478692 0.010982650191387569 0.13132528714242955\n",
      "it 1, train loss = 1.329821e+00\n",
      "0.6829478498734229 0.010977476022965789 0.1313487647066814\n",
      "it 1, train loss = 1.428739e+00\n",
      "0.6582036332893272 0.007297514104321162 0.15616454389528542\n",
      "it 1, train loss = 1.322220e+00\n",
      "0.6836182183341917 0.010975684932752836 0.131188135616814\n",
      "it 1, train loss = 1.511863e+00\n",
      "0.680862344115718 0.01098157063763893 0.1318467702146891\n",
      "it 1, train loss = 1.485680e+00\n",
      "0.6820426932829758 0.010984026385591376 0.1314906181930632\n",
      "it 1, train loss = 1.319545e+00\n",
      "0.6814729469066245 0.010972021217995913 0.13184314932981248\n",
      "it 1, train loss = 1.405637e+00\n",
      "0.41597554950987176 0.007688903542194432 0.25947397784251686\n",
      "it 1, train loss = 1.388360e+00\n",
      "0.6826988665408414 0.01098387528810767 0.1313145624821401\n",
      "it 1, train loss = 1.433552e+00\n",
      "0.6818318505259703 0.010985619373028685 0.13152786055074867\n",
      "it 1, train loss = 1.286829e+00\n",
      "0.6823101013492567 0.010975795664958595 0.13155313842661567\n",
      "it 1, train loss = 1.453932e+00\n",
      "0.681709689277593 0.010981016429711806 0.13162783183573548\n",
      "it 1, train loss = 1.568352e+00\n",
      "0.6809517989296446 0.010988188586028857 0.1317202432309675\n",
      "it 1, train loss = 1.280428e+00\n",
      "0.6500620946053448 0.007155394137078434 0.16198111703569534\n",
      "it 1, train loss = 1.459470e+00\n",
      "0.6824216713593464 0.010985178423555666 0.13137184647801198\n",
      "it 1, train loss = 1.483683e+00\n",
      "0.6809478641611426 0.010982216363608722 0.1318127816531433\n",
      "it 1, train loss = 1.487361e+00\n",
      "0.6801149128833175 0.010977785794884874 0.13211248204464476\n",
      "it 1, train loss = 1.312075e+00\n",
      "0.6831750456658584 0.010973541685841954 0.13135584621735302\n",
      "it 1, train loss = 1.487485e+00\n",
      "0.3459355216626044 0.006370598967599112 0.21756566993748766\n",
      "it 1, train loss = 1.211168e+00\n",
      "0.6824037409179989 0.010977705619672864 0.13149244627945453\n",
      "it 1, train loss = 1.389604e+00\n",
      "0.6810248567367759 0.010975014554175325 0.1319110726066735\n",
      "it 1, train loss = 1.317756e+00\n",
      "0.6835015350767639 0.01097512120986789 0.131239743071429\n",
      "it 1, train loss = 1.527878e+00\n",
      "0.6812996968961658 0.010986801625199391 0.1316480088856847\n",
      "it 1, train loss = 1.515682e+00\n",
      "0.6800669646237069 0.010980583923847029 0.1320898630635273\n",
      "it 1, train loss = 1.516960e+00\n",
      "0.6587974542312302 0.007370136126669904 0.15603573328408427\n",
      "it 1, train loss = 1.453107e+00\n",
      "0.6815996645423738 0.010888912191313934 0.13186718803895356\n",
      "it 1, train loss = 1.313114e+00\n",
      "0.6819680530143526 0.010971565529809213 0.13171062514476303\n",
      "it 1, train loss = 1.490039e+00\n",
      "0.6814491725638967 0.010983885879527018 0.1316485448166629\n",
      "it 1, train loss = 1.296911e+00\n",
      "0.6821466787716823 0.010972461884714495 0.13164811887991548\n",
      "it 1, train loss = 1.371235e+00\n",
      "0.6829571554898142 0.010977071006605528 0.13132905268335035\n",
      "it 1, train loss = 1.608051e+00\n",
      "0.67996161715353 0.010986191587742323 0.13201741755810706\n",
      "it 1, train loss = 1.477614e+00\n",
      "0.6394520259160185 0.007125398173526158 0.16666780486416963\n",
      "it 1, train loss = 1.356648e+00\n",
      "0.6816209289809799 0.01082871229970843 0.13224433673631902\n",
      "it 1, train loss = 1.321309e+00\n",
      "0.6428821327695908 0.007113729620257915 0.16662490867424365\n",
      "it 1, train loss = 1.445588e+00\n",
      "0.6819495684798613 0.010969223130658845 0.13158911564992762\n",
      "it 1, train loss = 1.178532e+00\n",
      "0.6854328627506547 0.01097349987202621 0.1307370949403915\n",
      "it 1, train loss = 1.373263e+00\n",
      "0.6185301727481861 0.007072482667337672 0.1730551975579218\n",
      "it 1, train loss = 1.447183e+00\n",
      "0.6805687832235328 0.010983170303980204 0.13190569945394057\n",
      "it 1, train loss = 1.490829e+00\n",
      "0.33266138436972864 0.007513022741484596 0.17610006595228\n",
      "it 1, train loss = 1.281963e+00\n",
      "0.6839439049089029 0.01098029495144178 0.13103568864360995\n",
      "it 1, train loss = 1.427043e+00\n",
      "0.6818330714987109 0.01098200997896352 0.13159601681465438\n",
      "it 1, train loss = 1.490746e+00\n",
      "0.6421015629462371 0.00712212651748808 0.16606546157144728\n",
      "it 1, train loss = 1.357442e+00\n",
      "0.6836432055714962 0.010987175869554503 0.1310163063644188\n",
      "it 1, train loss = 1.510118e+00\n",
      "0.6797234891331877 0.010998675094008203 0.1321492172524197\n",
      "it 1, train loss = 1.466120e+00\n",
      "0.6577098028158299 0.007252377397535131 0.15688597016293274\n",
      "it 1, train loss = 1.370017e+00\n",
      "0.6830683844790792 0.010984805240394184 0.13120185173684076\n",
      "it 1, train loss = 1.339773e+00\n",
      "0.6812284307115687 0.010976147273097064 0.13183547038160318\n",
      "it 1, train loss = 1.344008e+00\n",
      "0.6832123384273988 0.010977668636244102 0.13127605591357142\n",
      "it 1, train loss = 1.218268e+00\n",
      "0.6351607779602212 0.00709639465332983 0.16912294678941947\n",
      "it 1, train loss = 1.590187e+00\n",
      "0.6794692331112501 0.01098331692493978 0.13219640084202844\n",
      "44 [1, 3, 1]\n",
      "it 1, train loss = 1.391726e+00\n",
      "0.6462844342879491 0.007127423170985705 0.1648490706648445\n",
      "it 1, train loss = 1.351633e+00\n",
      "0.6466204418800146 0.007122367357793741 0.1648039635842124\n",
      "it 1, train loss = 1.204738e+00\n",
      "0.6426624953016791 0.007105282015594036 0.16676303526693972\n",
      "it 1, train loss = 1.453569e+00\n",
      "0.6818763301172655 0.010983844484659337 0.1315372418731127\n",
      "it 1, train loss = 1.567328e+00\n",
      "0.6366760495811452 0.007108232423864136 0.1681310756559658\n",
      "it 1, train loss = 1.441796e+00\n",
      "0.6816728137576834 0.010980594044936709 0.13164244967198516\n",
      "it 1, train loss = 1.597820e+00\n",
      "0.6359670896158259 0.007096746734412273 0.16798786457279397\n",
      "it 1, train loss = 1.388065e+00\n",
      "0.6823318309973568 0.010981017034879522 0.13145582118704033\n",
      "it 1, train loss = 1.452720e+00\n",
      "0.6820900252868921 0.01098428628254512 0.13147232511347692\n",
      "it 1, train loss = 1.479030e+00\n",
      "0.6197015754926225 0.007087443516084695 0.1733603806041931\n",
      "it 1, train loss = 1.355822e+00\n",
      "0.6831774556291905 0.010983874892273107 0.13119330831758172\n",
      "it 1, train loss = 1.480337e+00\n",
      "0.6812515324838431 0.01098587853983972 0.1316774361846995\n",
      "it 1, train loss = 1.417655e+00\n",
      "0.6813138081495516 0.010979679641420156 0.13175207043515172\n",
      "it 1, train loss = 1.307931e+00\n",
      "0.6534372868092347 0.007148039421672324 0.16108717646506068\n",
      "it 1, train loss = 1.298647e+00\n",
      "0.617852661391582 0.007090244914996814 0.17437221828351834\n",
      "it 1, train loss = 1.471179e+00\n",
      "0.6077320275259049 0.006955645907060487 0.18855926081351207\n",
      "it 1, train loss = 1.382989e+00\n",
      "0.6823682703526582 0.010980068587902413 0.13147274079859048\n",
      "it 1, train loss = 1.392839e+00\n",
      "0.6432963741877481 0.007113991903644034 0.16620278170065683\n",
      "it 1, train loss = 1.419264e+00\n",
      "0.6829061233884164 0.011002641534102402 0.13125007323864726\n",
      "it 1, train loss = 1.405890e+00\n",
      "0.682721925264123 0.010982234735492858 0.13133403241939465\n",
      "it 1, train loss = 1.580440e+00\n",
      "0.6804566788973642 0.010988455812549284 0.13184995502246732\n",
      "it 1, train loss = 1.321252e+00\n",
      "0.3495859628367343 0.006334132697333159 0.19647956895102592\n",
      "it 1, train loss = 1.195542e+00\n",
      "0.6843135562802825 0.010975443780204352 0.13100687905305364\n",
      "it 1, train loss = 1.365701e+00\n",
      "0.6824342718285044 0.010979251705227441 0.13146098083483365\n",
      "it 1, train loss = 1.434081e+00\n",
      "0.6824574394797839 0.010985559122260598 0.13139969246973893\n",
      "it 1, train loss = 1.193485e+00\n",
      "0.6854924037657364 0.010976591607876098 0.13067393902015131\n",
      "it 1, train loss = 1.362410e+00\n",
      "0.669963133155016 0.008582462013009943 0.14460793474418646\n",
      "it 1, train loss = 1.488843e+00\n",
      "0.6822144004766677 0.01098402661192757 0.13144103154743497\n",
      "it 1, train loss = 1.388169e+00\n",
      "0.6829882055636793 0.010982934490685102 0.13125551031441215\n",
      "it 1, train loss = 1.307995e+00\n",
      "0.63883607753013 0.0071018333467732534 0.16766459763687638\n",
      "it 1, train loss = 1.359722e+00\n",
      "0.6826324541190464 0.010978555907050516 0.13141980985233612\n",
      "it 1, train loss = 1.521243e+00\n",
      "0.6369536338797996 0.007112580478556044 0.1680586154039133\n",
      "it 1, train loss = 1.629258e+00\n",
      "0.6516410108512214 0.007232148507598936 0.15892403301655467\n",
      "it 1, train loss = 1.423975e+00\n",
      "0.6427521719244292 0.007116199559041242 0.16631156069948233\n",
      "it 1, train loss = 1.456288e+00\n",
      "0.6607451374384956 0.007383959199436374 0.15462273211829322\n",
      "it 1, train loss = 1.328395e+00\n",
      "0.6467608549060938 0.007115108905629017 0.16477939751218765\n",
      "it 1, train loss = 1.273866e+00\n",
      "0.6360942534640432 0.007102732392852804 0.16884696067377952\n",
      "it 1, train loss = 1.350700e+00\n",
      "0.6832463466271272 0.010979800462169927 0.1312328472860379\n",
      "it 1, train loss = 1.322314e+00\n",
      "0.6439974994683552 0.007116681470576359 0.16613285347362908\n",
      "it 1, train loss = 1.382288e+00\n",
      "0.6807475588787707 0.01051069009158447 0.1335663112315816\n",
      "it 1, train loss = 1.545791e+00\n",
      "0.6486650651553524 0.007136420919543646 0.16277342582958626\n",
      "it 1, train loss = 1.328660e+00\n",
      "0.6831150847540648 0.010977896775031845 0.13129903520271782\n",
      "it 1, train loss = 1.452910e+00\n",
      "0.6412387606218313 0.007099413697066986 0.16637783707461434\n",
      "it 1, train loss = 1.419605e+00\n",
      "0.6817613190245772 0.010982382074075077 0.13161172910007043\n",
      "it 1, train loss = 1.458944e+00\n",
      "0.631970396057258 0.007091896509290041 0.1694828403128293\n",
      "it 1, train loss = 1.514228e+00\n",
      "0.622152218151902 0.0070925051514544556 0.17261902397403764\n",
      "it 1, train loss = 1.326878e+00\n",
      "0.6831767833800315 0.01098346846968096 0.13122692055155566\n",
      "it 1, train loss = 1.229646e+00\n",
      "0.6843631011966508 0.010975664878999317 0.13099778013993396\n",
      "it 1, train loss = 1.439365e+00\n",
      "0.6413264183049519 0.007102573415998991 0.1665931444484062\n",
      "it 1, train loss = 1.396500e+00\n",
      "0.6824724670094363 0.010980087140574975 0.13143641085054752\n",
      "56 [1, 4, 1]\n",
      "it 1, train loss = 1.451629e+00\n",
      "0.6366588663392452 0.007100865414497128 0.168265637748256\n",
      "it 1, train loss = 1.413082e+00\n",
      "0.6825830587595916 0.010862381300021135 0.13165097246199547\n",
      "it 1, train loss = 1.422189e+00\n",
      "0.6827170072483514 0.010980195669386843 0.1313616519004162\n",
      "it 1, train loss = 1.426405e+00\n",
      "0.6828535110155566 0.010985256691973484 0.13125672288860446\n",
      "it 1, train loss = 1.342187e+00\n",
      "0.6387060060410377 0.007104267488434451 0.16781241159842825\n",
      "it 1, train loss = 1.360964e+00\n",
      "0.6833206061176574 0.01098068277692615 0.131202318920194\n",
      "it 1, train loss = 1.403054e+00\n",
      "0.6528175344524257 0.0071539494054615824 0.16095404137419625\n",
      "it 1, train loss = 1.489691e+00\n",
      "0.4276310556657934 0.006405663264520401 0.19723559040084174\n",
      "it 1, train loss = 1.447233e+00\n",
      "0.6819065150953711 0.010985354436687394 0.13150987416027857\n",
      "it 1, train loss = 1.412919e+00\n",
      "0.6820778862886818 0.010979598033595692 0.13154903876691604\n",
      "it 1, train loss = 1.394548e+00\n",
      "0.683296306431664 0.010981043398056004 0.13120255854108606\n",
      "it 1, train loss = 1.472555e+00\n",
      "0.6827372134532014 0.010988108920354843 0.1312425614779275\n",
      "it 1, train loss = 1.422839e+00\n",
      "0.6817343077211081 0.010984117406273726 0.13157924626353135\n",
      "it 1, train loss = 1.410117e+00\n",
      "0.6386664854328234 0.007106448694484547 0.16780556241820355\n",
      "it 1, train loss = 1.491032e+00\n",
      "0.6473058384826871 0.00712423205716068 0.16379490950835265\n",
      "it 1, train loss = 1.431001e+00\n",
      "0.6356610392756481 0.00709853755421739 0.1685143447801509\n",
      "it 1, train loss = 1.335261e+00\n",
      "0.6833143269160223 0.010992606144122634 0.13119375004917916\n",
      "it 1, train loss = 1.490924e+00\n",
      "0.6447677523620857 0.00711816499072643 0.16532872186654604\n",
      "it 1, train loss = 1.432080e+00\n",
      "0.6395208640404274 0.007097690047015674 0.16708815120076048\n",
      "it 1, train loss = 1.394189e+00\n",
      "0.6823627965989558 0.011101536618875429 0.13115791691252557\n",
      "it 1, train loss = 1.372498e+00\n",
      "0.6557072540894828 0.007192842869721086 0.1587661584998852\n",
      "it 1, train loss = 1.479063e+00\n",
      "0.6341604528067508 0.007088986351538832 0.16879295829588084\n",
      "it 1, train loss = 1.275945e+00\n",
      "0.6834379298219929 0.010977719265689755 0.1312113696431527\n",
      "it 1, train loss = 1.465295e+00\n",
      "0.6816972544383978 0.010981591007756638 0.1316142854259407\n",
      "it 1, train loss = 1.398435e+00\n",
      "0.641238141028686 0.0071232472053203814 0.16635931357920994\n",
      "it 1, train loss = 1.406489e+00\n",
      "0.6547462216631112 0.007172327438747468 0.1595280174899809\n",
      "it 1, train loss = 1.347494e+00\n",
      "0.6375486889924794 0.007098306288143353 0.16802930876073116\n",
      "it 1, train loss = 1.436341e+00\n",
      "0.6827493485493795 0.010984396834984877 0.13129371170377357\n",
      "it 1, train loss = 1.433340e+00\n",
      "0.6818614710991758 0.01098728499470381 0.1314950100374185\n",
      "it 1, train loss = 1.374220e+00\n",
      "0.6829720295636436 0.01098323216163684 0.13126058857442996\n",
      "it 1, train loss = 1.375195e+00\n",
      "0.6829215263925503 0.010983989330927575 0.13127118236527452\n",
      "it 1, train loss = 1.500288e+00\n",
      "0.6813370060932621 0.010984388678016788 0.13167864389675402\n",
      "it 1, train loss = 1.405699e+00\n",
      "0.64701576353491 0.007124014537980782 0.16428029203048933\n",
      "it 1, train loss = 1.410571e+00\n",
      "0.6827534479561322 0.010982220246395641 0.131323425863482\n",
      "it 1, train loss = 1.402702e+00\n",
      "0.6420924918062755 0.0071119754541313805 0.16650848115697325\n",
      "it 1, train loss = 1.396632e+00\n",
      "0.6480605070338696 0.007125852631084237 0.16396407130841822\n",
      "it 1, train loss = 1.433073e+00\n",
      "0.6482408117679407 0.007126521327835026 0.16371720787597294\n",
      "it 1, train loss = 1.464862e+00\n",
      "0.6476961150539067 0.007122871837503914 0.16370968444887982\n",
      "it 1, train loss = 1.422138e+00\n",
      "0.6383271053318211 0.0071028466093027605 0.16779660822174236\n",
      "it 1, train loss = 1.302654e+00\n",
      "0.6449765747592303 0.007111900940592596 0.16580102183470505\n",
      "it 1, train loss = 1.275210e+00\n",
      "0.6385790873739566 0.007097518671029311 0.1679103115640026\n",
      "it 1, train loss = 1.495265e+00\n",
      "0.6821475489797858 0.010987439611445314 0.13141148553300505\n",
      "it 1, train loss = 1.565668e+00\n",
      "0.6386159294421008 0.007108290336510777 0.16733623309678242\n",
      "it 1, train loss = 1.368631e+00\n",
      "0.6837159873141894 0.010983030977806587 0.13105353736858166\n",
      "it 1, train loss = 1.451034e+00\n",
      "0.6817216514067309 0.010973850283389935 0.13156155783646042\n",
      "it 1, train loss = 1.498183e+00\n",
      "0.62790781710692 0.007094046936847184 0.1707931159849759\n",
      "it 1, train loss = 1.353382e+00\n",
      "0.6429332749652947 0.00711199118944814 0.16638379670482836\n",
      "it 1, train loss = 1.425151e+00\n",
      "0.6251743537090911 0.006457885877724992 0.18252472303864778\n",
      "it 1, train loss = 1.385311e+00\n",
      "0.6827850970691464 0.010983126816263204 0.13130904574472377\n",
      "it 1, train loss = 1.489212e+00\n",
      "0.6819087753229545 0.010986371943499728 0.13149015105221232\n",
      "64 [1, 2, 2, 1]\n",
      "it 1, train loss = 1.409183e+00\n",
      "0.6854213679519142 0.010937023750843308 0.13165268780036157\n",
      "it 1, train loss = 1.433243e+00\n",
      "0.6336589643631128 0.007120946929190748 0.17516531889486048\n",
      "it 1, train loss = 1.386913e+00\n",
      "0.6385630486867861 0.007220559782439554 0.17594561010279947\n",
      "it 1, train loss = 1.527639e+00\n",
      "0.6812761190974606 0.01095809520054408 0.13207332327391033\n",
      "it 1, train loss = 1.392678e+00\n",
      "0.6815305997581933 0.011009060214169072 0.1312725573097916\n",
      "it 1, train loss = 1.438092e+00\n",
      "0.6823708245592083 0.010956514880116775 0.13182031513691686\n",
      "it 1, train loss = 1.464682e+00\n",
      "0.6411535595682252 0.007161496502984386 0.1643963275850267\n",
      "it 1, train loss = 1.492343e+00\n",
      "0.6265564441003073 0.007038783966062975 0.17394759034325316\n",
      "it 1, train loss = 1.421225e+00\n",
      "0.639710936345123 0.00704176931084146 0.1718300146377701\n",
      "it 1, train loss = 1.359603e+00\n",
      "0.6780246226848726 0.01085184396012871 0.13303346566666674\n",
      "it 1, train loss = 1.284370e+00\n",
      "0.6834487491424555 0.010946083450111891 0.13183309559891115\n",
      "it 1, train loss = 1.328156e+00\n",
      "0.6844215604979696 0.01099361790993869 0.13108883811117006\n",
      "it 1, train loss = 1.416179e+00\n",
      "0.5353864375974379 0.006967067208702201 0.20119293750649986\n",
      "it 1, train loss = 1.522596e+00\n",
      "0.6659419112906082 0.01107773855413979 0.13450490291488904\n",
      "it 1, train loss = 1.462512e+00\n",
      "0.6681310135447639 0.01102620088365532 0.1346089518178905\n",
      "it 1, train loss = 1.446578e+00\n",
      "0.3256904401632245 0.006938944920519264 0.1787851568010344\n",
      "it 1, train loss = 1.492168e+00\n",
      "0.6395137788023524 0.007082933731752973 0.17288674811993682\n",
      "it 1, train loss = 1.466336e+00\n",
      "0.3192375282093723 0.007877210331511664 0.16434669095163298\n",
      "it 1, train loss = 1.632263e+00\n",
      "0.6343113334902948 0.007048625544389598 0.17926298978966537\n",
      "it 1, train loss = 1.407454e+00\n",
      "0.5497416738656266 0.006963830867511547 0.19796976987644818\n",
      "it 1, train loss = 1.190262e+00\n",
      "0.6729455489431413 0.01100116594514359 0.13370490653063233\n",
      "it 1, train loss = 1.450813e+00\n",
      "0.6840477137121556 0.01093696060861407 0.13199379210757717\n",
      "it 1, train loss = 1.376054e+00\n",
      "0.6349830703239734 0.0068219043458168155 0.17948836370889762\n",
      "it 1, train loss = 1.330407e+00\n",
      "0.6393479025430103 0.006995294614310552 0.16943090534623456\n",
      "it 1, train loss = 1.597476e+00\n",
      "0.6233892092099328 0.007150674511527706 0.17914152986520213\n",
      "it 1, train loss = 1.396362e+00\n",
      "0.6055835506879571 0.007047026640776613 0.18295866813768372\n",
      "it 1, train loss = 1.518553e+00\n",
      "0.6714037811815255 0.011051166607572731 0.13342639812010834\n",
      "it 1, train loss = 1.425392e+00\n",
      "0.31660083588434496 0.008133088034687389 0.16489903679465387\n",
      "it 1, train loss = 1.389330e+00\n",
      "0.6736529838951316 0.01101312229403918 0.1333112709102982\n",
      "it 1, train loss = 1.516155e+00\n",
      "0.6838488692626424 0.010990575769617499 0.13065576488313865\n",
      "it 1, train loss = 1.309045e+00\n",
      "0.3172415902244202 0.007895763672667836 0.16435519000237248\n",
      "it 1, train loss = 1.538237e+00\n",
      "0.5346710397083995 0.007014582093787905 0.1951967334071217\n",
      "it 1, train loss = 1.359624e+00\n",
      "0.6163426995775098 0.007029560952505829 0.17711500077434236\n",
      "it 1, train loss = 1.477858e+00\n",
      "0.6797753550078304 0.01097720429003698 0.13218790444789738\n",
      "it 1, train loss = 1.334228e+00\n",
      "0.6841105391973001 0.010978620754897124 0.13098830989645355\n",
      "it 1, train loss = 1.313207e+00\n",
      "0.6809374087247523 0.011011144333981912 0.13139218749455253\n",
      "it 1, train loss = 1.422231e+00\n",
      "0.6758709886825633 0.011053678406892547 0.13219643899199443\n",
      "it 1, train loss = 1.335147e+00\n",
      "0.6774725462982203 0.010985089232186314 0.13270200425385367\n",
      "it 1, train loss = 1.194187e+00\n",
      "0.6886460568386968 0.01127458283245403 0.12893835569518394\n",
      "it 1, train loss = 1.435706e+00\n",
      "0.6710913274997006 0.011043927062330565 0.13356853648223116\n",
      "it 1, train loss = 1.342005e+00\n",
      "0.6838790561248304 0.010953021054057266 0.13145553126545642\n",
      "it 1, train loss = 1.275223e+00\n",
      "0.3170567369410308 0.007844215793381211 0.1642426442405936\n",
      "it 1, train loss = 1.281262e+00\n",
      "0.6368902301605339 0.007162157237093715 0.1753696258968392\n",
      "it 1, train loss = 1.358304e+00\n",
      "0.6808413172804179 0.010970372247596519 0.13200224087274776\n",
      "it 1, train loss = 1.507932e+00\n",
      "0.6793437416972714 0.010958305356458906 0.13260199862429997\n",
      "it 1, train loss = 1.409997e+00\n",
      "0.6837165007299134 0.010977561921455329 0.13114353744423654\n",
      "it 1, train loss = 1.436791e+00\n",
      "0.6750099046531808 0.010995098501115052 0.13321236459852032\n",
      "it 1, train loss = 1.581601e+00\n",
      "0.6812924228625041 0.011030560554266213 0.131911171361648\n",
      "it 1, train loss = 1.398115e+00\n",
      "0.6188480719681076 0.007103735869452673 0.17277981151536353\n",
      "it 1, train loss = 1.397517e+00\n",
      "0.6038117998161407 0.0070256871572105094 0.18235526386046866\n",
      "80 [1 3 2 1]\n",
      "it 1, train loss = 1.366772e+00\n",
      "0.6754227028375182 0.011028235290935932 0.1326130518994742\n",
      "it 1, train loss = 1.391897e+00\n",
      "0.6731768890555339 0.010999839598622304 0.1336474472458931\n",
      "it 1, train loss = 1.514643e+00\n",
      "0.6104216365126995 0.007052794931372737 0.18012395080280635\n",
      "it 1, train loss = 1.370012e+00\n",
      "0.6724754329741812 0.011010294630460545 0.13365691953969086\n",
      "it 1, train loss = 1.357089e+00\n",
      "0.6774588916348975 0.011055124260350338 0.1317716990075669\n",
      "it 1, train loss = 1.339267e+00\n",
      "0.6748358788992967 0.010980042610829332 0.1334964738052173\n",
      "it 1, train loss = 1.403639e+00\n",
      "0.6503792461113689 0.007172811503116361 0.16453099928988285\n",
      "it 1, train loss = 1.452206e+00\n",
      "0.5972765479212772 0.0070546524925729865 0.18328530473626983\n",
      "it 1, train loss = 1.491298e+00\n",
      "0.6803277717782334 0.010689463027777556 0.13407640018203548\n",
      "it 1, train loss = 1.447919e+00\n",
      "0.6801657184049801 0.011001622961669719 0.1317194464005655\n",
      "it 1, train loss = 1.602693e+00\n",
      "0.5959872987098265 0.006520601448669272 0.1908011755005164\n",
      "it 1, train loss = 1.350155e+00\n",
      "0.6740818528448294 0.011085036911811967 0.1336408703618874\n",
      "it 1, train loss = 1.413745e+00\n",
      "0.6260890459545508 0.0067948704437965045 0.18158305125358062\n",
      "it 1, train loss = 1.238235e+00\n",
      "0.6786501333229034 0.011017520125868662 0.13230820518525793\n",
      "it 1, train loss = 1.647591e+00\n",
      "0.6157749341187257 0.0070274744008611565 0.1775697930838267\n",
      "it 1, train loss = 1.379157e+00\n",
      "0.5997191182115534 0.00702939185850481 0.18084064245688566\n",
      "it 1, train loss = 1.398052e+00\n",
      "0.6783697213773463 0.01100517672876521 0.13217710376170885\n",
      "it 1, train loss = 1.299352e+00\n",
      "0.6723201672756572 0.010869270162244761 0.1338844664196889\n",
      "it 1, train loss = 1.347914e+00\n",
      "0.6752034100800363 0.011195998035140134 0.13232420626434407\n",
      "it 1, train loss = 1.498457e+00\n",
      "0.5667936706291187 0.007016170205225489 0.19174444375691518\n",
      "it 1, train loss = 1.530823e+00\n",
      "0.6744662957288241 0.011042353550651826 0.13269675219386218\n",
      "it 1, train loss = 1.554563e+00\n",
      "0.6684553885668314 0.011029179529032367 0.13449956303839053\n",
      "it 1, train loss = 1.462160e+00\n",
      "0.6763756036009698 0.01099951261126266 0.1327716808411316\n",
      "it 1, train loss = 1.425514e+00\n",
      "0.6746859743579852 0.010959464351441506 0.13387530187846877\n",
      "it 1, train loss = 1.382418e+00\n",
      "0.5145031326560084 0.006862604007732103 0.20898400566788128\n",
      "it 1, train loss = 1.397433e+00\n",
      "0.3284484233305212 0.0070712499030877955 0.18807812262947524\n",
      "it 1, train loss = 1.251127e+00\n",
      "0.6751726357602789 0.011006725323806135 0.1329925440704862\n",
      "it 1, train loss = 1.426564e+00\n",
      "0.6377866449279768 0.007021147580250546 0.17103608235247636\n",
      "it 1, train loss = 1.390884e+00\n",
      "0.6842285424067275 0.010974980401785102 0.13090084582623135\n",
      "it 1, train loss = 1.374275e+00\n",
      "0.6069942636646326 0.007017602602162775 0.1796820236034515\n",
      "it 1, train loss = 1.427151e+00\n",
      "0.6767008678324885 0.01096542888138224 0.13320545880224824\n",
      "it 1, train loss = 1.373141e+00\n",
      "0.344415884918267 0.00615341886248865 0.20427249658477478\n",
      "it 1, train loss = 1.566337e+00\n",
      "0.6133900328928028 0.007021450972278708 0.1783440481209731\n",
      "it 1, train loss = 1.496475e+00\n",
      "0.6439379927698092 0.007167783411419257 0.16283031505771792\n",
      "it 1, train loss = 1.403924e+00\n",
      "0.675521237732458 0.011005882903889676 0.13291426598463177\n",
      "it 1, train loss = 1.502632e+00\n",
      "0.6333908676934543 0.007046577882070627 0.17179404988514962\n",
      "it 1, train loss = 1.368611e+00\n",
      "0.694206177290902 0.013190676837424704 0.12228243843687599\n",
      "it 1, train loss = 1.254406e+00\n",
      "0.6296381169273525 0.007093039264589376 0.1742253918351284\n",
      "it 1, train loss = 1.341302e+00\n",
      "0.6202337627624782 0.006626167293929264 0.18359188466070803\n",
      "it 1, train loss = 1.733291e+00\n",
      "0.6695418056114886 0.010981559871838197 0.13490363266294822\n",
      "it 1, train loss = 1.480415e+00\n",
      "0.3169711026733229 0.007042488164824569 0.18911546793849793\n",
      "it 1, train loss = 1.468329e+00\n",
      "0.6231731177329232 0.007033284002008714 0.17563614843621947\n",
      "it 1, train loss = 1.493895e+00\n",
      "0.6758225615062022 0.01099392204659065 0.13312467030200653\n",
      "it 1, train loss = 1.485083e+00\n",
      "0.6741297636444379 0.01101795844470567 0.13311260912038902\n",
      "it 1, train loss = 1.322911e+00\n",
      "0.677350534372654 0.011104340898276874 0.1322356497030539\n",
      "it 1, train loss = 1.464478e+00\n",
      "0.6775440361261562 0.011552515457739714 0.1315420643010287\n",
      "it 1, train loss = 1.544929e+00\n",
      "0.6382935540814206 0.006875437078435573 0.16643643534554967\n",
      "it 1, train loss = 1.531947e+00\n",
      "0.6810885364078649 0.010939169164452952 0.1327341016715694\n",
      "it 1, train loss = 1.391248e+00\n",
      "0.6803852500246971 0.010998300845330055 0.13170818055811026\n",
      "it 1, train loss = 1.414290e+00\n",
      "0.6771866867012757 0.01103075886893922 0.13216803027669263\n",
      "84 [1 2 3 1]\n",
      "it 1, train loss = 1.423915e+00\n",
      "0.6771357629814411 0.01100684019483206 0.1324552997814046\n",
      "it 1, train loss = 1.548306e+00\n",
      "0.6751670194125626 0.01104186189655052 0.1325076102782338\n",
      "it 1, train loss = 1.555324e+00\n",
      "0.6299575314602489 0.00715066121597819 0.16633194476207544\n",
      "it 1, train loss = 1.264397e+00\n",
      "0.6727005067114518 0.01101947795442848 0.13348509817173754\n",
      "it 1, train loss = 1.533181e+00\n",
      "0.4021171176930821 0.00691315112789993 0.2528175550982293\n",
      "it 1, train loss = 1.362085e+00\n",
      "0.6455208461648602 0.007203516504563794 0.1615856659064082\n",
      "it 1, train loss = 1.280231e+00\n",
      "0.6447365936545324 0.0068799173145548745 0.16553829186535923\n",
      "it 1, train loss = 1.243209e+00\n",
      "0.34402022808976057 0.006152336932403083 0.20338710073768443\n",
      "it 1, train loss = 1.427768e+00\n",
      "0.678984337519772 0.011045220186843618 0.13149817182953388\n",
      "it 1, train loss = 1.402858e+00\n",
      "0.6769089170328102 0.010892225741454493 0.13268869503956882\n",
      "it 1, train loss = 1.501399e+00\n",
      "0.6776136494060097 0.01107969502655936 0.13147954205549164\n",
      "it 1, train loss = 1.369515e+00\n",
      "0.6702080253444649 0.011053459830633203 0.13366811278074767\n",
      "it 1, train loss = 1.429400e+00\n",
      "0.6801542141156266 0.010994654720246624 0.13182271221593186\n",
      "it 1, train loss = 1.358277e+00\n",
      "0.6792982353883664 0.011032262932150691 0.131578662632638\n",
      "it 1, train loss = 1.485221e+00\n",
      "0.6724783741694 0.011052790485496736 0.13308689697189177\n",
      "it 1, train loss = 1.356463e+00\n",
      "0.6779953494612699 0.011015406515135204 0.13211017079644513\n",
      "it 1, train loss = 1.303108e+00\n",
      "0.6089879461513197 0.007033449100469061 0.18066307495322828\n",
      "it 1, train loss = 1.355365e+00\n",
      "0.44109781016394534 0.006938106191609851 0.2183016322922895\n",
      "it 1, train loss = 1.236146e+00\n",
      "0.6781702475083778 0.011032605352538034 0.13188105588622243\n",
      "it 1, train loss = 1.462508e+00\n",
      "0.6775769045296187 0.011061838687135355 0.13169898426728974\n",
      "it 1, train loss = 1.405804e+00\n",
      "0.6751187573007904 0.011038700388614056 0.13256375485055974\n",
      "it 1, train loss = 1.400139e+00\n",
      "0.6812015370071102 0.01100597682976299 0.13139420869075327\n",
      "it 1, train loss = 1.417349e+00\n",
      "0.676068200328889 0.011007763934866733 0.1327518157311168\n",
      "it 1, train loss = 1.446195e+00\n",
      "0.616862555078046 0.007070258559866613 0.17976558655772296\n",
      "it 1, train loss = 1.418268e+00\n",
      "0.6492412204255303 0.007168466662408927 0.16117495282201175\n",
      "it 1, train loss = 1.412897e+00\n",
      "0.6754917053254746 0.011042065691788172 0.13243527386534926\n",
      "it 1, train loss = 1.379868e+00\n",
      "0.6755446208756704 0.011055164272097037 0.13226850463065543\n",
      "it 1, train loss = 1.327393e+00\n",
      "0.6799778414895462 0.010985808545990343 0.1320071832947102\n",
      "it 1, train loss = 1.421918e+00\n",
      "0.6741655747078481 0.011002852801978465 0.13333640506720637\n",
      "it 1, train loss = 1.384700e+00\n",
      "0.6812925108959501 0.011064531652529588 0.1306965242925451\n",
      "it 1, train loss = 1.571521e+00\n",
      "0.6811933959939762 0.011044879219676118 0.1310117699042076\n",
      "it 1, train loss = 1.474061e+00\n",
      "0.6203992686234115 0.007140012063886506 0.17224433471359607\n",
      "it 1, train loss = 1.562502e+00\n",
      "0.3993068357292772 0.006716991550381144 0.24370794492867645\n",
      "it 1, train loss = 1.467083e+00\n",
      "0.33103630189167754 0.005987117962239192 0.20056183332177596\n",
      "it 1, train loss = 1.422968e+00\n",
      "0.6758263876019787 0.011043270228268075 0.13233934538329675\n",
      "it 1, train loss = 1.285700e+00\n",
      "0.6232939529904976 0.007117043300100133 0.16951832073813097\n",
      "it 1, train loss = 1.428719e+00\n",
      "0.6735104012549664 0.011017755725504626 0.13328733521723182\n",
      "it 1, train loss = 1.405014e+00\n",
      "0.680134787839241 0.011028153803491425 0.1313899352717778\n",
      "it 1, train loss = 1.521715e+00\n",
      "0.6693656765707349 0.011060054934726149 0.1338162251839514\n",
      "it 1, train loss = 1.355590e+00\n",
      "0.6778500354876337 0.011019399232187923 0.13207972125338116\n",
      "it 1, train loss = 1.516249e+00\n",
      "0.6530346919837023 0.007241829984442674 0.15852654554388343\n",
      "it 1, train loss = 1.439482e+00\n",
      "0.6809037297823156 0.011031681620002228 0.13115839834879117\n",
      "it 1, train loss = 1.474868e+00\n",
      "0.6704433336683888 0.01102943671673825 0.1339453968528203\n",
      "it 1, train loss = 1.342299e+00\n",
      "0.640054787350468 0.007151365208029292 0.16447947763359227\n",
      "it 1, train loss = 1.349327e+00\n",
      "0.678737432364321 0.01105436933822493 0.13146063052010482\n",
      "it 1, train loss = 1.430984e+00\n",
      "0.31684406962200873 0.008049347850791153 0.16477542930159852\n",
      "it 1, train loss = 1.487272e+00\n",
      "0.5661291707489281 0.007044159978469086 0.19065006647341276\n",
      "it 1, train loss = 1.259299e+00\n",
      "0.6615969005549768 0.007232281668357346 0.15701883711468592\n",
      "it 1, train loss = 1.508637e+00\n",
      "0.6753258973449998 0.011016403559955623 0.1327851093315821\n",
      "it 1, train loss = 1.525209e+00\n",
      "0.673312154524489 0.01105055197408913 0.13290634263015755\n",
      "104 [1, 3, 3, 1]\n",
      "it 1, train loss = 1.312144e+00\n",
      "0.6782400229340919 0.010994707149387547 0.13234540609051829\n",
      "it 1, train loss = 1.401629e+00\n",
      "0.6770947753011574 0.010986912190754186 0.1327731591993309\n",
      "it 1, train loss = 1.405553e+00\n",
      "0.3314775084598074 0.007064077063696028 0.18722661868794935\n",
      "it 1, train loss = 1.526947e+00\n",
      "0.6267979484806847 0.0061858125843108555 0.18295611320675478\n",
      "it 1, train loss = 1.649739e+00\n",
      "0.6115404023846743 0.007085825543937353 0.18156275395356497\n",
      "it 1, train loss = 1.520236e+00\n",
      "0.6701882170532231 0.011049368819911535 0.1337359246884358\n",
      "it 1, train loss = 1.372600e+00\n",
      "0.6739391786834897 0.010930783743407158 0.13315160514059848\n",
      "it 1, train loss = 1.456697e+00\n",
      "0.6777475975890167 0.011013659496155822 0.13221727174523581\n",
      "it 1, train loss = 1.444493e+00\n",
      "0.6767967365876834 0.011026838360368055 0.13229901481423953\n",
      "it 1, train loss = 1.496854e+00\n",
      "0.6726763233753499 0.011019460281632237 0.13349074054830237\n",
      "it 1, train loss = 1.506480e+00\n",
      "0.6737856914989295 0.01101895196282813 0.13343844801577578\n",
      "it 1, train loss = 1.374718e+00\n",
      "0.4314709212117191 0.007083665432725953 0.18635817880881886\n",
      "it 1, train loss = 1.592860e+00\n",
      "0.6149517827313304 0.006373902881652391 0.18582863218208182\n",
      "it 1, train loss = 1.539975e+00\n",
      "0.6754198215741782 0.01096221369824606 0.13258836188216158\n",
      "it 1, train loss = 1.262536e+00\n",
      "0.6752153870932702 0.011051642022502035 0.1324015789671206\n",
      "it 1, train loss = 1.456445e+00\n",
      "0.6725743606163984 0.01095468876111975 0.13392923104637708\n",
      "it 1, train loss = 1.490579e+00\n",
      "0.6791904513321626 0.010996316614449947 0.1316475918723425\n",
      "it 1, train loss = 1.497946e+00\n",
      "0.666765215331475 0.011072787038554395 0.1344814863234424\n",
      "it 1, train loss = 1.404626e+00\n",
      "0.31665546106325715 0.008165841929801198 0.16488129793200468\n",
      "it 1, train loss = 1.433855e+00\n",
      "0.6730125716333452 0.011026741782183451 0.13329515188720056\n",
      "it 1, train loss = 1.457490e+00\n",
      "0.6820217218690072 0.010957491985037756 0.13187946568954548\n",
      "it 1, train loss = 1.382562e+00\n",
      "0.6181691824820471 0.00700853371805906 0.17602291375603832\n",
      "it 1, train loss = 1.354943e+00\n",
      "0.608443474854926 0.007050783845461814 0.18158918735296378\n",
      "it 1, train loss = 1.398439e+00\n",
      "0.6795483575659625 0.011032464219486669 0.13149245266249432\n",
      "it 1, train loss = 1.454414e+00\n",
      "0.6728666084848973 0.011075565037866535 0.132731562374876\n",
      "it 1, train loss = 1.331115e+00\n",
      "0.3156813532856417 0.008212891258419308 0.16477761794767817\n",
      "it 1, train loss = 1.431753e+00\n",
      "0.674970222854724 0.011042929664728154 0.1325544617676468\n",
      "it 1, train loss = 1.387820e+00\n",
      "0.6732164211694347 0.01102792699888365 0.13322010472490545\n",
      "it 1, train loss = 1.372896e+00\n",
      "0.6782232664112935 0.010995654882266474 0.13233596489623076\n",
      "it 1, train loss = 1.515628e+00\n",
      "0.670410577797166 0.011046640073173452 0.1341192882856715\n",
      "it 1, train loss = 1.489291e+00\n",
      "0.6468265882447056 0.007202573865550103 0.1609465444362426\n",
      "it 1, train loss = 1.481393e+00\n",
      "0.599170918509957 0.007033234675156562 0.18181349837814847\n",
      "it 1, train loss = 1.347784e+00\n",
      "0.6747243259398368 0.011002770275651464 0.1328935125657425\n",
      "it 1, train loss = 1.399091e+00\n",
      "0.6793930965638117 0.01096299201273729 0.13245860468894263\n",
      "it 1, train loss = 1.307728e+00\n",
      "0.6239592835720891 0.007030526891831895 0.175278167329381\n",
      "it 1, train loss = 1.414424e+00\n",
      "0.6059050633772395 0.006413161734659699 0.1885303324835199\n",
      "it 1, train loss = 1.580472e+00\n",
      "0.6744931538239223 0.011196976487098779 0.1328210295822831\n",
      "it 1, train loss = 1.341188e+00\n",
      "0.6361548270482332 0.007147991804144225 0.16624819269373198\n",
      "it 1, train loss = 1.503329e+00\n",
      "0.5874052275134872 0.0070264821939606875 0.18680876028254176\n",
      "it 1, train loss = 1.495306e+00\n",
      "0.4179836651074864 0.006800454906104537 0.2321489559635517\n",
      "it 1, train loss = 1.504360e+00\n",
      "0.5868809424774004 0.007030967089797374 0.1864454479971122\n",
      "it 1, train loss = 1.410589e+00\n",
      "0.6205424022162032 0.007034389907671243 0.17580180401086462\n",
      "it 1, train loss = 1.433077e+00\n",
      "0.6127423025572329 0.007041244739997431 0.17857360950987897\n",
      "it 1, train loss = 1.421742e+00\n",
      "0.6729689416170621 0.011050902508795327 0.13298478151060195\n",
      "it 1, train loss = 1.375890e+00\n",
      "0.6753484945472938 0.010986484027871238 0.13325308651186912\n",
      "it 1, train loss = 1.509711e+00\n",
      "0.5361996500900512 0.0069413835886542215 0.2021631834608064\n",
      "it 1, train loss = 1.464329e+00\n",
      "0.6728453627413116 0.011039355678505905 0.13316927079504895\n",
      "it 1, train loss = 1.308717e+00\n",
      "0.6761749328765749 0.01101471175529946 0.1326043527937534\n",
      "it 1, train loss = 1.574234e+00\n",
      "0.6739372096542331 0.011032825078679823 0.132963405249455\n",
      "it 1, train loss = 1.519768e+00\n",
      "0.6723556197569656 0.011014303780338835 0.133636749824989\n"
     ]
    }
   ],
   "source": [
    "# Next: find the standard deviation of R2 from 50 runs for each case.\n",
    "# Takes more than 3 hours to finish\n",
    "lamUT_vec = np.array(UTdata['F11'])\n",
    "lamET_vec = np.array(ETdata['F11'])\n",
    "lamPS_vec = np.array(PSdata['F11'])\n",
    "np_list = []\n",
    "r2_list_global = []\n",
    "mae_list_global = []\n",
    "for n in range(10,50):\n",
    "    n_params, layers = opt_arch_icnn(n)\n",
    "    if n_params in np_list:\n",
    "        continue\n",
    "    print(n_params, layers)\n",
    "    np_list.append(n_params)\n",
    "\n",
    "    r2_list_arch = []\n",
    "    mae_list_arch = []\n",
    "    for i in range(50): #50 runs for every architecture\n",
    "        key, subkey = random.split(key)\n",
    "        params = init_icnn(key, layers)\n",
    "\n",
    "        opt_init, opt_update, get_params = optimizers.adam(2.e-4) #Original: 2.e-4\n",
    "        opt_state = opt_init(params)\n",
    "        params, train_loss, val_loss = train_jp(loss_P11_ET, 2, F11_data, opt_state, key, nIter = 100000, print_freq=200000)\n",
    "\n",
    "        model = ICNN_model(params[0], params[1], normalization)\n",
    "\n",
    "        lambdas     = [lamUT_vec,       lamET_vec,      lamPS_vec       ]\n",
    "        P11_gt_list = [UTdata['P11'],   ETdata['P11'],  PSdata['P11']   ]\n",
    "        P11funs     = [P11_UT,          P11_ET,         P11_PS          ]\n",
    "\n",
    "        r2 = []\n",
    "        mae = []\n",
    "        for lam, P11_gt, P11fun in zip(lambdas, P11_gt_list, P11funs):\n",
    "            P11 = P11fun(lam, model, normalization)\n",
    "            r2i = r2_score(P11_gt, P11)\n",
    "            r2i = np.clip(r2i, a_min=0.0, a_max=1.0)\n",
    "            r2.append(r2i)\n",
    "\n",
    "            maei = onp.mean(onp.abs(P11_gt-P11))\n",
    "            mae.append(maei)\n",
    "        # r2 = np.mean(np.array(r2))\n",
    "        # print(*r2)\n",
    "        print(*mae)\n",
    "        r2_list_arch.append(r2)\n",
    "        mae_list_arch.append(mae)\n",
    "    r2_list_global.append(r2_list_arch)\n",
    "    mae_list_global.append(mae_list_arch)\n",
    "\n",
    "icnn_r2 = np.array(r2_list_global)\n",
    "icnn_mae = np.array(mae_list_global)\n",
    "with open('savednet/ICNN_r2_efficiency_ET.npy', 'wb') as f:\n",
    "    pickle.dump([np_list, icnn_r2], f)\n",
    "\n",
    "with open('savednet/ICNN_mae_efficiency_ET.npy', 'wb') as f:\n",
    "    pickle.dump([np_list, icnn_mae], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 1, train loss = 8.187440e-01\n",
      "2141466321.3764899 5.706807551596808 0.017627599466425524\n",
      "it 1, train loss = 8.249277e-01\n",
      "247376.72751749418 2.2401459764934235 0.01765376658779182\n",
      "it 1, train loss = 9.192630e-01\n",
      "247379.90354958683 2.24021510856873 0.017653758097655386\n",
      "it 1, train loss = 9.438593e-01\n",
      "2141446071.5869079 5.706798810637099 0.01762759625890641\n",
      "it 1, train loss = 9.324953e-01\n",
      "2141438992.4896064 5.706781321667573 0.017627598134820014\n",
      "it 1, train loss = 9.324569e-01\n",
      "2141456200.7710176 5.706787845986882 0.017627596914539852\n",
      "it 1, train loss = 7.723456e-01\n",
      "2141455168.1247995 5.708910320846771 0.01762759689605753\n",
      "it 1, train loss = 8.441127e-01\n",
      "2141465421.170485 5.706841900048218 0.01762759793326956\n",
      "it 1, train loss = 8.644549e-01\n",
      "2141453938.8915308 5.706801253576282 0.017627596446478173\n",
      "it 1, train loss = 8.520307e-01\n",
      "2141453544.825723 5.7067961446665905 0.017627596789755628\n",
      "it 1, train loss = 8.734942e-01\n",
      "2141456856.9572191 5.707928303942827 0.017627596948218027\n",
      "it 1, train loss = 9.200208e-01\n",
      "2141477387.089688 5.70684621721471 0.01762762315606167\n",
      "it 1, train loss = 9.093380e-01\n",
      "2141459951.470655 5.7067922147386065 0.01762759695799715\n",
      "it 1, train loss = 8.868391e-01\n",
      "2140596060.171913 5.705763748912118 0.017627284387813417\n",
      "it 1, train loss = 7.199315e-01\n",
      "2140019571.0994577 5.705022780623522 0.01762700508253607\n",
      "it 1, train loss = 8.473314e-01\n",
      "2141461170.5892115 5.706811083316493 0.017627596899087817\n",
      "it 1, train loss = 8.440545e-01\n",
      "2142757904.2742815 5.708430671281746 0.01762812450854726\n",
      "it 1, train loss = 8.593234e-01\n",
      "2140735218.0829155 5.705952476748492 0.017627313335529356\n",
      "it 1, train loss = 3.065065e-01\n",
      "247379.5483498343 2.240108103768895 0.017653758118904184\n",
      "it 1, train loss = 5.010959e-01\n",
      "2141454711.6430116 5.706787470545755 0.017627596877369648\n",
      "it 1, train loss = 8.710759e-01\n",
      "247379.37480824455 2.240134190944952 0.017653757414425264\n",
      "it 1, train loss = 8.933754e-01\n",
      "2140761570.6168842 5.705957866570138 0.01762734500715073\n",
      "it 1, train loss = 9.200504e-01\n",
      "2141426794.1019583 5.706809631810003 0.017627592276186474\n",
      "it 1, train loss = 8.491545e-01\n",
      "2140493558.4693975 5.705603697641539 0.017627219489155087\n",
      "it 1, train loss = 8.711191e-01\n",
      "2144826880.3032916 5.711052406592404 0.017629003790471543\n",
      "it 1, train loss = 9.248295e-01\n",
      "2141445229.7562635 5.70678058948913 0.017627596291361094\n",
      "it 1, train loss = 7.503250e-01\n",
      "2141452215.4008114 5.707706442239303 0.01762759571104944\n",
      "it 1, train loss = 8.631698e-01\n",
      "2141454916.9912338 5.706776580150116 0.017627596895653228\n",
      "it 1, train loss = 9.527500e-01\n",
      "2142250323.8024476 5.707890182428829 0.01762795066848757\n",
      "it 1, train loss = 9.490997e-01\n",
      "2141459216.7791843 5.706785473936141 0.017627596955067024\n",
      "it 1, train loss = 7.950568e-01\n",
      "2141447096.799893 5.706771029165489 0.017627596498838896\n",
      "it 1, train loss = 7.609068e-01\n",
      "2141454814.9227624 5.7068424796787385 0.01762759685117384\n",
      "it 1, train loss = 7.498655e-01\n",
      "2141445572.6394296 5.7067822714572305 0.01762759628637974\n",
      "it 1, train loss = 7.969326e-01\n",
      "2141454478.9516687 5.706790990882674 0.017627596782861858\n",
      "it 1, train loss = 7.033780e-01\n",
      "2141456892.1713793 5.707240437577059 0.017627596922091236\n",
      "it 1, train loss = 8.141821e-01\n",
      "2141452355.0510602 5.706784759241576 0.01762759669625667\n",
      "it 1, train loss = 9.323364e-01\n",
      "2141375350.69984 5.706818391621135 0.017627578492957558\n",
      "it 1, train loss = 9.223380e-01\n",
      "2141445962.8281631 5.706786737735034 0.017627596797407517\n",
      "it 1, train loss = 9.194828e-01\n",
      "2141152667.8202233 5.706450220220183 0.01762749032801926\n",
      "it 1, train loss = 8.873065e-01\n",
      "2141458273.5077407 5.706800843627039 0.01762759686765065\n",
      "it 1, train loss = 8.754803e-01\n",
      "2141454698.3102758 5.706830171401373 0.01762759633849562\n",
      "it 1, train loss = 9.198126e-01\n",
      "2141465351.7381191 5.706816091191289 0.017627597007397855\n",
      "it 1, train loss = 8.793450e-01\n",
      "2141461550.8706086 5.706803343590991 0.017627596987565546\n",
      "it 1, train loss = 6.165166e-01\n",
      "2141425511.787728 5.706819572091808 0.017627590208457552\n",
      "it 1, train loss = 5.518962e-01\n",
      "2141474979.8784368 5.706853453587108 0.017627597146656744\n",
      "it 1, train loss = 4.605764e-01\n",
      "2141478428.6898682 5.706853972686931 0.017627597303290946\n",
      "it 1, train loss = 6.695093e-01\n",
      "2141483714.885666 5.706873097011117 0.01762760414760734\n",
      "it 1, train loss = 8.300309e-01\n",
      "2141407008.8852363 5.7068265643927205 0.017627588157563063\n",
      "it 1, train loss = 8.715057e-01\n",
      "2141456104.1937683 5.706802552169371 0.017627596757924486\n",
      "it 1, train loss = 9.331343e-01\n",
      "247379.9724256158 2.2401524003494493 0.017653757338954405\n"
     ]
    }
   ],
   "source": [
    "lamUT_vec = np.array(UTdata['F11'])\n",
    "lamET_vec = np.array(ETdata['F11'])\n",
    "lamPS_vec = np.array(PSdata['F11'])\n",
    "r2_list = []\n",
    "mae_list = []\n",
    "for i in range(50): #50 runs for every architecture\n",
    "    key, subkey = random.split(key)\n",
    "    params = init_cann(key)\n",
    "\n",
    "    opt_init, opt_update, get_params = optimizers.adam(2.e-4) #Original: 1.e-4\n",
    "    opt_state = opt_init(params)\n",
    "    params, train_loss, val_loss = train_jp(loss_P11_PS, 1, F11_data, opt_state, key, nIter = 100000, print_freq=1000000)\n",
    "\n",
    "    model = CANN_model(params[0], params[1], normalization)\n",
    "\n",
    "    lambdas     = [lamUT_vec,       lamET_vec,      lamPS_vec       ]\n",
    "    P11_gt_list = [UTdata['P11'],   ETdata['P11'],  PSdata['P11']   ]\n",
    "    P11funs     = [P11_UT,          P11_ET,         P11_PS          ]\n",
    "\n",
    "    r2 = []\n",
    "    mae = []\n",
    "    for lam, P11_gt, P11fun in zip(lambdas, P11_gt_list, P11funs):\n",
    "        P11 = P11fun(lam, model, normalization)\n",
    "        r2i = r2_score(P11_gt, P11)\n",
    "        r2i = np.clip(r2i, a_min=0.0, a_max=1.0)\n",
    "        r2.append(r2i)\n",
    "\n",
    "        maei = onp.mean(onp.abs(P11_gt-P11))\n",
    "        mae.append(maei)\n",
    "    # r2 = np.mean(np.array(r2))\n",
    "    print(*mae)\n",
    "    r2_list.append(r2)\n",
    "    mae_list.append(mae)\n",
    "cann_r2 = np.array(r2_list)\n",
    "cann_mae = np.array(mae_list)\n",
    "with open('savednet/CANN_r2_efficiency_PS.npy', 'wb') as f:\n",
    "    pickle.dump([[24], cann_r2], f)\n",
    "with open('savednet/CANN_mae_efficiency_PS.npy', 'wb') as f:\n",
    "    pickle.dump([[24], cann_mae], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 [1 1 2 1]\n",
      "it 1, train loss = 2.272110e+00\n",
      "0.2533086142510153 3.7262970413526215 0.016723981780638112\n",
      "it 1, train loss = 2.452063e+00\n",
      "0.2508070793810832 3.7516906447653935 0.016702754280029576\n",
      "it 1, train loss = 6.406696e-01\n",
      "0.2510481069930433 3.7518158953205947 0.016705529378323036\n",
      "it 1, train loss = 5.637222e-03\n",
      "0.2510896243576407 3.7437918302218796 0.016702080822713267\n",
      "it 1, train loss = 2.763702e+00\n",
      "0.24799099632103655 3.9241011477538836 0.016710378792177738\n",
      "it 1, train loss = 2.339619e+00\n",
      "0.25735031514949713 3.616717532430623 0.016738774608298184\n",
      "it 1, train loss = 6.846681e-02\n",
      "0.2491770932838638 3.964632283609834 0.01669345551089398\n",
      "it 1, train loss = 2.240116e+00\n",
      "0.2489167617995479 3.918202688018482 0.016703736689135974\n",
      "it 1, train loss = 1.822053e+00\n",
      "0.2501745771594058 3.7846344644727345 0.01670497209454531\n",
      "it 1, train loss = 2.144212e+00\n",
      "0.2515099150098937 3.7428249196383065 0.016713044062397975\n",
      "it 1, train loss = 2.819238e+00\n",
      "0.2496712499498031 3.8219091019214044 0.01670024905343733\n",
      "it 1, train loss = 2.435799e+00\n",
      "0.2501259398698755 3.8056362551361427 0.01669595049148066\n",
      "it 1, train loss = 2.926595e+00\n",
      "0.2532145814540135 3.6904108801288165 0.016717981498606302\n",
      "it 1, train loss = 2.945488e+00\n",
      "0.2532282546759996 3.698815058814897 0.016720580756876165\n",
      "it 1, train loss = 2.353089e+00\n",
      "0.2499368925327836 3.808009766481622 0.016698066101347785\n",
      "it 1, train loss = 1.465616e+00\n",
      "0.24993455155583405 3.799882172761702 0.016710703570742006\n",
      "it 1, train loss = 8.596052e-01\n",
      "0.2501161809034318 3.799212472604586 0.016700568116370617\n",
      "it 1, train loss = 2.907525e+00\n",
      "0.2563848476826155 3.6679789333166934 0.016743832775272614\n",
      "it 1, train loss = 2.501473e+00\n",
      "0.24886744279700945 3.8574515784042633 0.01670206208231051\n",
      "it 1, train loss = 2.521057e+00\n",
      "0.2504868266776437 3.773149803622676 0.016698506903163966\n",
      "it 1, train loss = 2.023675e+00\n",
      "0.2528134645855702 3.701850089011373 0.0167164175858335\n",
      "it 1, train loss = 2.923731e-01\n",
      "0.24884135802469914 3.8472392739156835 0.016693674701279162\n",
      "it 1, train loss = 2.053169e+00\n",
      "0.25067058080090043 3.7848257605656035 0.01671572520456978\n",
      "it 1, train loss = 2.026716e+00\n",
      "0.24992332141263762 3.808217563024678 0.0167002345639787\n",
      "it 1, train loss = 7.057693e-01\n",
      "0.24955208353292505 3.8080770490832037 0.01670054936946053\n",
      "it 1, train loss = 2.733636e+00\n",
      "0.2503045344617514 3.8049071741057228 0.016714415535062845\n",
      "it 1, train loss = 2.792365e+00\n",
      "0.2505984485376508 3.8539796243389066 0.01669570669988755\n",
      "it 1, train loss = 2.802687e+00\n",
      "0.25312523439186135 3.6926967898491974 0.016717611540588977\n",
      "it 1, train loss = 2.475917e+00\n",
      "0.2491317784941537 3.8821204950882606 0.016700611476233355\n",
      "it 1, train loss = 2.334193e+00\n",
      "0.25036825715585076 3.8177127626594105 0.016693423959140162\n",
      "it 1, train loss = 8.144331e-01\n",
      "0.24990140285513565 3.8129851071868446 0.016698187295713478\n",
      "it 1, train loss = 2.890619e+00\n",
      "0.25107788578969653 3.778386391174661 0.01671879001341481\n",
      "it 1, train loss = 2.689848e+00\n",
      "0.25090940573407944 3.848566413580852 0.016694828984289683\n",
      "it 1, train loss = 1.776501e+00\n",
      "0.2507044986476834 3.761298945000025 0.01670668023483327\n",
      "it 1, train loss = 1.488615e+00\n",
      "0.2507388997601366 3.850396023082438 0.01669553006938829\n",
      "it 1, train loss = 1.086478e+00\n",
      "0.2511390246225367 3.8718242916052192 0.01669714611034839\n",
      "it 1, train loss = 1.618490e+00\n",
      "0.2475480680028843 3.9833315676848637 0.016703500564645736\n",
      "it 1, train loss = 5.747302e-01\n",
      "0.25002137754289655 3.822218069672879 0.016713883991168913\n",
      "it 1, train loss = 1.526789e+00\n",
      "0.24920029999367457 3.8277213151535543 0.016703419392660006\n",
      "it 1, train loss = 2.929097e+00\n",
      "0.24955809673760046 3.841853249249746 0.016699076084537988\n",
      "it 1, train loss = 1.670180e+00\n",
      "0.2505773448399957 3.8588001215807206 0.01669621449497938\n",
      "it 1, train loss = 5.293704e-01\n",
      "0.24876102581532333 3.875337246607977 0.016700464545685818\n",
      "it 1, train loss = 2.023093e+00\n",
      "0.2501061012337808 3.807601921701639 0.016714226491414665\n",
      "it 1, train loss = 2.549241e+00\n",
      "0.2494768814833635 3.8250049628790594 0.016702650998894036\n",
      "it 1, train loss = 2.408672e+00\n",
      "0.25061834222018287 3.7727104437483026 0.016706252133765837\n",
      "it 1, train loss = 1.741079e+00\n",
      "0.250329859927328 3.8371411786311267 0.016695143428901806\n",
      "it 1, train loss = 2.857763e+00\n",
      "0.24908393650971472 3.8237709775423783 0.01670130067845421\n",
      "it 1, train loss = 1.688158e+00\n",
      "0.25013624911269383 3.790265833954772 0.016697421075199257\n",
      "it 1, train loss = 1.124926e+00\n",
      "0.25194954477707115 3.7412019866935537 0.016718204792669923\n",
      "it 1, train loss = 4.774259e-01\n",
      "0.24780112800102758 3.9709754111996243 0.01668508273693223\n",
      "16 [1, 2, 2, 1]\n",
      "it 1, train loss = 2.293048e+00\n",
      "0.24759667349754813 3.8580293999174433 0.016696615171338816\n",
      "it 1, train loss = 5.732283e-02\n",
      "0.24851661997945051 4.280392650198613 0.01668945053949882\n",
      "it 1, train loss = 2.228618e+00\n",
      "0.24947467813471755 4.107145069772872 0.01671764229277539\n",
      "it 1, train loss = 2.815039e+00\n",
      "0.25082255337634524 3.770218378459315 0.016709758984788155\n",
      "it 1, train loss = 2.321783e+00\n",
      "0.25136116375563833 3.9459876977167396 0.01673441939240773\n",
      "it 1, train loss = 2.318933e+00\n",
      "0.24887283613437142 4.090192201170996 0.016722238494223992\n",
      "it 1, train loss = 1.362159e+00\n",
      "0.2570562958059336 3.821746863286278 0.016752046112478637\n",
      "it 1, train loss = 2.678418e+00\n",
      "0.25883428704345257 3.6814610485920336 0.01674724687018922\n",
      "it 1, train loss = 2.971971e+00\n",
      "0.2554098880154418 3.8368359847747113 0.016743336914701652\n",
      "it 1, train loss = 3.582851e-01\n",
      "0.24697305487953478 3.955853367048992 0.016690298938990683\n",
      "it 1, train loss = 1.444436e+00\n",
      "0.24879572954544224 4.308568662302579 0.016666131660237327\n",
      "it 1, train loss = 3.891906e-01\n",
      "0.24832786563918088 4.192218681721241 0.016714127292825\n",
      "it 1, train loss = 5.073081e-01\n",
      "0.24739741390098038 3.903442456724801 0.016691257851422454\n",
      "it 1, train loss = 1.277109e-01\n",
      "0.2624768911449143 4.916674506694555 0.01659540647240123\n",
      "it 1, train loss = 2.040909e+00\n",
      "0.2472238301212239 4.060077491950991 0.016694734256882733\n",
      "it 1, train loss = 1.475202e+00\n",
      "0.24639428064802818 4.089093137611089 0.01668940425502972\n",
      "it 1, train loss = 4.518218e-01\n",
      "0.24777841811974166 4.0201132525067464 0.016707547541745964\n",
      "it 1, train loss = 2.860845e+00\n",
      "0.25232295909016 3.7853850921045478 0.0167217649858016\n",
      "it 1, train loss = 2.450831e+00\n",
      "0.2507516807361675 3.8257736368147497 0.016717053992963405\n",
      "it 1, train loss = 1.260606e+00\n",
      "0.24941146573981826 3.9755848850860476 0.016724429167547152\n",
      "it 1, train loss = 1.563381e+00\n",
      "0.24902368563928293 4.004915089771079 0.01671911204901562\n",
      "it 1, train loss = 5.490342e-02\n",
      "0.24723108917522005 3.9367255106316787 0.016709071886701902\n",
      "it 1, train loss = 1.096270e-01\n",
      "0.24718982450254184 4.065211784761401 0.016695083378932347\n",
      "it 1, train loss = 1.774157e+00\n",
      "0.2484308488780385 3.940779486980238 0.016714218771292075\n",
      "it 1, train loss = 1.744674e+00\n",
      "0.25022553309389955 4.375123894315459 0.01667913345321942\n",
      "it 1, train loss = 2.014424e+00\n",
      "0.2453890656831566 4.032673499151835 0.016689056634130055\n",
      "it 1, train loss = 2.846126e+00\n",
      "0.25260488737339665 3.7925792298215244 0.01672578427672184\n",
      "it 1, train loss = 3.748302e-02\n",
      "0.2487082162335569 3.9689725420044493 0.01671605008587283\n",
      "it 1, train loss = 2.526177e+00\n",
      "0.24792457798028103 4.064904711805747 0.016695651599956788\n",
      "it 1, train loss = 2.741636e+00\n",
      "0.2482093254227812 3.9470015001355554 0.01671156997797908\n",
      "it 1, train loss = 6.082519e-01\n",
      "0.24837136261667106 3.93348616789758 0.0167098129005106\n",
      "it 1, train loss = 2.229630e-01\n",
      "0.24724116432592516 4.0080792993669645 0.016697226679491424\n",
      "it 1, train loss = 1.548817e+00\n",
      "0.25905532641266826 3.6059312773273384 0.01674497258667255\n",
      "it 1, train loss = 1.104968e+00\n",
      "0.24840899729107563 4.0674319812508095 0.016717959546352735\n",
      "it 1, train loss = 1.553842e+00\n",
      "0.2459016045807984 4.116402554497023 0.016684717966050075\n",
      "it 1, train loss = 2.954075e+00\n",
      "0.2525170130709682 3.8549288267258324 0.01673612487157598\n",
      "it 1, train loss = 2.797719e+00\n",
      "0.25237416506380894 3.915284280599176 0.016737983028876656\n",
      "it 1, train loss = 2.457547e+00\n",
      "0.24932856678117962 3.8196498011402213 0.016705125153754465\n",
      "it 1, train loss = 1.161939e+00\n",
      "0.24867309149215935 4.1536338624363776 0.01670901342639271\n",
      "it 1, train loss = 1.531675e+00\n",
      "0.24708351350111213 4.098753636658621 0.016696897887263303\n",
      "it 1, train loss = 1.520584e+00\n",
      "0.24907880308507685 4.056013814951509 0.016721438296284703\n",
      "it 1, train loss = 1.473055e+00\n",
      "0.24869321020044513 3.9924017024860365 0.016711819910264207\n",
      "it 1, train loss = 1.671477e+00\n",
      "0.24752463103436115 3.9326238899314574 0.016697441398222188\n",
      "it 1, train loss = 3.171942e-01\n",
      "0.24705293218260324 4.084943571078732 0.01669764421125557\n",
      "it 1, train loss = 3.371805e-01\n",
      "0.235246797378631 4.1286375091973895 0.016653584554507675\n",
      "it 1, train loss = 8.511036e-01\n",
      "0.25121984461549235 4.410072485942615 0.016676169360056392\n",
      "it 1, train loss = 2.570496e+00\n",
      "0.26104623288155965 3.6451676829531836 0.016768091715425433\n",
      "it 1, train loss = 2.717581e+00\n",
      "0.25099219826211544 3.9385689312751833 0.016733285251942023\n",
      "it 1, train loss = 1.737602e+00\n",
      "0.25230644440984706 3.898570861474168 0.01672878843096574\n",
      "it 1, train loss = 2.478715e+00\n",
      "0.24959206141288345 3.888302248721204 0.016716972742126645\n",
      "22 [1 3 2 1]\n",
      "it 1, train loss = 2.989505e+00\n",
      "0.27372213572639154 3.401933820990262 0.016252526614839075\n",
      "it 1, train loss = 2.749964e+00\n",
      "0.2516244361397969 4.00434383736228 0.016737886158116142\n",
      "it 1, train loss = 1.533547e+00\n",
      "0.24601514547489559 4.098000511599359 0.016502948602289052\n",
      "it 1, train loss = 1.301523e+00\n",
      "0.24941400122755547 3.8804805140407486 0.016718486656443734\n",
      "it 1, train loss = 6.984019e-01\n",
      "0.24941969841877992 4.518099599612986 0.016602979576274577\n",
      "it 1, train loss = 2.844844e+00\n",
      "0.2517204722917122 4.0349026970713116 0.01673912027241735\n",
      "it 1, train loss = 2.742608e+00\n",
      "0.25281441763323914 4.029303748789501 0.016746353616333382\n",
      "it 1, train loss = 1.006205e+00\n",
      "0.24852197241315885 4.224361074028781 0.016715837142185143\n",
      "it 1, train loss = 1.841105e+00\n",
      "0.2572919114716272 3.8382287488677322 0.016758821414438373\n",
      "it 1, train loss = 2.161821e+00\n",
      "0.255360547228071 4.109655056447019 0.0167592369089313\n",
      "it 1, train loss = 1.493737e+00\n",
      "0.2522093906192955 4.053610795672055 0.016741561024183692\n",
      "it 1, train loss = 2.287871e+00\n",
      "0.25210505822007234 4.052718967769881 0.0167399168980778\n",
      "it 1, train loss = 2.526912e+00\n",
      "0.25552777336549726 3.980141091550508 0.016752755837880554\n",
      "it 1, train loss = 2.219124e+00\n",
      "0.25406299466353927 4.123493804840855 0.016740064105595427\n",
      "it 1, train loss = 2.514161e+00\n",
      "0.2518675081806797 4.09277678568561 0.016728229937276073\n",
      "it 1, train loss = 7.849469e-01\n",
      "0.2526787079434998 3.981357004150586 0.016749009955056328\n",
      "it 1, train loss = 6.372249e-01\n",
      "0.2509162042696895 4.03099599311441 0.016726226586103083\n",
      "it 1, train loss = 1.559690e+00\n",
      "0.25186642264880943 4.396614751768171 0.016683095645433986\n",
      "it 1, train loss = 5.341687e-01\n",
      "0.2520987127630076 4.079773286900137 0.016732228336776456\n",
      "it 1, train loss = 6.675971e-01\n",
      "0.2490873647377532 4.046380164131398 0.0167185585065037\n",
      "it 1, train loss = 1.997342e+00\n",
      "0.24976897234997883 4.071338685350398 0.0165575552068607\n",
      "it 1, train loss = 2.106560e+00\n",
      "0.25365696223568196 3.9142786975297903 0.016746373337110935\n",
      "it 1, train loss = 1.546396e+00\n",
      "0.24672077487600574 3.9903629785423185 0.016701858332734867\n",
      "it 1, train loss = 1.479436e-01\n",
      "0.25307113148680516 3.967204506223995 0.016745775436461514\n",
      "it 1, train loss = 5.599218e-01\n",
      "0.25218102109842255 4.044338375912012 0.016732299042064108\n",
      "it 1, train loss = 2.916281e+00\n",
      "0.2561519875237681 3.8403820516336395 0.01674414009003181\n",
      "it 1, train loss = 1.490811e-02\n",
      "0.2487869580453139 4.006402102878643 0.01671157341169994\n",
      "it 1, train loss = 1.537665e+00\n",
      "0.25753275696400746 4.025956761330251 0.016776166894785068\n",
      "it 1, train loss = 2.600598e+00\n",
      "0.2508756008283033 3.9608005368345953 0.01672905350505496\n",
      "it 1, train loss = 2.092505e+00\n",
      "0.251757686097173 3.9332043310163924 0.01673712248579757\n",
      "it 1, train loss = 2.361180e+00\n",
      "0.25779127791600387 3.9996904045721027 0.0167690049950862\n",
      "it 1, train loss = 5.458153e-01\n",
      "0.2631338255941767 3.4018061306223073 0.016178320975752756\n",
      "it 1, train loss = 3.144682e-02\n",
      "0.24979954003709257 4.263278673062299 0.016709834154944862\n",
      "it 1, train loss = 2.676069e+00\n",
      "0.2515885511204845 4.391497492202652 0.01668680266914629\n",
      "it 1, train loss = 7.761632e-01\n",
      "0.253273203586602 4.0034498066982955 0.016758050199049933\n",
      "it 1, train loss = 2.899858e+00\n",
      "0.2581665822479706 3.8351056979209885 0.016761108048215963\n",
      "it 1, train loss = 7.956619e-01\n",
      "0.24786251081452254 4.037309104396873 0.016707990516803217\n",
      "it 1, train loss = 1.645225e+00\n",
      "0.26248517262923243 3.794581669349532 0.01677299870096861\n",
      "it 1, train loss = 1.847057e+00\n",
      "0.26070917277140243 3.8240366984201635 0.016759202223779614\n",
      "it 1, train loss = 2.961565e+00\n",
      "0.254665438237521 3.8517056587352045 0.01674646413996602\n",
      "it 1, train loss = 9.651301e-01\n",
      "0.24640811564405576 4.189704372289111 0.01667877138104917\n",
      "it 1, train loss = 2.066187e+00\n",
      "0.25376094707129293 3.8918444933721297 0.01673935109972272\n",
      "it 1, train loss = 2.707248e+00\n",
      "0.2505911124025284 3.636380259362006 0.016195158999032466\n",
      "it 1, train loss = 6.109885e-02\n",
      "0.24874973434912628 4.185072225938632 0.01671076410301279\n",
      "it 1, train loss = 5.943867e-01\n",
      "0.2769480580963494 3.5026171730568585 0.01678820368402819\n",
      "it 1, train loss = 1.235783e+00\n",
      "0.25413401142368675 4.486119698575903 0.016692498860326886\n",
      "it 1, train loss = 2.691162e-01\n",
      "0.25106168932717415 4.063339394063549 0.016728156341190775\n",
      "it 1, train loss = 5.067446e-01\n",
      "0.250361695712487 3.8827347106113868 0.016716462363943484\n",
      "it 1, train loss = 7.443281e-01\n",
      "0.24961186166623311 3.9180302385966495 0.01671789616758606\n",
      "it 1, train loss = 1.719026e+00\n",
      "0.24917926344145763 4.016442505899539 0.01671985203564588\n",
      "30 [1, 3, 3, 1]\n",
      "it 1, train loss = 3.837891e-01\n",
      "0.25516837784550167 3.717874137242373 0.01675181397745246\n",
      "it 1, train loss = 3.464583e-01\n",
      "0.2473499356110362 4.1667082451985396 0.01668633261894215\n",
      "it 1, train loss = 1.742021e+00\n",
      "0.24736869198174663 3.979724907836833 0.01667627437333125\n",
      "it 1, train loss = 9.098767e-01\n",
      "0.25319011324916735 4.676505979507229 0.01660841258549541\n",
      "it 1, train loss = 2.362201e-02\n",
      "0.2438855758059589 4.031175808726088 0.016652889575357006\n",
      "it 1, train loss = 1.176025e+00\n",
      "1.0831112511899774 5.437155755130071 0.016335339158447156\n",
      "it 1, train loss = 2.828475e+00\n",
      "0.25051755036813655 4.386385079822325 0.016661907585742392\n",
      "it 1, train loss = 7.640274e-01\n",
      "0.24855633698889934 4.217585993327332 0.016699394926187393\n",
      "it 1, train loss = 1.559550e+00\n",
      "0.2473025214797066 4.149353774973801 0.01669979663757352\n",
      "it 1, train loss = 2.486345e+00\n",
      "0.24839720763289236 4.006877984283083 0.016694835070352142\n",
      "it 1, train loss = 1.950757e+00\n",
      "0.2442451553590937 4.089915884922719 0.01663424532460478\n",
      "it 1, train loss = 2.029162e+00\n",
      "0.24806830945410815 4.236955088933951 0.016677197979355067\n",
      "it 1, train loss = 2.577936e+00\n",
      "0.452610282289606 5.685732416041504 0.016173087924391268\n",
      "it 1, train loss = 1.326288e+00\n",
      "0.2462533226202424 4.073868660788217 0.0166712915627805\n",
      "it 1, train loss = 7.246407e-01\n",
      "1.1049903412876643 5.3973153842861485 0.01633919715193882\n",
      "it 1, train loss = 1.726322e+00\n",
      "0.24682216888036368 4.065581669167879 0.01669321322035855\n",
      "it 1, train loss = 6.460849e-01\n",
      "0.24838275003834764 4.333120732597683 0.016651880437301956\n",
      "it 1, train loss = 5.622326e-01\n",
      "0.2493629465898345 4.327982990242397 0.016671589431874537\n",
      "it 1, train loss = 1.289490e+00\n",
      "0.2501891188140097 4.025821754115244 0.016701669495221953\n",
      "it 1, train loss = 2.779576e+00\n",
      "0.32888268674884175 5.266503197831134 0.016445181494218995\n",
      "it 1, train loss = 2.314009e+00\n",
      "0.2376031704047405 4.425124957048479 0.016482154579853296\n",
      "it 1, train loss = 3.752786e-01\n",
      "0.25022573185725977 4.019810797238044 0.016716695602385038\n",
      "it 1, train loss = 8.715402e-01\n",
      "0.25074726762579397 4.40691297620623 0.016657355403030517\n",
      "it 1, train loss = 2.081239e+00\n",
      "0.24999345022103023 4.484468127043627 0.016619588163506817\n",
      "it 1, train loss = 1.052688e+00\n",
      "0.24932356395142943 4.080614372136696 0.016704085371155898\n",
      "it 1, train loss = 1.893058e+00\n",
      "0.24954091333697004 4.422923376590742 0.016645457839784196\n",
      "it 1, train loss = 9.616314e-01\n",
      "0.2523612362743619 4.586567095957918 0.016630075088688027\n",
      "it 1, train loss = 1.649437e+00\n",
      "0.2513459073342942 4.009540388912798 0.01671789002075974\n",
      "it 1, train loss = 2.575824e+00\n",
      "0.24560909926147165 4.145043209057934 0.016649936020556554\n",
      "it 1, train loss = 1.429380e-02\n",
      "0.821221599782274 5.942265622452394 0.016360976503475608\n",
      "it 1, train loss = 2.583899e+00\n",
      "0.24477057672901922 4.2015610185646395 0.016668556463016824\n",
      "it 1, train loss = 4.238941e-01\n",
      "0.2507519922591348 4.395957932655396 0.01667004541000306\n",
      "it 1, train loss = 3.464671e-02\n",
      "1.0837834691479475 5.458075988737504 0.016335110570784517\n",
      "it 1, train loss = 1.920296e+00\n",
      "0.24653099376311688 4.172009745659902 0.01667482087211169\n",
      "it 1, train loss = 2.488784e+00\n",
      "0.24915703409213466 4.333240144461213 0.016665948428897175\n",
      "it 1, train loss = 1.063541e-01\n",
      "0.24767100528427874 3.86713918241603 0.01669435036636918\n",
      "it 1, train loss = 2.852547e+00\n",
      "0.6583741902083889 6.2612091109744945 0.016281303801504325\n",
      "it 1, train loss = 1.187851e+00\n",
      "0.22847978466185587 4.12064172649153 0.01661250298428767\n",
      "it 1, train loss = 2.878539e+00\n",
      "0.2467506304572957 4.183163157278871 0.016676096616329315\n",
      "it 1, train loss = 1.588442e+00\n",
      "0.24488395115156564 4.145323793950723 0.016664312018371017\n",
      "it 1, train loss = 6.887598e-01\n",
      "0.24840868457812162 4.3610657266349815 0.016641530401920773\n",
      "it 1, train loss = 2.602178e+00\n",
      "0.25250710447041874 4.433768418659099 0.016660529637622223\n",
      "it 1, train loss = 2.460105e-01\n",
      "0.24771060085821997 3.893258089716389 0.016685436888862776\n",
      "it 1, train loss = 1.413730e+00\n",
      "0.6593969507949095 6.515325622240134 0.016065774796042005\n",
      "it 1, train loss = 1.875334e+00\n",
      "0.2467020253497345 4.274648418162798 0.016640344557067498\n",
      "it 1, train loss = 8.565974e-01\n",
      "0.2503944924343711 4.248980862588873 0.016696241756818192\n",
      "it 1, train loss = 1.359493e+00\n",
      "0.911587197714916 5.879406514039496 0.016363284306112884\n",
      "it 1, train loss = 1.494545e+00\n",
      "0.7833577754226353 6.039099147173149 0.01636105787824813\n",
      "it 1, train loss = 1.731480e+00\n",
      "0.34482314996297136 2.364247788415206 0.00400201410896358\n",
      "it 1, train loss = 2.185951e+00\n",
      "0.7118821963617702 6.241146764729231 0.016314715774658938\n",
      "38 [1 4 3 1]\n",
      "it 1, train loss = 2.018535e+00\n",
      "0.25419733500558644 3.9731133069967086 0.016737195769448536\n",
      "it 1, train loss = 2.371487e+00\n",
      "0.25092977462177085 4.123860432184101 0.01671128938389889\n",
      "it 1, train loss = 1.089188e+00\n",
      "0.8494665405631845 5.939707097637428 0.016355288015619352\n",
      "it 1, train loss = 2.113811e+00\n",
      "0.2501031363891468 4.04731284734078 0.016697346102665923\n",
      "it 1, train loss = 1.170591e+00\n",
      "0.4876755994700977 5.6915362403262915 0.016119827823643816\n",
      "it 1, train loss = 1.512729e+00\n",
      "0.7848255434805702 6.1938272915721 0.016371097685011902\n",
      "it 1, train loss = 2.728140e+00\n",
      "0.2513235508214057 4.1303640610138155 0.016708756922082738\n",
      "it 1, train loss = 2.699634e+00\n",
      "0.24869622034169997 4.251226604100342 0.016680432532174386\n",
      "it 1, train loss = 2.685926e+00\n",
      "1.0625488716198845 5.439440253192724 0.016333926288424123\n",
      "it 1, train loss = 2.866963e+00\n",
      "0.24938226485696466 4.213607317067337 0.01668526145854417\n",
      "it 1, train loss = 1.362867e+00\n",
      "1.0964216164476885 5.422686810067695 0.01633684245601991\n",
      "it 1, train loss = 1.546965e+00\n",
      "0.2478665731062406 4.227937616912035 0.016678347035471493\n",
      "it 1, train loss = 2.702840e+00\n",
      "0.7442184503177325 6.098832766604764 0.016315882286591567\n",
      "it 1, train loss = 2.625609e+00\n",
      "0.2502936050920951 4.094948034794836 0.016709861602583535\n",
      "it 1, train loss = 2.466095e+00\n",
      "0.9993448806224096 5.7469425056034105 0.01635089714575871\n",
      "it 1, train loss = 1.502791e+00\n",
      "0.9542448742474565 5.807224097961955 0.01636089816162747\n",
      "it 1, train loss = 6.107269e-01\n",
      "0.2607452449463935 4.895613771763309 0.016583185557261747\n",
      "it 1, train loss = 2.036472e+00\n",
      "0.24968401432867157 4.285978469154003 0.016671400810399813\n",
      "it 1, train loss = 1.548652e-01\n",
      "0.2508666273128113 4.31984895596506 0.016680044157101975\n",
      "it 1, train loss = 2.792164e+00\n",
      "0.7072458157535183 6.311463497008085 0.016282459089674687\n",
      "it 1, train loss = 3.565919e-01\n",
      "0.24769344608259328 4.205571923500575 0.016683121565741844\n",
      "it 1, train loss = 1.534696e+00\n",
      "0.9820025093822768 5.736158739723227 0.01634769399572065\n",
      "it 1, train loss = 1.565457e+00\n",
      "0.2487686268185714 4.185483307179659 0.016698262014487254\n",
      "it 1, train loss = 2.528703e+00\n",
      "0.2500692676197918 4.247337672820985 0.01669129642989532\n",
      "it 1, train loss = 8.039854e-01\n",
      "0.25078075092973656 4.3696403677808595 0.01667091644146681\n",
      "it 1, train loss = 2.896858e+00\n",
      "0.2514571243119472 4.231899210052047 0.016706255543952487\n",
      "it 1, train loss = 1.588548e+00\n",
      "0.2540085962410357 4.62484003041101 0.016632202926585477\n",
      "it 1, train loss = 2.376668e+00\n",
      "0.7536503411113417 6.229330839537345 0.016357440684584525\n",
      "it 1, train loss = 1.374320e+00\n",
      "1.0017992651078296 5.56736462692056 0.01632570088278528\n",
      "it 1, train loss = 2.170054e+00\n",
      "0.253362619502065 4.639906634213897 0.01659256354624084\n",
      "it 1, train loss = 1.438743e-01\n",
      "0.603024164745068 6.589275683750655 0.015992797231831907\n",
      "it 1, train loss = 1.730203e+00\n",
      "0.25137070654418375 4.173437386056635 0.016709123496305718\n",
      "it 1, train loss = 2.246072e+00\n",
      "0.2519804952639134 4.199555092060795 0.016709623898140676\n",
      "it 1, train loss = 2.547172e+00\n",
      "0.25046442761327004 4.024000095790237 0.016708423442721113\n",
      "it 1, train loss = 1.625657e+00\n",
      "0.38199663456897626 2.304342841624155 0.00394226003158666\n",
      "it 1, train loss = 1.611155e+00\n",
      "0.2506388458144295 4.26557411556426 0.016693615779250884\n",
      "it 1, train loss = 2.725979e+00\n",
      "0.24828268586923963 4.147403031054218 0.01669611577125407\n",
      "it 1, train loss = 2.812628e+00\n",
      "1.1423517337065934 5.413198337838467 0.016355318512859023\n",
      "it 1, train loss = 1.542493e+00\n",
      "0.9329100579506389 5.789197726283462 0.01634824407281796\n",
      "it 1, train loss = 1.806684e+00\n",
      "0.2515481911882404 4.338836937699798 0.016688283217855725\n",
      "it 1, train loss = 2.280842e+00\n",
      "0.2513359391433871 4.135981816885003 0.016714540637945866\n",
      "it 1, train loss = 2.989300e+00\n",
      "0.25175167880292154 4.434913647182312 0.01665933504606199\n",
      "it 1, train loss = 1.224912e+00\n",
      "0.24796335657673915 4.062183044215819 0.016698809254447323\n",
      "it 1, train loss = 1.233097e+00\n",
      "0.24950485177561685 4.133862200023465 0.016699696975180523\n",
      "it 1, train loss = 6.435426e-03\n",
      "0.9167848216046753 5.80656738366072 0.01634942120833683\n",
      "it 1, train loss = 1.801234e+00\n",
      "0.24987421855463512 4.240186914277649 0.01669020246324854\n",
      "it 1, train loss = 7.899808e-01\n",
      "0.24554820000814273 4.139697000469148 0.016684892201387948\n",
      "it 1, train loss = 4.912282e-01\n",
      "0.25422211666038236 4.636475433749148 0.01662051386177651\n",
      "it 1, train loss = 3.048073e-01\n",
      "0.2550089546382807 4.763871279872973 0.016596479402092208\n",
      "it 1, train loss = 3.024913e-01\n",
      "1.127534871168746 5.379262538011008 0.016285320299257302\n",
      "48 [1, 4, 4, 1]\n",
      "it 1, train loss = 1.242552e+00\n",
      "0.253070662910988 4.613989289540776 0.01661474227481592\n",
      "it 1, train loss = 2.386743e+00\n",
      "1.0824464104987244 5.414605772871936 0.016343703709181565\n",
      "it 1, train loss = 3.727252e-01\n",
      "0.331866191352038 2.4585983890406635 0.004037526652562544\n",
      "it 1, train loss = 2.108559e+00\n",
      "0.2763209628106141 4.982290388216356 0.016510473870440332\n",
      "it 1, train loss = 3.672464e-01\n",
      "0.9559643396333033 5.733728169569284 0.01634828311924694\n",
      "it 1, train loss = 2.904962e+00\n",
      "0.25222449468520874 4.60509822577165 0.016585970190458994\n",
      "it 1, train loss = 1.660848e+00\n",
      "0.6629450242802689 6.360593370427371 0.01621909129467192\n",
      "it 1, train loss = 2.280518e+00\n",
      "1.0955331225305296 5.4407292081064424 0.016347613911522837\n",
      "it 1, train loss = 4.451241e-02\n",
      "1.1088224604947314 5.4105109490776115 0.01634584453315283\n",
      "it 1, train loss = 1.348710e+00\n",
      "0.3093106639915414 2.397390101403921 0.003997210389874841\n",
      "it 1, train loss = 1.985242e+00\n",
      "0.37888514791573213 2.402404689860124 0.003965386700224677\n",
      "it 1, train loss = 2.241241e+00\n",
      "0.6438579938269493 6.3424799642784695 0.016222927475419\n",
      "it 1, train loss = 1.871307e-01\n",
      "0.8932646949255786 5.822093625779701 0.01634358446975812\n",
      "it 1, train loss = 2.899511e+00\n",
      "0.2536036971436232 4.670573943544264 0.016609289588469887\n",
      "it 1, train loss = 1.567415e+00\n",
      "0.28546202786893443 5.068259984561736 0.01651973039892696\n",
      "it 1, train loss = 2.145466e+00\n",
      "1.1237621625446035 5.351758156422388 0.016353327775040574\n",
      "it 1, train loss = 2.796570e+00\n",
      "0.25241853676316967 4.602027388723628 0.016583685965391823\n",
      "it 1, train loss = 1.609744e+00\n",
      "0.24735763284846435 4.107814376836414 0.016686949382780678\n",
      "it 1, train loss = 9.686930e-02\n",
      "0.6269025502663168 5.101068469963171 0.0161763099760076\n",
      "it 1, train loss = 1.503670e+00\n",
      "1.1000014165610403 5.4090571337056295 0.016344885482691033\n",
      "it 1, train loss = 1.223545e+00\n",
      "0.6338959020636971 6.365881002763245 0.01621099201104762\n",
      "it 1, train loss = 1.778581e+00\n",
      "0.3529462507146242 2.501682161542149 0.003982195395562653\n",
      "it 1, train loss = 2.863249e+00\n",
      "0.2518124607155975 4.374470979517716 0.016670471598330686\n",
      "it 1, train loss = 2.106791e+00\n",
      "1.0857617089801264 5.436349176768297 0.016340347837128573\n",
      "it 1, train loss = 1.966859e+00\n",
      "0.27339860027606067 4.9783058545776155 0.016528499028958546\n",
      "it 1, train loss = 2.968465e+00\n",
      "0.37780928769364 5.390493374593025 0.01631482036985815\n",
      "it 1, train loss = 2.714796e+00\n",
      "1.081372566244621 5.440083227168214 0.016350124931057128\n",
      "it 1, train loss = 1.485266e+00\n",
      "1.128467120637286 5.470759262769728 0.01634836878212919\n",
      "it 1, train loss = 1.713451e+00\n",
      "1.143155584690696 5.308917502105464 0.016361476278659335\n",
      "it 1, train loss = 6.884842e-02\n",
      "0.6753615904229079 6.2877058942855575 0.016253494071427548\n",
      "it 1, train loss = 1.602824e-01\n",
      "0.596055882616531 6.469742058550636 0.016114761202764345\n",
      "it 1, train loss = 6.867080e-01\n",
      "0.2498944776763171 4.2941768788573675 0.016667695568419728\n",
      "it 1, train loss = 1.580109e+00\n",
      "0.15683543683962076 3.6018541929081334 0.004060531076883151\n",
      "it 1, train loss = 1.677471e-02\n",
      "0.32790419976503415 2.5275250433838106 0.004042281611952975\n",
      "it 1, train loss = 1.812810e+00\n",
      "1.0201561394258887 6.11977995020073 0.01633674810693458\n",
      "it 1, train loss = 2.381416e+00\n",
      "0.8177883909598147 6.152179556234809 0.016382711103834224\n",
      "it 1, train loss = 3.431643e-01\n",
      "0.8895838778997023 5.913950324992118 0.016362551801083192\n",
      "it 1, train loss = 2.624085e+00\n",
      "0.29895354423034254 5.138845710710537 0.016489489552817795\n",
      "it 1, train loss = 2.582232e+00\n",
      "0.34779860770466686 5.177630872397443 0.016353200351448514\n",
      "it 1, train loss = 1.754329e+00\n",
      "1.0638740182997282 5.634251075934482 0.016344144443882727\n",
      "it 1, train loss = 1.691055e+00\n",
      "0.24846691462835374 4.057910529080816 0.016686977844708864\n",
      "it 1, train loss = 4.622925e-01\n",
      "0.43993701300631516 5.4945896729120225 0.016198622398599767\n",
      "it 1, train loss = 1.620437e+00\n",
      "0.2526593114463024 4.6388004020847005 0.01660479976846642\n",
      "it 1, train loss = 2.386449e+00\n",
      "1.1210435787499482 5.369319471417147 0.016347963781547688\n",
      "it 1, train loss = 2.741906e+00\n",
      "0.2466059386326992 4.254047643220527 0.01662352264848991\n",
      "it 1, train loss = 2.221928e+00\n",
      "0.7751547543812286 6.110244607906123 0.016355234454293387\n",
      "it 1, train loss = 2.138113e+00\n",
      "0.8296607478514203 6.065382051773876 0.016380811724181016\n",
      "it 1, train loss = 2.135295e+00\n",
      "0.24770672775106992 4.26321593654298 0.01665413324023602\n",
      "it 1, train loss = 4.845867e-01\n",
      "0.24377387205766735 4.148051037945154 0.0166117780235154\n",
      "it 1, train loss = 2.738629e+00\n",
      "1.1091687806997306 5.395833590986905 0.01634846554671765\n"
     ]
    }
   ],
   "source": [
    "lamUT_vec = np.array(UTdata['F11'])\n",
    "lamET_vec = np.array(ETdata['F11'])\n",
    "lamPS_vec = np.array(PSdata['F11'])\n",
    "np_list = []\n",
    "r2_list_global = []\n",
    "mae_list_global = []\n",
    "for n in range(10,50):\n",
    "    n_params, layers = opt_arch_node(n)\n",
    "    if n_params in np_list:\n",
    "        continue\n",
    "    print(n_params, layers)\n",
    "    np_list.append(n_params)\n",
    "\n",
    "    r2_list_arch = []\n",
    "    mae_list_arch = []\n",
    "    for i in range(50): #50 runs for every architecture\n",
    "        key, subkey = random.split(key)\n",
    "        params = init_node(key, layers)\n",
    "\n",
    "        # Make sure you are starting at a good place\n",
    "        loss = loss_P11_PS(params, F11_data, 3)\n",
    "        while loss>3.0:\n",
    "            key, subkey = random.split(key)\n",
    "            params = init_node(key, layers)\n",
    "            loss = loss_P11_PS(params, F11_data, 3)\n",
    "\n",
    "        opt_init, opt_update, get_params = optimizers.adam(2.e-4) #Original: 2.e-4\n",
    "        opt_state = opt_init(params)\n",
    "        params, train_loss, val_loss = train_jp(loss_P11_PS, 3, F11_data, opt_state, key, nIter = 100000, print_freq=1000000)\n",
    "\n",
    "        model = NODE_model(params[0], params[1])\n",
    "\n",
    "        lambdas     = [lamUT_vec,       lamET_vec,      lamPS_vec       ]\n",
    "        P11_gt_list = [UTdata['P11'],   ETdata['P11'],  PSdata['P11']   ]\n",
    "        P11funs     = [P11_UT,          P11_ET,         P11_PS          ]\n",
    "\n",
    "        r2 = []\n",
    "        mae = []\n",
    "        for lam, P11_gt, P11fun in zip(lambdas, P11_gt_list, P11funs):\n",
    "            P11 = P11fun(lam, model, normalization)\n",
    "            r2i = r2_score(P11_gt, P11)\n",
    "            r2i = np.clip(r2i, a_min=0.0, a_max=1.0)\n",
    "            r2.append(r2i)\n",
    "\n",
    "            maei = onp.mean(onp.abs(P11_gt-P11))\n",
    "            mae.append(maei)\n",
    "        # r2 = np.mean(np.array(r2))\n",
    "        # print(*r2)\n",
    "        print(*mae)\n",
    "        r2_list_arch.append(r2)\n",
    "        mae_list_arch.append(mae)\n",
    "    r2_list_global.append(r2_list_arch)\n",
    "    mae_list_global.append(mae_list_arch)\n",
    "node_r2 = np.array(r2_list_global)\n",
    "node_mae = np.array(mae_list_global)\n",
    "with open('savednet/NODE_r2_efficiency_PS.npy', 'wb') as f:\n",
    "    pickle.dump([np_list, node_r2], f)\n",
    "with open('savednet/NODE_mae_efficiency_PS.npy', 'wb') as f:\n",
    "    pickle.dump([np_list, node_mae], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 [1 1 1]\n",
      "it 1, train loss = 1.005734e+00\n",
      "0.6848119463247647 0.06480659818044032 0.030023291573076767\n",
      "it 1, train loss = 8.919212e-01\n",
      "0.29197227474470355 2.593960576498475 0.01941844358726515\n",
      "it 1, train loss = 7.771648e-01\n",
      "0.3540076558324843 1.6352699272784768 0.016089237194597848\n",
      "it 1, train loss = 9.105900e-01\n",
      "0.7004251427124756 0.2581012823608262 0.030023882158676657\n",
      "it 1, train loss = 8.801838e-01\n",
      "0.5012816226266528 3.0533050036719445 0.020766563736928704\n",
      "it 1, train loss = 1.035734e+00\n",
      "0.6885232492465133 0.08428861893262998 0.03002242788110562\n",
      "it 1, train loss = 9.072805e-01\n",
      "1.001584074807844 3.8759297118666023 0.027826517025104414\n",
      "it 1, train loss = 1.051232e+00\n",
      "0.6865324942745089 0.06182087445428667 0.030022402794267284\n",
      "it 1, train loss = 9.288619e-01\n",
      "0.683681185860754 0.08618431723166632 0.030032503359634637\n",
      "it 1, train loss = 9.222937e-01\n",
      "0.6911357972561899 0.11158131978049157 0.030032075091685106\n",
      "it 1, train loss = 1.033372e+00\n",
      "0.6839948648993037 0.07773815286344089 0.030022887428224707\n",
      "it 1, train loss = 9.709498e-01\n",
      "0.6797632480135356 0.14107222315048154 0.030024767246407926\n",
      "it 1, train loss = 1.015298e+00\n",
      "0.6812229628294836 0.12038203833557928 0.030023631425441205\n",
      "it 1, train loss = 8.896526e-01\n",
      "0.1393442435100827 2.3037141526418328 0.01807215168587314\n",
      "it 1, train loss = 9.296848e-01\n",
      "0.6897012429679481 0.10064210772358452 0.030027101998033094\n",
      "it 1, train loss = 9.834914e-01\n",
      "0.6828002115160898 0.09679343806679763 0.03002451616540695\n",
      "it 1, train loss = 9.573328e-01\n",
      "0.6824021490452716 0.10420305829707516 0.030026675675673947\n",
      "it 1, train loss = 8.398150e-01\n",
      "0.20330510009606523 1.9956456079967841 0.0163548554849466\n",
      "it 1, train loss = 8.337090e-01\n",
      "0.5114686749364318 1.935051769205576 0.016399694814658448\n",
      "it 1, train loss = 9.429062e-01\n",
      "0.6906650306054984 0.1152592807615138 0.030024412267745354\n",
      "it 1, train loss = 1.081736e+00\n",
      "0.6827454911440506 0.097289496963049 0.03002239129258152\n",
      "it 1, train loss = 8.253520e-01\n",
      "0.1920796521886112 1.057046952325056 0.016079596057080182\n",
      "it 1, train loss = 7.971346e-01\n",
      "0.17234064347492178 1.5796281728201695 0.016129254809569196\n",
      "it 1, train loss = 1.097937e+00\n",
      "0.7061491180457525 0.33224838088696745 0.030021453900597628\n",
      "it 1, train loss = 9.386598e-01\n",
      "0.7928510573194498 7.6964875661554 0.029151222277977812\n",
      "it 1, train loss = 1.164309e+00\n",
      "0.6888992823658603 0.08513721172072557 0.030021537468714266\n",
      "it 1, train loss = 9.790822e-01\n",
      "1.6028857517375803 12.684516854447763 0.03002055485787863\n",
      "it 1, train loss = 8.771902e-01\n",
      "0.3384181723958918 2.2896185584898734 0.0169862076760073\n",
      "it 1, train loss = 1.116266e+00\n",
      "0.6959771455699848 0.18134866944757833 0.03002157988748591\n",
      "it 1, train loss = 1.089785e+00\n",
      "0.6759193713389046 0.19489929321398378 0.030021756983585675\n",
      "it 1, train loss = 7.708245e-01\n",
      "0.2863345352824512 1.0034100785698292 0.01611047410924951\n",
      "it 1, train loss = 7.503223e-01\n",
      "0.21183879466872446 1.4291904089029415 0.01606582756295635\n",
      "it 1, train loss = 1.084311e+00\n",
      "0.7077314293996821 0.3570991356104388 0.0300214367754521\n",
      "it 1, train loss = 1.231917e+00\n",
      "0.6781460346701783 0.16111126999953895 0.03002105775600241\n",
      "it 1, train loss = 1.100874e+00\n",
      "1.0078246132458137 3.8864727294872057 0.031615769545595994\n",
      "it 1, train loss = 7.940968e-01\n",
      "0.2348818137968156 0.9188552504281403 0.0160794570904863\n",
      "it 1, train loss = 1.025386e+00\n",
      "0.7092502166626027 0.38449198019839553 0.03002168492636092\n",
      "it 1, train loss = 8.606380e-01\n",
      "0.1713824397057743 1.4999893478143644 0.016297866781489766\n",
      "it 1, train loss = 1.209128e+00\n",
      "0.7173063328541097 0.49032529891219123 0.03002101438344846\n",
      "it 1, train loss = 1.004057e+00\n",
      "1.5177120647788191 12.15383180360626 0.03002130218130859\n",
      "it 1, train loss = 1.096138e+00\n",
      "0.6826600996031397 0.09697660137687418 0.030022025963553887\n",
      "it 1, train loss = 8.587261e-01\n",
      "0.1598105105755763 1.086041372770806 0.0162571585927225\n",
      "it 1, train loss = 9.865784e-01\n",
      "0.6902307393905042 0.10822463311060836 0.03002427249650225\n",
      "it 1, train loss = 9.952861e-01\n",
      "0.6810411335429819 0.12259257785830441 0.030024057580368087\n",
      "it 1, train loss = 9.153948e-01\n",
      "0.7074796705499727 0.3328977704835861 0.0300306421539959\n",
      "it 1, train loss = 1.182718e+00\n",
      "0.6950364077271394 0.16485476232735852 0.030021332491957757\n",
      "it 1, train loss = 9.744285e-01\n",
      "1.5814836525706324 12.551496087090118 0.030020696303562917\n",
      "it 1, train loss = 8.957090e-01\n",
      "0.9562606293069945 3.952589907633424 0.026319721868641344\n",
      "it 1, train loss = 9.939163e-01\n",
      "0.68583304182833 0.058680258602803896 0.03002342011126474\n",
      "it 1, train loss = 8.602431e-01\n",
      "0.32950733923145203 1.7941970028594447 0.016412205662908637\n",
      "16 [1, 2, 1]\n",
      "it 1, train loss = 9.836024e-01\n",
      "0.18494479040091236 0.12114347250206156 0.016208851013050002\n",
      "it 1, train loss = 7.944466e-01\n",
      "0.1800189169146865 0.13409655034099796 0.01612421520742171\n",
      "it 1, train loss = 8.546508e-01\n",
      "0.18350328932272675 0.13467673076120698 0.01612627843671598\n",
      "it 1, train loss = 9.980489e-01\n",
      "0.6871827061713524 0.06572833474007916 0.030022940511303164\n",
      "it 1, train loss = 9.177469e-01\n",
      "0.18286111520920925 0.12254983469322604 0.01617387787600969\n",
      "it 1, train loss = 8.435016e-01\n",
      "0.17855623494470826 0.1278857040664896 0.016142569987541967\n",
      "it 1, train loss = 1.014691e+00\n",
      "0.17750231212867912 0.12685436947007975 0.01614609406207152\n",
      "it 1, train loss = 1.033009e+00\n",
      "0.25488761482669725 0.1026324174525456 0.016312713351443364\n",
      "it 1, train loss = 8.642069e-01\n",
      "0.1778581250449767 0.12562613999874425 0.016145019309601584\n",
      "it 1, train loss = 8.694502e-01\n",
      "0.18212394410154276 0.11947800143398085 0.016181909039871458\n",
      "it 1, train loss = 9.219491e-01\n",
      "0.4166254653741811 0.10049043352828925 0.016447878316972024\n",
      "it 1, train loss = 1.099152e+00\n",
      "0.6963455381331913 0.1935633066734512 0.03002707209049741\n",
      "it 1, train loss = 1.181537e+00\n",
      "0.17918429483852097 0.12101227898825506 0.01618656907937215\n",
      "it 1, train loss = 1.016262e+00\n",
      "0.1798061304719289 0.12888903254648276 0.016151225755513428\n",
      "it 1, train loss = 9.631192e-01\n",
      "0.18069507184254463 0.1333509511905335 0.016124153648709953\n",
      "it 1, train loss = 8.788098e-01\n",
      "0.1808407191630087 0.13073814634544842 0.01613866964985123\n",
      "it 1, train loss = 9.172755e-01\n",
      "0.2807729188718888 0.10722560612740406 0.01634780021020803\n",
      "it 1, train loss = 8.245998e-01\n",
      "0.1877933637958191 0.12210052646527965 0.01619316242174739\n",
      "it 1, train loss = 9.587689e-01\n",
      "0.2144816245870614 0.11526783813742934 0.01625078069956345\n",
      "it 1, train loss = 1.027502e+00\n",
      "0.17989352085321314 0.11796223617175278 0.01618535487133051\n",
      "it 1, train loss = 7.915443e-01\n",
      "0.18205656601937656 0.13087807859522302 0.016126865922610777\n",
      "it 1, train loss = 1.050197e+00\n",
      "0.6435372241698292 0.11093439600814559 0.016592713425378294\n",
      "it 1, train loss = 8.123466e-01\n",
      "0.6831318720705525 0.08734886906988684 0.030021856161608407\n",
      "it 1, train loss = 8.791243e-01\n",
      "0.190090326193443 0.12444540285981004 0.016189333391083905\n",
      "it 1, train loss = 1.103103e+00\n",
      "0.5324534715457844 0.10544931631586328 0.016547991229847635\n",
      "it 1, train loss = 8.492544e-01\n",
      "0.18232911104741992 0.12631870850152332 0.01617392819930158\n",
      "it 1, train loss = 7.464996e-01\n",
      "0.1783065311616185 0.1203815956889498 0.01611696130934821\n",
      "it 1, train loss = 9.716305e-01\n",
      "0.18194973566677697 0.11939465685772674 0.01618885910482714\n",
      "it 1, train loss = 9.688053e-01\n",
      "0.7279637465559402 0.1480699186565391 0.016428008889755225\n",
      "it 1, train loss = 9.321143e-01\n",
      "0.17863900953472 0.12727783349971683 0.01612518164839266\n",
      "it 1, train loss = 1.121691e+00\n",
      "0.7232574511171233 1.635408184279615 0.027830709519678334\n",
      "it 1, train loss = 9.800489e-01\n",
      "0.8373278032547877 0.12075183448674037 0.016480461524958294\n",
      "it 1, train loss = 1.003037e+00\n",
      "0.17888311682560812 0.12563435536325684 0.016138722503596995\n",
      "it 1, train loss = 9.067428e-01\n",
      "0.31249516511540903 0.09937480759181333 0.016375874456657894\n",
      "it 1, train loss = 9.347314e-01\n",
      "0.48965072138328836 0.11207913654193208 0.0164712321914231\n",
      "it 1, train loss = 8.844255e-01\n",
      "0.6903941674549466 0.10655021432765052 0.03002197373847335\n",
      "it 1, train loss = 1.024874e+00\n",
      "0.2581213327318985 0.10878137138799783 0.016303813408494794\n",
      "it 1, train loss = 1.072893e+00\n",
      "0.7007384215951659 0.2454648220283025 0.030030551497449327\n",
      "it 1, train loss = 9.144908e-01\n",
      "0.18195313534030355 0.11147953517788715 0.01619612970772661\n",
      "it 1, train loss = 9.897696e-01\n",
      "0.34158136804315775 0.0992145208861813 0.016396273264424756\n",
      "it 1, train loss = 8.391050e-01\n",
      "0.17812427407750137 0.126130835718751 0.01614438518991115\n",
      "it 1, train loss = 9.723565e-01\n",
      "0.30991426204378475 0.09898284517597437 0.016360391150162\n",
      "it 1, train loss = 1.066981e+00\n",
      "0.5771702299798571 0.10638383947116532 0.016551427094402042\n",
      "it 1, train loss = 9.438350e-01\n",
      "0.18608840134113294 0.10784382766352078 0.016216518867285444\n",
      "it 1, train loss = 8.754443e-01\n",
      "0.36189341936544905 0.10723175627201655 0.016411483458372428\n",
      "it 1, train loss = 1.065458e+00\n",
      "0.28912759756654544 0.10227421549562589 0.016993162194712315\n",
      "it 1, train loss = 8.963452e-01\n",
      "0.17972835797839865 0.12730889131780448 0.01614320551664671\n",
      "it 1, train loss = 9.248415e-01\n",
      "0.6835957470577454 0.08094138275218331 0.03002385595536789\n",
      "it 1, train loss = 8.569524e-01\n",
      "0.17999980647827316 0.1324301934878183 0.016129359500193296\n",
      "it 1, train loss = 1.016208e+00\n",
      "0.1854485973447972 0.11323482400427325 0.016222186178181715\n",
      "22 [1, 3, 1]\n",
      "it 1, train loss = 9.556679e-01\n",
      "0.17980080072905252 0.12744914057840304 0.01615371138635254\n",
      "it 1, train loss = 8.991704e-01\n",
      "0.1790489910383279 0.12878718947175702 0.016143091188760693\n",
      "it 1, train loss = 1.067458e+00\n",
      "0.2563368260613378 0.10309268922558293 0.01633867781347788\n",
      "it 1, train loss = 9.480195e-01\n",
      "0.17963431788427786 0.12783003861449446 0.016150825952760687\n",
      "it 1, train loss = 9.030802e-01\n",
      "0.18170881643526915 0.1252966043848518 0.016163566325103008\n",
      "it 1, train loss = 9.541481e-01\n",
      "0.1793762530823952 0.1283529731338004 0.016136612919514768\n",
      "it 1, train loss = 8.864620e-01\n",
      "0.1797927137932122 0.1245894992009491 0.016154656779487245\n",
      "it 1, train loss = 9.089243e-01\n",
      "0.18019541199451022 0.1331918635194897 0.016145732473532536\n",
      "it 1, train loss = 9.278242e-01\n",
      "0.2006416681645344 0.11219617219256584 0.016241934368381446\n",
      "it 1, train loss = 1.004610e+00\n",
      "0.19759690530476043 0.10927547273710693 0.016242388516913245\n",
      "it 1, train loss = 9.668503e-01\n",
      "0.17906900001294607 0.12777106371169278 0.016144291029841298\n",
      "it 1, train loss = 1.003971e+00\n",
      "0.17962444627508536 0.12922936650513897 0.016152469208672027\n",
      "it 1, train loss = 1.018828e+00\n",
      "0.17774302241041923 0.12324617915548852 0.01616974341099981\n",
      "it 1, train loss = 1.052421e+00\n",
      "0.17864252193116348 0.1237313628308823 0.016163337825142617\n",
      "it 1, train loss = 1.014361e+00\n",
      "0.27472614677828916 0.10056531601484735 0.016334854908196566\n",
      "it 1, train loss = 8.985644e-01\n",
      "0.17714791782217318 0.11540754842231626 0.016159926938992767\n",
      "it 1, train loss = 1.036531e+00\n",
      "0.21575091002234095 0.11742040430282423 0.016278824720390096\n",
      "it 1, train loss = 9.377949e-01\n",
      "0.17728974857270188 0.12089784426319015 0.01615178025868579\n",
      "it 1, train loss = 1.093228e+00\n",
      "0.18526284759525233 0.11739920802627914 0.016232167263689024\n",
      "it 1, train loss = 8.875697e-01\n",
      "0.17937318411913963 0.12728065405761543 0.01614434016470487\n",
      "it 1, train loss = 9.008007e-01\n",
      "0.17741052739239255 0.12198041448881129 0.016149156898214705\n",
      "it 1, train loss = 1.007121e+00\n",
      "0.17918727205755733 0.1261393376495816 0.01622919009510198\n",
      "it 1, train loss = 8.049349e-01\n",
      "0.18259752511397834 0.13286008987331124 0.016131740628881997\n",
      "it 1, train loss = 9.578199e-01\n",
      "0.17770677964603487 0.11997756074962931 0.01615086693256202\n",
      "it 1, train loss = 7.317076e-01\n",
      "0.17891331297475477 0.12684508511055825 0.016128300418785206\n",
      "it 1, train loss = 9.149610e-01\n",
      "0.17795365767143415 0.125596835915451 0.016154447138831494\n",
      "it 1, train loss = 9.390706e-01\n",
      "0.17683104721570797 0.11530068544430569 0.016161344621915975\n",
      "it 1, train loss = 9.432783e-01\n",
      "0.4826892692768023 0.10586273245427566 0.016477387219142523\n",
      "it 1, train loss = 8.879085e-01\n",
      "0.1846514361108427 0.1326467063960223 0.016130045871499733\n",
      "it 1, train loss = 9.772009e-01\n",
      "0.17976060516544973 0.1285135482632884 0.01614538566863246\n",
      "it 1, train loss = 9.591694e-01\n",
      "0.18110949310661253 0.13252997684254153 0.016143547714765272\n",
      "it 1, train loss = 8.742272e-01\n",
      "0.18040745103340683 0.12217171444451282 0.01615519925041664\n",
      "it 1, train loss = 8.671986e-01\n",
      "0.17991163150096576 0.12954095363052015 0.016139260553531907\n",
      "it 1, train loss = 9.309308e-01\n",
      "0.1823513689545706 0.11581831530216971 0.016193930873000608\n",
      "it 1, train loss = 1.051042e+00\n",
      "0.18025127402008362 0.12065527632015872 0.016187123814270076\n",
      "it 1, train loss = 9.821862e-01\n",
      "0.4153719218845775 0.10201393409904312 0.016453467824740553\n",
      "it 1, train loss = 9.240201e-01\n",
      "0.17755031726538054 0.12403725505117341 0.016144248418707225\n",
      "it 1, train loss = 9.199097e-01\n",
      "0.17828356793726796 0.12064664581860658 0.01615289032071438\n",
      "it 1, train loss = 1.033469e+00\n",
      "0.1843089030495503 0.1209582541543131 0.016211925283920407\n",
      "it 1, train loss = 1.017675e+00\n",
      "0.18287755740002992 0.12174136728204135 0.01618582556319608\n",
      "it 1, train loss = 9.824365e-01\n",
      "0.179417989344565 0.1309032769331115 0.016140123120745843\n",
      "it 1, train loss = 1.025793e+00\n",
      "0.1797542073283447 0.12687728486683672 0.016157712221767925\n",
      "it 1, train loss = 1.042344e+00\n",
      "0.1797505678424546 0.11726953548041104 0.016174076532489066\n",
      "it 1, train loss = 1.059003e+00\n",
      "0.1790649215009742 0.12913681532258642 0.016152379616173\n",
      "it 1, train loss = 9.329459e-01\n",
      "0.17936454873800345 0.1193285124562368 0.01616380193611569\n",
      "it 1, train loss = 8.966317e-01\n",
      "0.18529680544198346 0.13622062633056747 0.01614319216152028\n",
      "it 1, train loss = 1.008432e+00\n",
      "0.178709486654922 0.12548109023230167 0.01615346937499964\n",
      "it 1, train loss = 8.962506e-01\n",
      "0.452725298277813 0.10086906887906094 0.01646875804235119\n",
      "it 1, train loss = 9.688631e-01\n",
      "0.1794724199050729 0.12033642250822281 0.016174695154011078\n",
      "it 1, train loss = 1.059290e+00\n",
      "0.18530746412283963 0.11293428129065713 0.016276582253507196\n",
      "28 [1, 4, 1]\n",
      "it 1, train loss = 9.434273e-01\n",
      "0.17819255782652368 0.12429761355213381 0.01616126598076667\n",
      "it 1, train loss = 9.454185e-01\n",
      "0.1882576316594639 0.12157038268142542 0.016211285492845312\n",
      "it 1, train loss = 1.014055e+00\n",
      "0.18223755219639462 0.11583483624788281 0.016190135591299594\n",
      "it 1, train loss = 8.765184e-01\n",
      "0.17984835070286906 0.1296981639345576 0.016145051424726532\n",
      "it 1, train loss = 9.627249e-01\n",
      "0.17817479805441264 0.12052656032549859 0.01615870566125351\n",
      "it 1, train loss = 8.414989e-01\n",
      "0.17849387839616568 0.12704705257283902 0.01614479566236054\n",
      "it 1, train loss = 9.307849e-01\n",
      "0.17888078240541516 0.12805953730128208 0.016145828227086875\n",
      "it 1, train loss = 9.387001e-01\n",
      "0.1798320554983421 0.12836001895327176 0.016138044250504077\n",
      "it 1, train loss = 9.537533e-01\n",
      "0.17861072732661282 0.1261793913707045 0.016152660887822332\n",
      "it 1, train loss = 9.111004e-01\n",
      "0.17985192630465463 0.12822560513556663 0.01614678690102324\n",
      "it 1, train loss = 9.885050e-01\n",
      "0.1799572495120139 0.12552155260733566 0.01646286776295047\n",
      "it 1, train loss = 9.421989e-01\n",
      "0.201148479010773 0.11508969504488029 0.01623868055137678\n",
      "it 1, train loss = 9.914297e-01\n",
      "0.17767818392641643 0.1149694862956733 0.016180685399488658\n",
      "it 1, train loss = 9.386943e-01\n",
      "0.17852375934618153 0.12660109659435537 0.0161583348900649\n",
      "it 1, train loss = 9.728261e-01\n",
      "0.18921826362683625 0.12178441388458427 0.01621030685328094\n",
      "it 1, train loss = 1.047252e+00\n",
      "0.3939755080494926 0.09898671846535156 0.016440860998101436\n",
      "it 1, train loss = 9.276220e-01\n",
      "0.17769455243691326 0.12182060064505557 0.01614996954008898\n",
      "it 1, train loss = 9.575876e-01\n",
      "0.17722163058441612 0.12368496320534146 0.016158971497556053\n",
      "it 1, train loss = 9.064255e-01\n",
      "0.1792488595825931 0.12468264565586556 0.01615184122657559\n",
      "it 1, train loss = 9.857058e-01\n",
      "0.17892484805963849 0.13065109014235537 0.016148885181330678\n",
      "it 1, train loss = 1.130535e+00\n",
      "0.18080650882365062 0.11219030938238014 0.01620435799161369\n",
      "it 1, train loss = 1.029531e+00\n",
      "0.17885780249120806 0.12407652906886935 0.016175081413070447\n",
      "it 1, train loss = 9.914848e-01\n",
      "0.17872436469776182 0.12406896966489528 0.01616374056338157\n",
      "it 1, train loss = 9.214085e-01\n",
      "0.17814583040806375 0.12303099921754257 0.016153601431608017\n",
      "it 1, train loss = 9.255879e-01\n",
      "0.17995915226254447 0.1162172948529291 0.016179505339129985\n",
      "it 1, train loss = 8.971551e-01\n",
      "0.17910764707254756 0.12096253861536903 0.016173480050349135\n",
      "it 1, train loss = 9.665117e-01\n",
      "0.18150414112973262 0.11844186479622483 0.016261147318084264\n",
      "it 1, train loss = 9.570746e-01\n",
      "0.17956282653153827 0.13055628393147234 0.016148705663663096\n",
      "it 1, train loss = 9.357761e-01\n",
      "0.17815665764161762 0.11893675356868612 0.016159402779487866\n",
      "it 1, train loss = 9.515354e-01\n",
      "0.17803716130080927 0.12509577803415417 0.01616450193564767\n",
      "it 1, train loss = 1.035043e+00\n",
      "0.17805377327187613 0.11097977127594891 0.01619127493753045\n",
      "it 1, train loss = 9.794608e-01\n",
      "0.17741946847503257 0.12613552509982953 0.01615354872039327\n",
      "it 1, train loss = 1.066402e+00\n",
      "0.18513433382617414 0.10949196412059846 0.016279588342401723\n",
      "it 1, train loss = 8.566136e-01\n",
      "0.1790707699495842 0.12730207993145856 0.016147876360297807\n",
      "it 1, train loss = 9.005991e-01\n",
      "0.17808469115005907 0.12490880586448494 0.016218023706650576\n",
      "it 1, train loss = 9.304843e-01\n",
      "0.17793022111100854 0.1225669974270678 0.016160255214249786\n",
      "it 1, train loss = 1.057306e+00\n",
      "0.1789089225032773 0.12574773306423714 0.016162897792464652\n",
      "it 1, train loss = 9.365095e-01\n",
      "0.17966491782008043 0.12758687789848533 0.01615326512531267\n",
      "it 1, train loss = 9.176455e-01\n",
      "0.1777859479492403 0.12237018103958466 0.01615721351556974\n",
      "it 1, train loss = 9.174405e-01\n",
      "0.17743136726128456 0.119107168532422 0.01615741523555226\n",
      "it 1, train loss = 1.071144e+00\n",
      "0.23018148714583492 0.11182088283155359 0.016300478569295323\n",
      "it 1, train loss = 9.683112e-01\n",
      "0.17940751237544186 0.11842412547780988 0.016173459521750443\n",
      "it 1, train loss = 9.875341e-01\n",
      "0.1784274481773941 0.12348627017241665 0.016156259931459634\n",
      "it 1, train loss = 9.009235e-01\n",
      "0.17801335992760456 0.12090599688888484 0.016164753091556412\n",
      "it 1, train loss = 9.879690e-01\n",
      "0.1793244027247592 0.1279588124194119 0.01615440198340645\n",
      "it 1, train loss = 9.616996e-01\n",
      "0.1872358088709129 0.11180310867307923 0.016215750401665337\n",
      "it 1, train loss = 1.076359e+00\n",
      "0.2572326665358666 0.0979312647575145 0.016340642889737994\n",
      "it 1, train loss = 1.075687e+00\n",
      "0.25378385035921247 0.10085369529748471 0.01633992274428426\n",
      "it 1, train loss = 1.029892e+00\n",
      "0.17788473696606122 0.11763186873494687 0.016190155363997597\n",
      "it 1, train loss = 9.531531e-01\n",
      "0.17809025454736208 0.11349168300792734 0.016179129348718613\n",
      "32 [1, 2, 2, 1]\n",
      "it 1, train loss = 9.315447e-01\n",
      "0.8352899285667493 4.931293990771491 0.016110602900931408\n",
      "it 1, train loss = 1.062474e+00\n",
      "0.7146997314415412 4.918832521184627 0.015986463414243434\n",
      "it 1, train loss = 9.047538e-01\n",
      "0.6691091732258175 4.62696095251837 0.016008816986399815\n",
      "it 1, train loss = 1.015581e+00\n",
      "0.6770333171493061 4.704522937000747 0.01603085545982957\n",
      "it 1, train loss = 9.253519e-01\n",
      "0.7045387450679413 4.8003240728611 0.01598721709476492\n",
      "it 1, train loss = 1.015438e+00\n",
      "0.5749289745789102 5.046663328023065 0.01612942132479809\n",
      "it 1, train loss = 9.238840e-01\n",
      "0.7338192080212022 4.299427104327629 0.015958623403085122\n",
      "it 1, train loss = 8.367957e-01\n",
      "0.7992679322900045 3.8963775022628995 0.015919503777809103\n",
      "it 1, train loss = 8.902347e-01\n",
      "0.7927886537988824 4.691605917605173 0.016140290503013296\n",
      "it 1, train loss = 9.161088e-01\n",
      "0.7578759254459498 4.42222388409449 0.015943748826202576\n",
      "it 1, train loss = 8.002129e-01\n",
      "0.7009936139175207 4.092959017534514 0.015982171726080744\n",
      "it 1, train loss = 8.447486e-01\n",
      "0.8561658342595132 13945.212577828537 0.016994639368032343\n",
      "it 1, train loss = 9.457015e-01\n",
      "0.7667850597727843 4.86856794425194 0.01595923650373719\n",
      "it 1, train loss = 9.385733e-01\n",
      "2.8263892785334757 0.1639232442302061 0.017355328655923246\n",
      "it 1, train loss = 9.542806e-01\n",
      "0.7017909167578463 5.052460493696527 0.016006616567648724\n",
      "it 1, train loss = 9.344436e-01\n",
      "0.6610074960243941 4.3418964032077705 0.01600903008015808\n",
      "it 1, train loss = 7.646053e-01\n",
      "0.7496828194569198 3.925773720861448 0.015949215640765853\n",
      "it 1, train loss = 8.067857e-01\n",
      "0.6234971093523154 4.457836189776271 0.016319561129308133\n",
      "it 1, train loss = 1.158085e+00\n",
      "0.6669907820988615 5.691541308744141 0.016040095285729097\n",
      "it 1, train loss = 9.765012e-01\n",
      "0.7374033832999862 4.584993500043142 0.015953799494714968\n",
      "it 1, train loss = 1.018321e+00\n",
      "0.7411478010946666 4.528488541458546 0.015956526969941293\n",
      "it 1, train loss = 1.057278e+00\n",
      "0.9976988646299645 53.09952734282691 0.028968347759500856\n",
      "it 1, train loss = 8.613367e-01\n",
      "0.3953420204858358 4.529300614246162 0.01637249632193768\n",
      "it 1, train loss = 9.188534e-01\n",
      "0.8541920160993607 8277.73544116023 0.016796348677621198\n",
      "it 1, train loss = 1.055349e+00\n",
      "1.1529505547079528 1.5126803305066665 0.01612253271290073\n",
      "it 1, train loss = 9.425795e-01\n",
      "0.7261954926447003 4.19443874064674 0.015959455194101947\n",
      "it 1, train loss = 8.274182e-01\n",
      "0.48818131232703266 4.4777109235932775 0.016201447247056588\n",
      "it 1, train loss = 7.569099e-01\n",
      "0.7606431808594769 3.7535538042200893 0.01592337017942817\n",
      "it 1, train loss = 9.425340e-01\n",
      "0.43285839450323665 1.0239401033591187 0.01656134467844163\n",
      "it 1, train loss = 9.896122e-01\n",
      "0.7164602645037893 4.973003292043624 0.01602334536585363\n",
      "it 1, train loss = 9.846960e-01\n",
      "0.8754658067296817 1075.498644974923 0.024929049015808176\n",
      "it 1, train loss = 8.546693e-01\n",
      "0.5940394219395138 4.788299153652155 0.01610378966138478\n",
      "it 1, train loss = 8.758140e-01\n",
      "0.6638176437418687 4.445106159335361 0.01602137962350005\n",
      "it 1, train loss = 1.028906e+00\n",
      "1.4090245043027008 0.11981543789098667 0.016237957394678527\n",
      "it 1, train loss = 1.081261e+00\n",
      "0.7885931228836633 4.355739067445224 0.015947449523682128\n",
      "it 1, train loss = 9.409900e-01\n",
      "0.8543478436796305 11481.412527277573 0.01667050118533375\n",
      "it 1, train loss = 9.980046e-01\n",
      "0.510180831199878 5.190376836141269 0.016184264855485193\n",
      "it 1, train loss = 1.068437e+00\n",
      "0.6874279361368514 5.0098885880392725 0.015995505680815546\n",
      "it 1, train loss = 9.785045e-01\n",
      "0.7622512458182655 4.3248388395030055 0.015934113516990953\n",
      "it 1, train loss = 9.971742e-01\n",
      "1.4430009223715468 0.10781697746001061 0.016164934692102836\n",
      "it 1, train loss = 1.050754e+00\n",
      "0.9816382336065169 5.718398854426674 0.016125707728106498\n",
      "it 1, train loss = 9.777669e-01\n",
      "0.3722822257061296 1.6213768673704978 0.016568269081423927\n",
      "it 1, train loss = 1.099535e+00\n",
      "1.7883910743233782 1.3433843852765561 0.016359448351705195\n",
      "it 1, train loss = 6.648349e-01\n",
      "0.7731852383285707 3.6072880518570076 0.015940889860115992\n",
      "it 1, train loss = 1.115328e+00\n",
      "0.7599502532984346 5.01517361817735 0.01595040980438746\n",
      "it 1, train loss = 9.288861e-01\n",
      "0.7067668136073013 4.6392440378757644 0.015989760562821236\n",
      "it 1, train loss = 9.365987e-01\n",
      "0.7864618866466864 4.169623012457601 0.015955093204837494\n",
      "it 1, train loss = 8.595749e-01\n",
      "0.7482779501055669 3.987778363593852 0.01595133878005337\n",
      "it 1, train loss = 9.118864e-01\n",
      "0.6922653027129537 4.895048667614067 0.01602154098295654\n",
      "it 1, train loss = 9.102515e-01\n",
      "0.49752081676189375 4.831919573063674 0.016185422176828775\n",
      "40 [1 3 2 1]\n",
      "it 1, train loss = 1.001359e+00\n",
      "0.7077614171572643 5.4026474876718105 0.016020096973801447\n",
      "it 1, train loss = 8.553764e-01\n",
      "0.7124058906095434 4.186054510043802 0.015963959270402096\n",
      "it 1, train loss = 1.101726e+00\n",
      "0.7864888969578646 4.552756084092437 0.015943489755639167\n",
      "it 1, train loss = 9.314096e-01\n",
      "0.8584508976098042 13086.869106750342 0.017892658142371055\n",
      "it 1, train loss = 1.059114e+00\n",
      "0.7372798844166004 4.951465452424795 0.01597284875363932\n",
      "it 1, train loss = 9.553916e-01\n",
      "0.6764525108138926 5.140962839302562 0.016028716132319737\n",
      "it 1, train loss = 9.683039e-01\n",
      "0.6871248557699491 5.296776406134311 0.016037079720600717\n",
      "it 1, train loss = 1.113227e+00\n",
      "0.7055426481750561 5.601667252301447 0.0160092278298625\n",
      "it 1, train loss = 8.347389e-01\n",
      "0.6752448244724151 4.641343682274796 0.016025479099653305\n",
      "it 1, train loss = 8.026828e-01\n",
      "0.7436558622175008 4.578788333892636 0.01597447236717461\n",
      "it 1, train loss = 9.889940e-01\n",
      "0.6621677816688708 5.154136431985518 0.016045042611178763\n",
      "it 1, train loss = 9.562943e-01\n",
      "0.8003420107775253 4.6598716664176605 0.01594102239470383\n",
      "it 1, train loss = 9.555174e-01\n",
      "0.6894965256012603 4.780368836407161 0.015997211031053143\n",
      "it 1, train loss = 9.617893e-01\n",
      "0.6810845142509752 4.788434645623052 0.016002268954611164\n",
      "it 1, train loss = 1.037959e+00\n",
      "0.7710064218152636 4.957277495574168 0.015956304021544774\n",
      "it 1, train loss = 8.352129e-01\n",
      "0.7302205768815682 4.157200378374995 0.01595834334061586\n",
      "it 1, train loss = 8.882379e-01\n",
      "0.721185928569199 4.587202811974361 0.015979377949811715\n",
      "it 1, train loss = 1.022748e+00\n",
      "0.6524863708183737 5.440415861476075 0.016078610088739703\n",
      "it 1, train loss = 9.981704e-01\n",
      "0.4799907563649136 4.775676841153304 0.01619291301183602\n",
      "it 1, train loss = 9.421602e-01\n",
      "0.853632663279773 5892.384297880599 0.01632372132323532\n",
      "it 1, train loss = 9.579289e-01\n",
      "0.7884768261056709 4.879420679761866 0.01596520228532612\n",
      "it 1, train loss = 9.881819e-01\n",
      "0.790029870491227 4.955099028751853 0.015949638151843346\n",
      "it 1, train loss = 1.032641e+00\n",
      "0.5999732527967149 5.138823638338514 0.016090447267351825\n",
      "it 1, train loss = 8.459753e-01\n",
      "0.7337415487294902 5.0264258572596034 0.015985374865200696\n",
      "it 1, train loss = 8.547118e-01\n",
      "0.39620605774425727 4.570685856692264 0.016275274136461314\n",
      "it 1, train loss = 8.145082e-01\n",
      "0.722043792761849 4.395791550758976 0.015981113036530683\n",
      "it 1, train loss = 9.833271e-01\n",
      "0.6219433412810781 5.34466888245443 0.016089090699735863\n",
      "it 1, train loss = 9.311701e-01\n",
      "0.5497153188043895 5.2176489379138165 0.016162664295125296\n",
      "it 1, train loss = 1.046927e+00\n",
      "0.6901819303676449 5.237659249142655 0.01601134151459519\n",
      "it 1, train loss = 9.602964e-01\n",
      "0.607970562729964 4.991241367344072 0.016076397335674116\n",
      "it 1, train loss = 9.373965e-01\n",
      "0.7250037027583776 4.974868756075534 0.01599961930156914\n",
      "it 1, train loss = 8.960941e-01\n",
      "0.7145343127565223 4.704188180298025 0.01599275917596266\n",
      "it 1, train loss = 8.886994e-01\n",
      "0.7643125428081784 4.698639834275734 0.015960530208797192\n",
      "it 1, train loss = 8.979587e-01\n",
      "0.6004969449158926 4.47884189371417 0.016075460281218205\n",
      "it 1, train loss = 1.057356e+00\n",
      "0.7062319705812805 5.157061202049196 0.016003557269924664\n",
      "it 1, train loss = 8.646497e-01\n",
      "0.7983112289204523 3.9623751089707118 0.01593778826149044\n",
      "it 1, train loss = 8.538596e-01\n",
      "0.6880842820617719 4.650623255364505 0.016008347429896656\n",
      "it 1, train loss = 9.360156e-01\n",
      "0.6962408107158211 5.085599864465906 0.01601376987953068\n",
      "it 1, train loss = 8.570087e-01\n",
      "0.6040054237551454 4.87059100430615 0.016090292530147442\n",
      "it 1, train loss = 1.085542e+00\n",
      "0.5233087785637602 1.8243020902561442 0.01652814154904867\n",
      "it 1, train loss = 8.147131e-01\n",
      "0.689745687110906 4.434689414410492 0.015994199862764562\n",
      "it 1, train loss = 9.624046e-01\n",
      "0.7132386831437085 4.684040205815897 0.01597233193219644\n",
      "it 1, train loss = 8.248634e-01\n",
      "0.5579878837665556 4.555634072349913 0.01615499655060761\n",
      "it 1, train loss = 9.646898e-01\n",
      "0.8135220574588147 4.6961512201536495 0.01594450269816632\n",
      "it 1, train loss = 9.627190e-01\n",
      "0.6540967423123212 4.893657000763036 0.016039630301050486\n",
      "it 1, train loss = 1.033080e+00\n",
      "0.5397250740102472 5.343537449425181 0.01616913320445224\n",
      "it 1, train loss = 1.120581e+00\n",
      "0.9309056984237353 1.0704539993083562 0.016718306975891887\n",
      "it 1, train loss = 1.035752e+00\n",
      "0.5885429363912104 5.586756475829834 0.016148171677909526\n",
      "it 1, train loss = 8.628484e-01\n",
      "0.636440660137209 4.821916426239029 0.01606789097292796\n",
      "it 1, train loss = 9.612393e-01\n",
      "0.7294891746620544 4.75101467540791 0.015964780210640492\n",
      "42 [1 2 3 1]\n",
      "it 1, train loss = 8.571446e-01\n",
      "0.699049944194261 4.375488048685575 0.015984702451316838\n",
      "it 1, train loss = 8.763839e-01\n",
      "0.8560955537160805 11415.31554518046 0.016831309240801933\n",
      "it 1, train loss = 9.867782e-01\n",
      "0.6952550620140632 5.022185846066335 0.01600758798456043\n",
      "it 1, train loss = 8.541845e-01\n",
      "0.8301530618173899 4.641973382072347 0.016110817311530592\n",
      "it 1, train loss = 8.202948e-01\n",
      "0.5188080803085051 4.055960271315515 0.016188948008477734\n",
      "it 1, train loss = 9.330887e-01\n",
      "0.7390690198774663 4.722967110182482 0.015955025441325835\n",
      "it 1, train loss = 9.673041e-01\n",
      "0.8245374945140488 3.737914387856417 0.016073823554876243\n",
      "it 1, train loss = 9.971455e-01\n",
      "0.6865042718406749 5.24379488520564 0.016024773175036266\n",
      "it 1, train loss = 1.052049e+00\n",
      "0.6415028676939072 5.291722023919901 0.0160571101472451\n",
      "it 1, train loss = 1.012510e+00\n",
      "0.6948198525063666 4.867125366044753 0.016002236611894073\n",
      "it 1, train loss = 9.353405e-01\n",
      "0.4983325095396951 4.6430703373395685 0.016186132505764974\n",
      "it 1, train loss = 1.131780e+00\n",
      "0.42645340451693214 5.42726901066858 0.016344177773145268\n",
      "it 1, train loss = 9.841190e-01\n",
      "2.2979899195753886 0.13938725885435002 0.016862231231099607\n",
      "it 1, train loss = 9.616291e-01\n",
      "0.7192686671136076 4.698643214130296 0.01597823825369484\n",
      "it 1, train loss = 9.331635e-01\n",
      "0.7403904491875883 4.809232964329847 0.015977278746171972\n",
      "it 1, train loss = 1.092641e+00\n",
      "1.2287014374244212 23.970782694774474 0.029406631275318094\n",
      "it 1, train loss = 8.578589e-01\n",
      "0.6441522788404059 4.647451501377439 0.016040189395470843\n",
      "it 1, train loss = 1.177312e+00\n",
      "0.7644886907632673 5.43731356205395 0.01596392599497114\n",
      "it 1, train loss = 1.009958e+00\n",
      "0.9158481126332484 177.3273110481216 0.02781771150351748\n",
      "it 1, train loss = 9.671410e-01\n",
      "0.7957439964966092 4.618944294856021 0.016109628157192715\n",
      "it 1, train loss = 9.935331e-01\n",
      "0.7151922656261867 5.356305567662177 0.01602711483645875\n",
      "it 1, train loss = 1.062078e+00\n",
      "2.7532105151095716 0.17169958402801272 0.01694101245492346\n",
      "it 1, train loss = 9.432137e-01\n",
      "0.8374330717899131 4.908747023863175 0.01612034797824663\n",
      "it 1, train loss = 1.013817e+00\n",
      "0.6340666257339803 4.6624246003974505 0.01604927715596052\n",
      "it 1, train loss = 8.768826e-01\n",
      "1.1673636803191882 1.1104069449983296 0.016317016350880013\n",
      "it 1, train loss = 9.991514e-01\n",
      "1.5973759304556483 0.11973152800820823 0.0162893928220638\n",
      "it 1, train loss = 8.991772e-01\n",
      "1.2220436801409646 0.21106759340916326 0.016193972201064655\n",
      "it 1, train loss = 8.618000e-01\n",
      "0.7480440387514707 4.2367902488378295 0.016106175082723998\n",
      "it 1, train loss = 7.914356e-01\n",
      "0.8645063124851723 3562.791323578189 0.019058488810207448\n",
      "it 1, train loss = 1.049898e+00\n",
      "0.6779835382433937 4.94969189227184 0.01602419805429677\n",
      "it 1, train loss = 9.446694e-01\n",
      "0.7262797352790414 4.575958331117684 0.01596962919525083\n",
      "it 1, train loss = 1.008118e+00\n",
      "0.9523512695534825 81.19731356472758 0.028573799184817665\n",
      "it 1, train loss = 9.745140e-01\n",
      "0.7177280042579873 4.934854116674785 0.01602568168874223\n",
      "it 1, train loss = 1.004715e+00\n",
      "15.165577835948156 0.6575515121502107 0.01710045689142025\n",
      "it 1, train loss = 8.710966e-01\n",
      "0.699662131999489 4.706902218091734 0.016024755644342577\n",
      "it 1, train loss = 9.598671e-01\n",
      "0.6755526183826245 5.235261255106637 0.016040400870506528\n",
      "it 1, train loss = 8.896595e-01\n",
      "0.6958548486787657 4.502893598801544 0.015989640287120004\n",
      "it 1, train loss = 1.122600e+00\n",
      "0.7600574547255641 5.240208553842945 0.01596006171432108\n",
      "it 1, train loss = 8.428477e-01\n",
      "0.8544340599071208 17924.40257593377 0.016878272519652883\n",
      "it 1, train loss = 9.714244e-01\n",
      "0.7517795899531543 5.152939377426444 0.015977875333509077\n",
      "it 1, train loss = 9.500027e-01\n",
      "0.7869035094273806 4.506060341811911 0.016109058825585686\n",
      "it 1, train loss = 9.006331e-01\n",
      "0.7273073330877114 5.012590431458245 0.016005531759445916\n",
      "it 1, train loss = 9.015434e-01\n",
      "0.6136128070570244 4.811682442589299 0.016099343518670345\n",
      "it 1, train loss = 9.275060e-01\n",
      "0.8389941201464443 4.946270201994333 0.01611387413117502\n",
      "it 1, train loss = 8.841332e-01\n",
      "0.7194506769625129 4.615429570766395 0.01598429694650206\n",
      "it 1, train loss = 1.073738e+00\n",
      "2.4093594973462285 0.5521420517293697 0.016804416557150807\n",
      "it 1, train loss = 1.019234e+00\n",
      "0.6627294016419256 5.289572029769148 0.016069540710135596\n",
      "it 1, train loss = 1.083409e+00\n",
      "2.3956167635087824 0.13562310647197687 0.01666325659419681\n",
      "it 1, train loss = 1.034634e+00\n",
      "0.6563178663225874 5.341616760719039 0.01605252095894313\n",
      "it 1, train loss = 9.850074e-01\n",
      "0.6479208117617492 5.009009517137411 0.016046395149823707\n",
      "52 [1, 3, 3, 1]\n",
      "it 1, train loss = 1.005013e+00\n",
      "0.6555877907351708 5.298157187449421 0.016045970287785347\n",
      "it 1, train loss = 9.341550e-01\n",
      "0.7057848879398644 5.090356217610912 0.016010907651616023\n",
      "it 1, train loss = 1.132104e+00\n",
      "0.7521278114839457 5.445858213211823 0.01600206004830379\n",
      "it 1, train loss = 9.832734e-01\n",
      "0.6624304909447641 5.3099777267740365 0.016046712702525744\n",
      "it 1, train loss = 8.830044e-01\n",
      "0.5513405619816928 5.0461885312708485 0.016166142477162367\n",
      "it 1, train loss = 9.864851e-01\n",
      "0.7592006785000653 5.181001468555257 0.015980671725371747\n",
      "it 1, train loss = 8.542760e-01\n",
      "0.7030940987225881 4.619463375113063 0.015995814673919886\n",
      "it 1, train loss = 8.656426e-01\n",
      "0.5893341426577321 4.621570887181628 0.01609621874481611\n",
      "it 1, train loss = 8.359044e-01\n",
      "0.48595548784149256 4.9002677517148125 0.016218088207966486\n",
      "it 1, train loss = 9.997692e-01\n",
      "0.6840293737192888 5.339212972453675 0.016033376143193053\n",
      "it 1, train loss = 1.031599e+00\n",
      "0.8370206777260262 5.053335580772244 0.01592712267993024\n",
      "it 1, train loss = 9.762451e-01\n",
      "0.7829602989235476 4.835060948164432 0.01596559007320361\n",
      "it 1, train loss = 9.218432e-01\n",
      "0.7032561661056559 5.094814976952385 0.016039519367008145\n",
      "it 1, train loss = 1.085652e+00\n",
      "0.8228269940720715 5.357868088671084 0.015934682363353936\n",
      "it 1, train loss = 9.281180e-01\n",
      "0.6921963647363945 4.82596077926514 0.01601093498305237\n",
      "it 1, train loss = 1.021871e+00\n",
      "0.6823482932129713 5.028115877828643 0.016052972546413073\n",
      "it 1, train loss = 9.477417e-01\n",
      "0.5744519887762732 5.4619761191305445 0.016158624451793602\n",
      "it 1, train loss = 9.770495e-01\n",
      "0.7173983314846123 5.2337068421799975 0.016007530507695208\n",
      "it 1, train loss = 8.685227e-01\n",
      "0.8068455592921043 4.065745298563507 0.0159359368089722\n",
      "it 1, train loss = 1.049380e+00\n",
      "0.7521527361441468 5.259090752326044 0.01597183872877384\n",
      "it 1, train loss = 9.619265e-01\n",
      "0.465008857602061 5.411611486995944 0.016242730994894882\n",
      "it 1, train loss = 8.344792e-01\n",
      "0.6342241555250889 4.9455430168862 0.016076620339778518\n",
      "it 1, train loss = 1.005649e+00\n",
      "0.6414857348898738 5.215182310853055 0.016052054943411706\n",
      "it 1, train loss = 9.464840e-01\n",
      "0.672345931293951 5.1484193224021775 0.01603728707242708\n",
      "it 1, train loss = 9.440285e-01\n",
      "0.7802017480683806 4.496999001912517 0.015941031560063715\n",
      "it 1, train loss = 9.826394e-01\n",
      "0.7497242922349298 4.305448088693298 0.015956081142944955\n",
      "it 1, train loss = 9.693656e-01\n",
      "0.6084664971293647 4.936988701054005 0.016128020169509184\n",
      "it 1, train loss = 9.366160e-01\n",
      "0.721681293617135 4.669273072571161 0.01600939067129392\n",
      "it 1, train loss = 9.611495e-01\n",
      "0.6978154437310422 5.409966781161432 0.01603079674027056\n",
      "it 1, train loss = 8.888559e-01\n",
      "0.6999956925661195 4.914569929935244 0.016008363406152352\n",
      "it 1, train loss = 9.701003e-01\n",
      "0.690985585759015 5.242838456706458 0.01601671872800644\n",
      "it 1, train loss = 9.465218e-01\n",
      "0.7307124439645333 4.983550256440136 0.01597551868292336\n",
      "it 1, train loss = 9.611571e-01\n",
      "0.6846073017606028 4.704259869305888 0.01602599035657788\n",
      "it 1, train loss = 8.760807e-01\n",
      "0.7404136257582736 4.824868375603683 0.01598754137117712\n",
      "it 1, train loss = 1.028377e+00\n",
      "0.6257651071910364 5.28373974308974 0.01607753856215489\n",
      "it 1, train loss = 1.137226e+00\n",
      "1.1278823936085298 0.8756695937948523 0.01625751025672215\n",
      "it 1, train loss = 1.040584e+00\n",
      "0.651285473507454 5.364376951542218 0.016055873395034708\n",
      "it 1, train loss = 1.072386e+00\n",
      "0.6666783594026715 5.508808875042622 0.016076326835987986\n",
      "it 1, train loss = 9.231605e-01\n",
      "0.37218224808457784 5.274963254440366 0.01635021076923795\n",
      "it 1, train loss = 1.082207e+00\n",
      "0.6430595495256165 5.579354791993537 0.01606966391986923\n",
      "it 1, train loss = 9.841091e-01\n",
      "0.7083108311679532 5.330059669196297 0.016015512799988484\n",
      "it 1, train loss = 1.015408e+00\n",
      "0.7411956785699448 5.509881382095269 0.016025611313353876\n",
      "it 1, train loss = 1.017363e+00\n",
      "0.7733872940700368 5.334533320068294 0.015975724933016057\n",
      "it 1, train loss = 1.048283e+00\n",
      "0.6505688256144324 5.0285001185094425 0.01603301692886941\n",
      "it 1, train loss = 9.185772e-01\n",
      "0.6890208980608985 4.781912756134913 0.01604729630663796\n",
      "it 1, train loss = 9.596164e-01\n",
      "0.7420057929222525 5.097468067776033 0.015969896650289684\n",
      "it 1, train loss = 1.060482e+00\n",
      "0.7249993460615055 5.385665780869933 0.015991356263928898\n",
      "it 1, train loss = 9.704785e-01\n",
      "0.4510837530457974 5.050087230781308 0.01624321695129555\n",
      "it 1, train loss = 8.618619e-01\n",
      "0.7826810953044264 4.81421143352204 0.01597892084248557\n",
      "it 1, train loss = 1.112089e+00\n",
      "0.7304358555575741 5.583890416590019 0.01600432388812157\n"
     ]
    }
   ],
   "source": [
    "# Next: find the standard deviation of R2 from 50 runs for each case.\n",
    "# Takes more than 3 hours to finish\n",
    "lamUT_vec = np.array(UTdata['F11'])\n",
    "lamET_vec = np.array(ETdata['F11'])\n",
    "lamPS_vec = np.array(PSdata['F11'])\n",
    "np_list = []\n",
    "r2_list_global = []\n",
    "mae_list_global = []\n",
    "for n in range(10,50):\n",
    "    n_params, layers = opt_arch_icnn(n)\n",
    "    if n_params in np_list:\n",
    "        continue\n",
    "    print(n_params, layers)\n",
    "    np_list.append(n_params)\n",
    "\n",
    "    r2_list_arch = []\n",
    "    mae_list_arch = []\n",
    "    for i in range(50): #50 runs for every architecture\n",
    "        key, subkey = random.split(key)\n",
    "        params = init_icnn(key, layers)\n",
    "\n",
    "        opt_init, opt_update, get_params = optimizers.adam(2.e-4) #Original: 2.e-4\n",
    "        opt_state = opt_init(params)\n",
    "        params, train_loss, val_loss = train_jp(loss_P11_PS, 2, F11_data, opt_state, key, nIter = 100000, print_freq=200000)\n",
    "\n",
    "        model = ICNN_model(params[0], params[1], normalization)\n",
    "\n",
    "        lambdas     = [lamUT_vec,       lamET_vec,      lamPS_vec       ]\n",
    "        P11_gt_list = [UTdata['P11'],   ETdata['P11'],  PSdata['P11']   ]\n",
    "        P11funs     = [P11_UT,          P11_ET,         P11_PS          ]\n",
    "\n",
    "        r2 = []\n",
    "        mae = []\n",
    "        for lam, P11_gt, P11fun in zip(lambdas, P11_gt_list, P11funs):\n",
    "            P11 = P11fun(lam, model, normalization)\n",
    "            r2i = r2_score(P11_gt, P11)\n",
    "            r2i = np.clip(r2i, a_min=0.0, a_max=1.0)\n",
    "            r2.append(r2i)\n",
    "\n",
    "            maei = onp.mean(onp.abs(P11_gt-P11))\n",
    "            mae.append(maei)\n",
    "        # r2 = np.mean(np.array(r2))\n",
    "        # print(*r2)\n",
    "        print(*mae)\n",
    "        r2_list_arch.append(r2)\n",
    "        mae_list_arch.append(mae)\n",
    "    r2_list_global.append(r2_list_arch)\n",
    "    mae_list_global.append(mae_list_arch)\n",
    "\n",
    "icnn_r2 = np.array(r2_list_global)\n",
    "icnn_mae = np.array(mae_list_global)\n",
    "with open('savednet/ICNN_r2_efficiency_PS.npy', 'wb') as f:\n",
    "    pickle.dump([np_list, icnn_r2], f)\n",
    "with open('savednet/ICNN_mae_efficiency_PS.npy', 'wb') as f:\n",
    "    pickle.dump([np_list, icnn_mae], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1,4,figsize=(20,4))\n",
    "\n",
    "loads = ['UT', 'ET', 'PS', 'ALL']\n",
    "for i, (ax, load) in enumerate(zip(axes, loads)):\n",
    "    models  = ['CANN',  'ICNN', 'NODE']\n",
    "    markers = ['o',     '^-',   's--' ]\n",
    "    for model, marker in zip(models, markers):\n",
    "        with open('savednet/'+model+'_r2_efficiency_' + load + '.npy', 'rb') as f:\n",
    "            np_list, r2 = pickle.load(f)\n",
    "        \n",
    "        if load=='ALL':\n",
    "            other_id = None\n",
    "        else:\n",
    "            other_id = np.delete(np.array([0,1,2]), i)\n",
    "        \n",
    "        if model=='CANN':\n",
    "            r2_mean = np.mean(r2,axis=0)\n",
    "            r2_stdv = np.std(r2, axis=0)\n",
    "            mean_trn = r2_mean[i]\n",
    "            stdv_trn = r2_stdv[i]\n",
    "            mean_val = np.mean(r2_mean[other_id])\n",
    "            stdv_val = np.mean(r2_stdv[other_id])\n",
    "            ms = 8\n",
    "        else:\n",
    "            r2_mean = np.mean(r2,axis=1)\n",
    "            r2_stdv = np.std(r2, axis=1)\n",
    "            mean_trn = r2_mean[:,i]\n",
    "            stdv_trn = r2_stdv[:,i]\n",
    "            if load == 'ALL':\n",
    "                pass\n",
    "            else:\n",
    "                other_id = np.delete(np.array([0,1,2]), i)\n",
    "                mean_val = np.mean(r2_mean, axis=1)\n",
    "                stdv_val = np.mean(r2_stdv, axis=1)\n",
    "                mean_val = np.squeeze(mean_val)\n",
    "                stdv_val = np.squeeze(stdv_val)\n",
    "                ax.errorbar(np_list, mean_val, stdv_val, elinewidth=0.5, capsize=3.0, fmt=marker, color='b', label=model + ' valid.', markersize=ms)\n",
    "            ms = 5\n",
    "        \n",
    "        ax.errorbar(np_list, mean_trn, stdv_trn, elinewidth=0.5, capsize=3.0, fmt=marker, color='k', label=model + ' train.', markersize=ms)\n",
    "    ax.set(ylim=[0,1.1], xlabel='Number of parameters', ylabel='$R^2$ (Mean)', title=load + ' training')\n",
    "axes[1].legend()\n",
    "fig.savefig('Figures/fig_rubber_efficiency_r2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1,4,figsize=(20,4))\n",
    "\n",
    "loads = ['UT', 'ET', 'PS', 'ALL']\n",
    "for i, (ax, load) in enumerate(zip(axes, loads)):\n",
    "    models  = ['CANN',  'ICNN', 'NODE']\n",
    "    markers = ['o',     '^-',   's--' ]\n",
    "    for model, marker in zip(models, markers):\n",
    "        with open('savednet/'+model+'_mae_efficiency_' + load + '.npy', 'rb') as f:\n",
    "            np_list, mae = pickle.load(f)\n",
    "        \n",
    "        if load=='ALL':\n",
    "            other_id = None\n",
    "        else:\n",
    "            other_id = np.delete(np.array([0,1,2]), i)\n",
    "        \n",
    "        if model=='CANN':\n",
    "            mae_mean = np.mean(mae,axis=0)\n",
    "            mae_stdv = np.std(mae, axis=0)\n",
    "            mean_trn = mae_mean[i]\n",
    "            stdv_trn = mae_stdv[i]\n",
    "            ms = 8\n",
    "            if load == 'ALL':\n",
    "                pass\n",
    "            else:\n",
    "                mean_val = np.mean(mae_mean[other_id])\n",
    "                stdv_val = np.mean(mae_stdv[other_id])\n",
    "                ax.errorbar(np_list, mean_val, stdv_val, elinewidth=0.5, capsize=3.0, fmt=marker, color='b', label=model + ' valid.', markersize=ms)\n",
    "        else:\n",
    "            mae_mean = np.mean(mae,axis=1)\n",
    "            mae_stdv = np.std(mae, axis=1)\n",
    "            mean_trn = mae_mean[:,i]\n",
    "            stdv_trn = mae_stdv[:,i]\n",
    "            ms = 5\n",
    "            if load == 'ALL':\n",
    "                pass\n",
    "            else:\n",
    "                other_id = np.delete(np.array([0,1,2]), i)\n",
    "                mean_val = np.mean(mae_mean[:,other_id], axis=1)\n",
    "                stdv_val = np.mean(mae_stdv[:,other_id], axis=1)\n",
    "                mean_val = np.squeeze(mean_val)\n",
    "                stdv_val = np.squeeze(stdv_val)\n",
    "                ax.errorbar(np_list, mean_val, stdv_val, elinewidth=0.5, capsize=3.0, fmt=marker, color='b', label=model + ' valid.', markersize=ms)\n",
    "\n",
    "        \n",
    "        ax.errorbar(np_list, mean_trn, stdv_trn, elinewidth=0.5, capsize=3.0, fmt=marker, color='k', label=model + ' train.', markersize=ms)\n",
    "    ax.set(xlabel='Number of parameters', ylabel='$MAE$', title=load + ' training', yscale='log')\n",
    "axes[1].legend()\n",
    "fig.savefig('Figures/fig_rubber_efficiency_mae.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1,4,figsize=(20,4))\n",
    "\n",
    "loads = ['UT', 'ET', 'PS', 'ALL']\n",
    "panels = ['a', 'b', 'c', 'd']\n",
    "for i, (ax, load, panel) in enumerate(zip(axes, loads, panels)):\n",
    "    models  = ['CANN',  'ICNN', 'NODE']\n",
    "    markers = ['o',     '^-',   's--' ]\n",
    "    for model, marker in zip(models, markers):\n",
    "        with open('savednet/'+model+'_mae_efficiency_' + load + '.npy', 'rb') as f:\n",
    "            np_list, mae = pickle.load(f)\n",
    "        \n",
    "        if load=='ALL':\n",
    "            other_id = None\n",
    "        else:\n",
    "            other_id = np.delete(np.array([0,1,2]), i)\n",
    "        \n",
    "        if model=='CANN':\n",
    "            mae_mean = np.mean(mae,axis=0)\n",
    "            mae_stdv = np.std(mae, axis=0)\n",
    "            mean_trn = mae_mean[i]\n",
    "            stdv_trn = mae_stdv[i]\n",
    "            ms = 8\n",
    "        else:\n",
    "            mae_mean = np.mean(mae,axis=1)\n",
    "            mae_stdv = np.std(mae, axis=1)\n",
    "            mean_trn = mae_mean[:,i]\n",
    "            stdv_trn = mae_stdv[:,i]\n",
    "            ms = 5\n",
    "        \n",
    "        ax.errorbar(np_list, mean_trn, stdv_trn, elinewidth=0.5, capsize=3.0, fmt=marker, color='k', label=model, markersize=ms)\n",
    "        if mean_trn.shape == ():\n",
    "            errorp = stdv_trn\n",
    "            errorm = onp.min(onp.array([stdv_trn, mean_trn])) - 1.e-6 # this is to make sure the lower bound of the error bar is not negative in log plots\n",
    "            data = onp.expand_dims(onp.array([onp.array(np_list).squeeze(), mean_trn, errorp, errorm]), axis=0)\n",
    "        else:\n",
    "            errorp = stdv_trn\n",
    "            errorm = onp.min(onp.array([stdv_trn, mean_trn]), axis=0) - 1.e-6 # this is to make sure the lower bound of the error bar is not negative in log plots\n",
    "            data = onp.array([onp.array(np_list).squeeze(), mean_trn, errorp, errorm]).T\n",
    "        columns = ['x', 'y', 'error+', 'error-']\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        df.to_csv('TikZ/fig_rubber_eff_data/'+model+'_'+panel+'.csv', index=False, sep = ' ')\n",
    "    ax.set(xlabel='Number of parameters', ylabel='$MAE$', title=load + ' training', yscale='log')\n",
    "axes[1].legend()\n",
    "fig.savefig('Figures/fig_rubber_efficiency_mae_trnonly.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
