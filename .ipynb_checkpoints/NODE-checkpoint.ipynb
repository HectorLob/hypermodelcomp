{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-K81jKMdefgh",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from jax import grad, vmap, jit, partial, random\n",
    "import numpy as onp\n",
    "from jax.experimental.ode import odeint\n",
    "from jax.experimental import optimizers\n",
    "from jax.scipy.optimize import minimize\n",
    "import jax\n",
    "key = random.PRNGKey(0)\n",
    "import pickle\n",
    "from jax.lax import fori_loop, scan\n",
    "from jax.config import config\n",
    "import timeit\n",
    "# config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xT_E9Xlw4Af0"
   },
   "source": [
    "# Neural ODE constitutive model\n",
    "\n",
    "## Constitutive modeling\n",
    "\n",
    "When modeling a hyperelastic material from a mechanical perspective, we would like to define a strain energy function:\n",
    "$$\\Psi(I_1, I_2, ...,I_n)$$\n",
    "Where $I_i$ represent the invariants of the right Cauchy deformation tensor. One desired property of this function is convexity. This is important because it guarantees that the material always a non-negative stiffness. To satisfy convexity, we have 2 equivalent options:\n",
    "\n",
    "- the second derivative (stiffness) is always non-negative.\n",
    "- the first derivative monotonically increases with respect to the input.\n",
    "\n",
    "A very commonly used form of $\\Psi$ is based on an additive decomposition:\n",
    "$$\\Psi(I_1, I_2, ...,I_n) = \\sum_i^n \\Psi_i(I_i)$$\n",
    "In this particular case, to satisfy convexity, we just need $\\frac{\\delta \\Psi_i}{\\delta I_i}$ to be monotonically increasing.\n",
    "\n",
    "\n",
    "## Neural networks for constitutive modeling\n",
    "\n",
    "In this context, we would like to approximate $\\Psi$ with a neural network to avoid the need of choosing a particular form of the equation. And to satisfy the convexity requirement, we would like the approximate $\\frac{\\delta \\Psi_i}{\\delta I_i}$  with a neural network that monotonically increases. This can be satisfied with a neural ordinary differential equation [1], which has the form:\n",
    "\n",
    "$$\\boldsymbol{y} = \\boldsymbol{x} + \\int_0^T f(\\boldsymbol{h}(t),t,\\boldsymbol{\\theta})dt$$\n",
    "\n",
    "where $f(\\boldsymbol{h}(t),t,\\boldsymbol{\\theta})$ is a neural network that represents the right hand side of an ODE. This mapping from $\\boldsymbol{x}$ to $\\boldsymbol{y}$ must monotonically increase or decrease following this reasoning:\n",
    "- For the kind of neural network that we use for right hand side of the ODE, we can guarantee that there is a unique solution for every initial condition. This means, in our context, that for every input there is a unique output [1].\n",
    "- But, we can also integrate the system backwards in time, meaning that we can use the output as an initial condition to go to the input. And from what we just said, there must be a unique input for every output. This means we can invert the system $y = g(x),\\, x = g^{-1}(y)$. \n",
    "- If a function can be inverted, it must monotonically increase or decrease.\n",
    "- Another way to interpret this, is to consider trajectories (solutions of the ODE) never cross each other. \n",
    "\n",
    "The idea is to approximate $\\frac{\\delta \\Psi_i}{\\delta I_i}$ with a neural ODE. But in general, any invertible architecture should work. \n",
    "$$\\frac{\\delta \\Psi_i}{\\delta I_i} = I_i + \\int_0^T f(\\boldsymbol{h}(t),t,\\boldsymbol{\\theta})dt$$\n",
    "$$\\boldsymbol{h}(0) = I_i$$\n",
    "\n",
    "The hope is that data with steer the function to be monotonically increasing instead of decreasing.\n",
    "\n",
    "If we want to compute the tangent stiffness matrix, which involves $\\frac{\\delta^2 \\Psi_i}{\\delta I_i^2}$, this can done efficiently with the adjoint method. \n",
    "\n",
    "In the rare case that would like to evaluate the strain energy function, we would have to do it numerically.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_existing_model = False\n",
    "model_name = 'P12AC1_bsxsy'\n",
    "dataset_name = 'P12AC1_bsxsy'\n",
    "#P1C1: n_offx = n_offy = 61, n_equi = n_strx = n_stry = 0\n",
    "#S111S1: n_offx = n_offy = n_equi = 183, n_strx = n_stry = 0\n",
    "#P12AC1: n_offx = 72, n_offy = 76, n_equi = 81, n_strx = 101, n_stry = 72\n",
    "#P12BC2: n_offx = 76, n_offy = 76, n_equi = 95, n_strx = 101, n_stry = 84\n",
    "#Unless you are working with multifidelity data, set all of the following to 0.\n",
    "n_offx = 0\n",
    "n_offy = 0\n",
    "n_equi = 0\n",
    "n_strx = 0\n",
    "n_stry = 0\n",
    "n_hf = n_offx + n_offy + n_equi + n_strx + n_stry\n",
    "\n",
    "with open('training_data/' + dataset_name + '.npy', 'rb') as f:\n",
    "    lamb, sigma_gt = np.load(f,allow_pickle=True)\n",
    "n_data = lamb.shape[0]\n",
    "weights = onp.ones([n_data,1])\n",
    "weights[:n_hf] = 10 #Weight of high fidelity loss\n",
    "#sigma_gt = np.concatenate((sigma_gt.transp,weights), axis=0)\n",
    "sigma_gt = np.hstack((sigma_gt,weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "haICRU55fadE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A generic function to compute the stress given the princial stretches\n",
    "# this will be used later for the NN\n",
    "@partial(jit, static_argnums=(4))\n",
    "def sigma(lamb1, lamb2, lamb3, p, NN, params):\n",
    "    I1_params, I2_params, Iv_params, Iw_params, J1_params, J2_params, J3_params, J4_params, J5_params, J6_params, \\\n",
    "        I_weights, theta, Psi1_bias, Psi2_bias = params\n",
    "    \n",
    "    a = 1/(1+np.exp(-I_weights))\n",
    "    \n",
    "    v0 = np.array([ np.cos(theta), np.sin(theta), 0])\n",
    "    w0 = np.array([-np.sin(theta), np.cos(theta), 0])\n",
    "    F = np.array([[lamb1, 0, 0],\n",
    "                  [0, lamb2, 0],\n",
    "                  [0, 0, lamb3]])\n",
    "    v = np.dot(F, v0)\n",
    "    w = np.dot(F, w0)\n",
    "    b = np.dot(F, F.T)\n",
    "    I1 = np.trace(b)\n",
    "    b2 = np.einsum('ij,jk->ik', b, b)\n",
    "    I2 = 0.5*(np.trace(b)**2 - np.trace(b2)) # again assuming is diagonal\n",
    "    Iv = np.sum(b*np.outer(v0,v0)) \n",
    "    Iw = np.sum(b*np.outer(w0,w0)) \n",
    "    \n",
    "    I1 = I1-3\n",
    "    I2 = I2-3\n",
    "    Iv = Iv-1\n",
    "    Iw = Iw-1\n",
    "    J1 = a[0]*I1+(1-a[0])*I2\n",
    "    J2 = a[1]*I1+(1-a[1])*Iv\n",
    "    J3 = a[2]*I1+(1-a[2])*Iw\n",
    "    J4 = a[3]*I2+(1-a[3])*Iv\n",
    "    J5 = a[4]*I2+(1-a[4])*Iw\n",
    "    J6 = a[5]*Iv+(1-a[5])*Iw\n",
    "    \n",
    "    #Iv, Iw, J1, J2, J3, J4, J5, J6 = 0.,0.,0.,0.,0.,0.,0.,0.\n",
    "    \n",
    "    Psi1 = NN(I1,  I1_params)\n",
    "    Psi2 = NN(I2,  I2_params)\n",
    "    Psiv = NN(Iv,  Iv_params)\n",
    "    Psiw = NN(Iw,  Iw_params)\n",
    "    Phi1 = NN(J1,  J1_params)\n",
    "    Phi2 = NN(J2,  J2_params)\n",
    "    Phi3 = NN(J3,  J3_params)\n",
    "    Phi4 = NN(J4,  J4_params)\n",
    "    Phi5 = NN(J5,  J5_params)\n",
    "    Phi6 = NN(J6,  J6_params)\n",
    "    \n",
    "    Psiv = np.max([Psiv, 0])\n",
    "    Psiw = np.max([Psiw, 0])\n",
    "    Phi1 = np.max([Phi1, 0])\n",
    "    Phi2 = np.max([Phi2, 0])\n",
    "    Phi3 = np.max([Phi3, 0])\n",
    "    Phi4 = np.max([Phi4, 0])\n",
    "    Phi5 = np.max([Phi5, 0])\n",
    "    Phi6 = np.max([Phi6, 0])\n",
    "    \n",
    "    Psi1 = Psi1 +     a[0]*Phi1 +     a[1]*Phi2 +     a[2]*Phi3 + np.exp(Psi1_bias)\n",
    "    Psi2 = Psi2 + (1-a[0])*Phi1 +     a[3]*Phi4 +     a[4]*Phi5 + np.exp(Psi2_bias)\n",
    "    Psiv = Psiv + (1-a[1])*Phi2 + (1-a[3])*Phi4 +     a[5]*Phi6\n",
    "    Psiw = Psiw + (1-a[2])*Phi3 + (1-a[4])*Phi5 + (1-a[5])*Phi6\n",
    "    \n",
    "    return -p*np.eye(3) + 2*Psi1*b + 2*Psi2*((I1+3)*b - b**2) + 2*Psiv*np.outer(v,v) + 2*Psiw*np.outer(w,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "V9TVy8W6KJnt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a wrapper to compute biaxial stresses easily\n",
    "@partial(jit, static_argnums=(1))\n",
    "def sigma_biaxial(lamb, NN, params):\n",
    "    # incompressibility\n",
    "    lamb3 = 1/(lamb[0]*lamb[1])\n",
    "    # use \\sigma_33 = 0 to compute pressure\n",
    "    p = sigma(lamb[0], lamb[1], lamb3, 0, NN, params)[2,2]\n",
    "    # return \\sigma_11 and \\sigma_22\n",
    "    return sigma(lamb[0], lamb[1], lamb3, p, NN, params)[[0,1],[0,1]]\n",
    "# in jax we do everything for one value and then vectorize with vmap\n",
    "sigma_biaxial_vmap = vmap(sigma_biaxial, in_axes=(0,None, None), out_axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fze3LRWYMjou",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fully connected neural network code\n",
    "def init_params(layers, key):\n",
    "    Ws = []\n",
    "    for i in range(len(layers) - 1):\n",
    "        std_glorot = np.sqrt(2/(layers[i] + layers[i + 1]))\n",
    "        key, subkey = random.split(key)\n",
    "        Ws.append(random.normal(subkey, (layers[i], layers[i + 1]))*std_glorot)\n",
    "    return Ws\n",
    "\n",
    "@jit\n",
    "def forward_pass(H, Ws):\n",
    "    N_layers = len(Ws)\n",
    "    for i in range(N_layers - 1):\n",
    "        H = np.matmul(H, Ws[i])\n",
    "        H = np.tanh(H)\n",
    "        Y = np.matmul(H, Ws[-1])\n",
    "    return Y\n",
    "\n",
    "@partial(jit, static_argnums=(0,))\n",
    "def step(loss, i, opt_state, X_batch, Y_batch):\n",
    "    params = get_params(opt_state)\n",
    "    g = grad(loss)(params, X_batch, Y_batch)\n",
    "    return opt_update(i, g, opt_state)\n",
    "\n",
    "def train(loss, X, Y, opt_state, key, nIter = 10000, batch_size = 10):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for it in range(nIter):\n",
    "        key, subkey = random.split(key)\n",
    "        idx_batch = random.choice(subkey, X.shape[0], shape = (batch_size,), replace = False)\n",
    "        opt_state = step(loss, it, opt_state, X[idx_batch], Y[idx_batch])         \n",
    "        if (it+1)% 10000 == 0:\n",
    "            params = get_params(opt_state)\n",
    "            train_loss_value = loss(params, X, Y)\n",
    "            train_loss.append(train_loss_value)\n",
    "            to_print = \"it %i, train loss = %e\" % (it+1, train_loss_value)\n",
    "            print(to_print)\n",
    "    return get_params(opt_state), train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bOIkF_yNqrf",
    "outputId": "a0d3f88b-cd08-4767-9e5b-5e0beb062e69",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def ODEforward_pass(y0, params):\n",
    "    f = lambda y, t: forward_pass(np.array([y]),params) # fake time argument for ODEint\n",
    "    return odeint(f, y0, np.array([0.0,1.0]))[-1] # integrate between 0 and 1 and return the results at 1\n",
    "\n",
    "def Eulerforward_pass(y0, params, steps = 10):\n",
    "    body_func = lambda y, i: (y + forward_pass(np.array([y]), params)[0], None)\n",
    "    out, _ = scan(body_func, y0, None, length = steps)\n",
    "    return out\n",
    "\n",
    "@jit\n",
    "def loss(params, lamb, sigma_gt):\n",
    "    sigma_pr = sigma_biaxial_vmap(lamb, Eulerforward_pass, params)\n",
    "    J_weights = params[10]\n",
    "    #sigma_gt[:,2] contains the weights of multifidelity data. i.e. sigma_gt[:,2] = 1 for low fidelity and 10 for high fidelity.\n",
    "    dummy1 = sigma_pr[:,0]\n",
    "    dummy2 = sigma_gt[:,0]\n",
    "    dummy3 = sigma_gt[:,2]\n",
    "    \n",
    "    #transform to log space:\n",
    "    # dummy1 = np.log(dummy1)\n",
    "    # dummy2 = np.log(dummy2)\n",
    "    loss_MSE = np.average((dummy1 - dummy2)**2*dummy3)\n",
    "                          \n",
    "    dummy1 = sigma_pr[:,1]\n",
    "    dummy2 = sigma_gt[:,1]\n",
    "    # dummy1 = np.log(dummy1)\n",
    "    # dummy2 = np.log(dummy2)\n",
    "    loss_MSE+= np.average((dummy1 - dummy2)**2*dummy3)\n",
    "    \n",
    "#    loss_L1 = np.array(1.0e-5*np.sum(np.abs(J_weights)), dtype = np.float64)\n",
    "#    loss_L1 = 1.0e-5*np.sum(np.abs(J_weights))\n",
    "    return  loss_MSE# + loss_L1\n",
    "\n",
    "layers = [1, 5, 5, 1]\n",
    "\n",
    "W1_params = init_params(layers, key)\n",
    "W2_params = init_params(layers, key)\n",
    "W4v_params = init_params(layers, key)\n",
    "W4w_params = init_params(layers, key)\n",
    "J1_params = init_params(layers, key)\n",
    "J2_params = init_params(layers, key)\n",
    "J3_params = init_params(layers, key)\n",
    "J4_params = init_params(layers, key)\n",
    "J5_params = init_params(layers, key)\n",
    "J6_params = init_params(layers, key)\n",
    "I_weights = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "theta = 1.0\n",
    "Psi1_bias = -3.0\n",
    "Psi2_bias = -3.0\n",
    "\n",
    "params = (W1_params, W2_params, W4v_params, W4w_params, J1_params, J2_params, J3_params, J4_params, J5_params, J6_params, I_weights, \\\n",
    "          theta, Psi1_bias, Psi2_bias)\n",
    "\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(1.e-4)\n",
    "opt_state = opt_init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/jax/numpy/lax_numpy.py:1674: FutureWarning: jax.numpy reductions won't accept lists and tuples in future versions, only scalars and ndarrays\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 10000, train loss = 1.068761e-02\n",
      "it 20000, train loss = 2.305864e-03\n",
      "it 30000, train loss = 6.219599e-04\n",
      "it 40000, train loss = 5.151139e-04\n",
      "it 50000, train loss = 4.814814e-04\n",
      "it 60000, train loss = 4.625857e-04\n",
      "it 70000, train loss = 4.621571e-04\n",
      "it 80000, train loss = 4.395556e-04\n",
      "it 90000, train loss = 4.256108e-04\n",
      "it 100000, train loss = 4.051035e-04\n",
      "it 110000, train loss = 3.769968e-04\n",
      "it 120000, train loss = 3.651644e-04\n",
      "it 130000, train loss = 3.059523e-04\n",
      "it 140000, train loss = 3.023825e-04\n",
      "it 150000, train loss = 2.986836e-04\n",
      "it 160000, train loss = 2.947662e-04\n",
      "it 170000, train loss = 2.941077e-04\n",
      "it 180000, train loss = 2.953334e-04\n",
      "it 190000, train loss = 2.981724e-04\n",
      "it 200000, train loss = 2.899026e-04\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -r 0\n",
    "if load_existing_model == False:\n",
    "    if lamb.shape[0] > 100:\n",
    "        batch_size = 100\n",
    "    else:\n",
    "        batch_size = lamb.shape[0]\n",
    "    \n",
    "    params, train_loss, val_loss = train(loss,lamb, sigma_gt, opt_state, key, nIter = 200000, batch_size = batch_size)\n",
    "    with open('savednet/' + model_name + '.npy', 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "else:\n",
    "    with open('savednet/' + model_name + '.npy', 'rb') as f:\n",
    "        params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyperparameter study\n",
    "\n",
    "| Network Architecture | Training Loss |\n",
    "|----------------------|---------------|\n",
    "| 1x5x1                | 5.52E-01      |\n",
    "| 1x5x5x1              | 2.23E-02      |\n",
    "| 1x5x5x5x1            | 5.66E-02      |\n",
    "\n",
    "| Network Architecture | Training Loss |\n",
    "|----------------------|---------------|\n",
    "| 1x3x3x1              | 2.45E-02      |\n",
    "| 1x5x5x1              | 2.23E-02      |\n",
    "| 1x8x8x1              | 7.27E-02      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Constitutive_NeuralODE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Intel",
   "language": "python",
   "name": "intel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
